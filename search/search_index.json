{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Linux Notes","text":"<p>My notes on programming, mostly using R, Python, and bash.</p>"},{"location":"disclosure/","title":"Copyrights and Privacy Policy","text":""},{"location":"disclosure/#copyrights","title":"Copyrights","text":"<p>This website inherits copyright and disclosures details from a parent website, SBAmin.com and it is available at sbamin.com/disclosure. Source code for this website is available at sbamin/code101 under MIT license.</p> <p>Use of Material for MkDocs theme is under MIT LICENSE agreement from Martin Donath.</p>"},{"location":"disclosure/#privacy","title":"Privacy","text":"<p>This website uses cookies, in particular, Google Analytics to monitor your one-time or repeated visits and page visit behavior. These cookies also measure relevant sections and pages of website which are frequently or rarely visited, e.g., using user-defined search keywords, and thus help me optimize contents and relevant meta data to make this documentation better.</p> <p>This website also uses other third-party cookies which are shipped along with use of their respective services, e.g., content-delivery network, java scripts, custom fonts, etc. These services are required for a proper functioning (aesthetics at-large) of this website. However, disabling these cookies should not render this website non-functional.</p> <p>Implicit consent until cookie consent is enabled on this site</p> <p>For time being, your visit to this site will imply an automatic consent of yours to accept these cookie policies. Until I update user-enabled cookie consent, you are encouraged to use web browser default settings or browser add-ons that can block such cookies.</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#bash","title":"bash","text":"<ul> <li>Setting up CPU env - Part 1</li> </ul>"},{"location":"tags/#cellpose","title":"cellpose","text":"<ul> <li>Image analysis on GPU-based HPC</li> </ul>"},{"location":"tags/#cellprofiler","title":"cellprofiler","text":"<ul> <li>Image analysis on GPU-based HPC</li> </ul>"},{"location":"tags/#conda","title":"conda","text":"<ul> <li>Setting up CPU env - Part 1</li> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> </ul>"},{"location":"tags/#database","title":"database","text":"<ul> <li>Setting up CPU env - Part 3</li> </ul>"},{"location":"tags/#deep-learning","title":"deep learning","text":"<ul> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#gpu","title":"gpu","text":"<ul> <li>Image analysis on GPU-based HPC</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#howto","title":"howto","text":"<ul> <li>Run OnDemand App with a custom conda env</li> </ul>"},{"location":"tags/#hpc","title":"hpc","text":"<ul> <li>Run OnDemand App with a custom conda env</li> <li>Setting up CPU env - Part 1</li> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> <li>Image analysis on GPU-based HPC</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#imaging","title":"imaging","text":"<ul> <li>Image analysis on GPU-based HPC</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#jupyter","title":"jupyter","text":"<ul> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> </ul>"},{"location":"tags/#kernels","title":"kernels","text":"<ul> <li>Setting up CPU env - Part 2</li> </ul>"},{"location":"tags/#onboarding","title":"onboarding","text":"<ul> <li>Run OnDemand App with a custom conda env</li> </ul>"},{"location":"tags/#ondemand","title":"ondemand","text":"<ul> <li>Run OnDemand App with a custom conda env</li> </ul>"},{"location":"tags/#programming","title":"programming","text":"<ul> <li>Setting up CPU env - Part 1</li> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#segmentation","title":"segmentation","text":"<ul> <li>Image analysis on GPU-based HPC</li> </ul>"},{"location":"tags/#setup","title":"setup","text":"<ul> <li>Setting up CPU env - Part 1</li> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#stardist","title":"stardist","text":"<ul> <li>Image analysis on GPU-based HPC</li> </ul>"},{"location":"tags/#startup","title":"startup","text":"<ul> <li>Setting up CPU env - Part 3</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/run-ondemand-app-with-a-custom-conda-env/","title":"Run OnDemand App with a custom conda env","text":"<p>Setup to run OnDemand apps on mccleary. As always, this is an overview and not an error-proof guide. Always, refer to McCleary Docs from Yale HPC Team and Contact HPC support for questions.</p>","tags":["onboarding","hpc","howto","ondemand"]},{"location":"blog/2023/run-ondemand-app-with-a-custom-conda-env/#command-line-setup","title":"Command Line setup","text":"<ul> <li>Login to mccleary using terminal. Guide</li> <li>Start an interactive job, so conda setup will not get be killed in the middle, corrupting your setup</li> </ul> <pre><code>## request 6 hours on devel partition with 4 cpus and 12G memory\nsrun --job-name=myinteractive --chdir=${HOME} --partition=devel --time=06:00:00 --mem=1G --nodes=1 --cpus-per-task=4 --mail-type=FAIL --export=all --pty bash --login\n</code></pre> <ul> <li> <p>Once interactive job starts, you will notice a terminal prompt changing from <code>netid@login1or2.mccleary</code> to <code>netid@giberrish_compute_node_id.mccleary</code>, e.g., <code>netid@r209u08n02.mccleary</code>. Now, you are set to run conda set up as following!</p> </li> <li> <p>load miniconda module with a stable version, 23.5.2 from yale hpc module library. Modules are pre-configured softwares from Yale HPC, so that you do not need to do setup/compile softwares. Guide for Modules</p> </li> </ul> <pre><code>module load miniconda/23.5.2\n</code></pre> <ul> <li>Create a conda env with the desired name at <code>-n</code> flag in the following command, e.g., renv</li> </ul> <p>Here, we are creating a conda env using speedier <code>mamba</code> instead of <code>conda</code> command. While creating a new conda env, we ask conda to install latest R software as well as several commonly used R packages or libraries (one that starts with <code>r-</code>). We also install several other softwares to setup python language and kernels which can be useful at the later date to run OnDemand apps like Jupyter and Code Server.</p> <pre><code>mamba create -n renv r-base r-essentials r-tidyverse r-tidymodels r-devtools r-biocmanager r-pak gcc_linux-64 cmake autoconf python numpy scipy pandas matplotlib scikit-learn ipython jupyter r-irkernel\n</code></pre> <ul> <li>Once installation is complete, activate a new conda env. Replace <code>renv</code> below with a name of conda env you used above.</li> </ul> <pre><code>mamba activate renv\n</code></pre> <ul> <li>You should notice terminal prompt change from <code>netid@XXXXX.mccleary</code> to <code>(renv)etid@XXXXX.mccleary</code>, meaning now you are within conda env named, renv.</li> <li>Once you are within a specific conda env, e.g., renv in this case, you can run R or other softwares you installed while creating conda env above. You can also install additional conda packages using <code>mamba install</code> and NOT <code>mamba create</code> command. The latter command is used only to create a new conda env. To find additional softwares available for conda env, go to https://anaconda.org/ and search for your R package or other softwares of interest, e.g., R packages like tidymodels, maftools, etc. or non-R softwares like bedtools, etc.</li> <li>Make sure to use <code>mamba</code> instead of <code>conda</code> command to install packages as <code>mamba install</code> is much faster and reliable than using <code>conda install</code>.</li> </ul> <p>Avoid installing conda packages from third-parties</p> <p>While searching https://anaconda.org/, always rely on packages stemming from either <code>conda-forge</code> or <code>bioconda</code> as those two are reliable software repositories and well-maintained. Never install conda packages from third-party developers.</p> <p>To avoid breaking your conda env, worth reading a detailed conda setup guide. Do not copy and paste commands but rather read and understand rationale behind maintaining a stable, error-free, conda env.</p> <ul> <li>Finally, create a file called <code>~/.condarc</code> on HPC and copy-paste following contents in it. You can use Files section from OnDemand dashboard and navigate to Home directory and create a file, <code>/.condarc</code>. Alternately, on command-line terminal, you can run <code>nano ~/.condarc</code> to create and edit file. See nano guide</li> </ul> <pre><code>#### YCRC suggested condarc config ####\nenv_prompt: '({name})'\nauto_activate_base: false\nchannels:\n  - conda-forge\n  - bioconda\n  - defaults\n#### Additional config ####\nauto_update_conda: False\nalways_yes: False\nadd_pip_as_python_dependency: True\nssl_verify: True\nallow_softlinks: True\nuse_only_tar_bz2: False\nanaconda_upload: False\n\nrepodata_threads: 4\nverify_threads: 4\nexecute_threads: 4\n</code></pre>","tags":["onboarding","hpc","howto","ondemand"]},{"location":"blog/2023/run-ondemand-app-with-a-custom-conda-env/#using-ondemand-apps","title":"Using OnDemand Apps","text":"<p>For loading your custom conda env in OnDemand apps like RStudio Server and Code Server, you need to set a custom user module, such that OnDemand can recognize this module and load it prior to starting OnDemand Apps.</p> <ul> <li>Create a directory where you can install/manage your modules. These modules are on top of modules managed by Yale HPC. Guide for Modules</li> </ul> <pre><code>mkdir -p /gpfs/gibbs/pi/labid/\"${USER}\"/hpcenv/opt/modules\n</code></pre> <ul> <li>Edit your <code>~/.bash_profile</code> file on HPC and add following.</li> </ul> <pre><code>module use --prepend /gpfs/gibbs/pi/labid/\"${USER}\"/hpcenv/opt/modules\n</code></pre> <ul> <li>Now, create a modulefile directory, named <code>renv</code> or the name of conda env you like to use in OnDemand App.</li> </ul> <pre><code>mkdir -p /gpfs/gibbs/pi/labid/\"${USER}\"/hpcenv/opt/modules/renv\n</code></pre> <ul> <li>Under <code>renv/</code> directory, create a modulefile named <code>1.0.lua</code> (our first version of this module) and add following contents to it.</li> </ul> <p>module directory name, <code>renv</code> must match conda env</p> <p>Ensure that module file, <code>1.0.lua</code> is under a directory that is named exactly as the name of conda env you are trying to load, e.g., <code>renv/1.0.lua</code> will assume that you are trying to load conda env, renv.</p> /renv/1.0.lua<pre><code>help([==[\n\nDescription\n===========\nPreload config for OnDemand Apps: Code Server and RStudio server.\n\nNote that this module, if loaded at the start of OnDemand VSCode or RStudio server, will be loaded\nsilently prior to initializing code-server.\n\nmodule list command may not show this module as being loaded but the module configs should have\nalready be applied to user env, e.g., PATH, LIBRARY_PATH, etc. should show directives set as per this module.\n\nMuch of config for this file rely on upstream config from Yale HPC module for miniconda:\n/vast/palmer/apps/avx2/modules/tools/miniconda/\n\nMore information\n================\n    + Yale HPC Guide: https://docs.ycrc.yale.edu/\n    + Module file: https://lmod.readthedocs.io/en/latest/\n    + Working with HPC: https://code.sbamin.com/hpc/\n\n]==])\n\nwhatis([==[Description: Preload config for OnDemand Apps: RStudio Server and Code Server.]==])\nwhatis([==[URL: https://docs.ycrc.yale.edu/]==])\nwhatis([==[URL: https://code.sbamin.com/hpc/]==])\n\n-- Module Name and Version are parsed by Lmod from dir/version string in module path\n-- REVIEW: ensure that this module file is under a directory that is named\n-- exactly as the name of conda env you are trying to load, e.g.,\n-- renv/1.0.lua will assume that you are trying to load conda env, renv\nlocal pkgName = myModuleName()\nlocal version = myModuleVersion()\nlocal pkgNameVer = myModuleFullName()\n\n--other modules\n-- load(\"module_name/version_id\")\n\n-- following bash variable will be visible once you start rstudio server or\n-- code server. Run command:\n-- echo ${my_vscode} from rstudio or code server to ensure you have loaded\n-- this module.\npushenv(\"my_vscode\", \"1.0\")\n\n-- NOTE: Preferably should use depends_on and/or prereq for module loads\n-- and not concatenate using bash &amp;&amp; directive.\nexecute{cmd=\"module load miniconda &amp;&amp; conda activate \"..pkgName, modeA={\"load\"}}\nexecute{cmd=\"conda deactivate &amp;&amp; module unload miniconda\", modeA={\"unload\"}}\n\n-- end --\n</code></pre>","tags":["onboarding","hpc","howto","ondemand"]},{"location":"blog/2023/run-ondemand-app-with-a-custom-conda-env/#test-run","title":"Test run","text":"<ul> <li>Exit all of sessions from terminal by <code>exit</code> and closing ondemand sessions, if any.</li> <li> <p>Login again to McCleary using ssh or OnDemand terminal interface under Clusters &gt; McCleary Shell Access.</p> </li> <li> <p>Start an interactive session as above.</p> </li> <li>Activate your new conda env via module you just created.</li> </ul> <pre><code>module load renv/1.0\n</code></pre> <ul> <li>Check if module is loaded or not with <code>module list</code> command which will show, both, <code>miniconda</code> and <code>renv</code> modules loaded!</li> <li>Run <code>R</code> or <code>python</code> and you should see those running.</li> <li>Also check <code>R --version</code> and note that version.</li> <li>Once done, unload module using <code>module unload renv/1.0</code> and then check with <code>module list</code>. Both, <code>miniconda</code> and <code>renv</code> should disappear now!</li> </ul>","tags":["onboarding","hpc","howto","ondemand"]},{"location":"blog/2023/run-ondemand-app-with-a-custom-conda-env/#ondemand-apps","title":"OnDemand Apps","text":"<p>Once module test run is working ok, open OnDemand terminal interface and under Interactive Apps menu, click RStudio Server and NOT Rstudio Desktop.</p> <ul> <li>Pick R version that is closest to one you noted above in the test run. Ideally, it should not deviate too much, e.g., if test run R version is 4.4 but available R versions in OnDemand Apps have only R 4.2 or R 4.3, ask HPC to install a new R for OnDemand Apps. If R version is 4.3.2 for your test run but OnDemand App has R 4.3.0, that's ok.</li> <li>Select number of hours, cpus, memory, and choose partition accordingly.</li> <li>Under Additional modules (optional) option, type <code>renv/1.0</code> or whichever module you just created above. If additional modules option is not visible, check <code>Check the box to view more options</code> option.</li> <li>Launch OnDemand App.</li> </ul> <p>Once OnDemand apps launches, open terminal from RStudio or code server, and ensure that your module is loaded by checking output of <code>module list</code>.</p>","tags":["onboarding","hpc","howto","ondemand"]},{"location":"hpc/","title":"Getting started with HPC","text":""},{"location":"hpc/#hello-world","title":"Hello World!","text":"<p>I work in computational cancer biology for over a decade now. While true scale of big data can vary across fields, I work on daily basis to analyze large-scale genomic data on the high-performance computing (HPC) infrastructure. Once every two years or so, I reset my HPC environment (env) so as to upgrade major version for programming languages, mainly R and Python. Here, reset as in I force my HPC login  terminal window to behave as if I am login to HPC for the first time with system defaults. I also prefer doing such hard reset over patching cumulative updates and tips I get from my twitterverse over those two years.</p> <p>I usually write notes while I do such resets and over years, I have found it quite useful, especially for debugging and when moving to new places with new HPC envs. So, this time, I thought to make these notes available here :fontawesome-solid-globe-africa: with those starting for the first time in HPC env or even programming on a linux run machine as the primary audience.</p> <p></p> <p>I started my journey in computational biology at Dana-Farber back in 2008. The entire programming env was quite new and daunting to me with no prior experience in programming. I recollect typing <code>cd</code>, <code>chdir</code>, etc. commands in Windows\u00ae DOS terminal for not more than an hour when our teachers introduced us about computers during my high-school years! Nonetheless, I got curious looking at that terminal  and then arrival of the internet during my medical school years fueled my curiosity and helped me get familiar with computers. I was not skilled and in fact, far from comfortable the time I joined Dana-Farber but along the ride, I was helped by my colleagues and mentors, and those with whom I'd not interacted in-person: In particular Hadley Wickham and tidyverse team for sharing an excellent documentation on programming in R and countless users at stackoverflow and biostars forums for sharing their ideas and bugfixes. So, it's paying it forward time and hope some of you find this documentation useful and a step closer towards a skilled programmer.</p> <p>Scope of this documentation</p> <p>I must say though that this website is more geared towards getting you started with linux computing and an attempt to lower the resistance to working on the terminal , i.e., to set up a stable linux env so that you can focus on learning programming instead of debugging on why certain commands or program fail to work.</p> <p>That said, this website is not intended to teach you on how to program in R or Python or other languages. I am certain that there are several excellent online course and textbook materials available for such learning. I will add a few of my favorite materials here in the future.</p> <p>I have tried to provide logic behind most of commands, including external links for technical words. However, you may find several keywords or phrases as jargon, e.g., you may wonder what <code>pip install</code> is or what is kernel? In such cases, I suggest to at least do a web search and familiarize yourself on what those words/phrases mean and why they could be relevant as you master your programming skills.</p>"},{"location":"hpc/#getting-started","title":"Getting Started","text":"<p>To get started, I have partitioned documentation into two major sections. The first section (CPU Computing) details - in three parts - how to setup HPC env using conda based package management system.</p> <ul> <li>In Part 1, we will start with HPC setup either from the scratch as in login to HPC on the day one or will reset or overhaul an existing HPC env<sup>1</sup>. We will install conda and set a default and minimal bash startup profile.</li> <li>In Part 2, we will setup two or more dedicated conda env to host commonly used programming languages in computational biology: R along with Python 3 that ships with conda. We will also setup Jupyter Lab, a popular user interface similar to RStudio to interact with hosted programming languages and files over the secure internet browser.</li> <li>Finally, in Part 3, we will charge up our HPC env by adding Julia - another popular language in computational biology. We will also install a few drivers and jupyter kernels to interact with externally hosted databases like SQL or postgresql. I will also outline setting up Modules, Snakemake-based workflows, and Singularity container system. Finally, we will launch our custom HPC env using a mighty bash startup sequence </li> </ul> <p>Second section (GPU computing) is an optional setup and intended for those working on GPU-enabled HPCs. We will walk through setting up commonly used GPU libraries, e.g., Tensorflow 2, Keras, and PyTorch. I am going to update this section in the near future.</p>"},{"location":"hpc/#feedback","title":"Feedback","text":"<p>Source code of this website is available at sbamin/code101. I welcome user contributions, including bugfixes, enhancements, and alternative approaches. Since I update my linux env once every two years or so, I will not be updating this website often. If I encounter major bugs in my working env, I will update this website accordingly.</p> <p>This website is tightly linked to related  github code repository at sbamin/code101, including page comments which are powered by GitHub Discussions. Please use github issues and pull requests for a bug report and contributions, respectively. For more, you can tag/follow me  at @sbamin.</p>"},{"location":"hpc/#acknowledgments","title":"Acknowledgments","text":"<p>Big shout-out to Martin Donath for sharing this great documentation ecosystem at Material for MkDocs. Over last three years, it has helped me a lot in documenting my daily work, progress notes, etc.</p> <ol> <li> <p>Preferably requires prior working experience with HPC env and following cautionary notes I have placed to do such reset.\u00a0\u21a9</p> </li> </ol>"},{"location":"hpc/cpu/sumner_1/","title":"Setting up CPU env - Part 1","text":"","tags":["hpc","setup","conda","bash","programming"]},{"location":"hpc/cpu/sumner_1/#set-up-for-hpc-sumner","title":"Set up for HPC Sumner","text":"<p>There are at least two scenarios with an existing HPC env:</p> <ol> <li>We start setting up HPC env from scratch, i.e., right after login to HPC login node for the first time or</li> <li>We already have an existing custom setup, e.g., using linuxbrew or even conda, and we like to start from fresh setup. This is true (as in my case) when you may have setup conda env in your home directory and conda env grew in size over time that you are now approaching disk quota of 50 GB for home directory. So, now we like to move it to tier 1 space (<code>/projects/</code>) which has disk quota in TBs in not GBs.</li> </ol> <p>Following will guide setting up HPC env for both scenarios.</p> <p>Change user paths per your username and HPC paths</p> <p>Most of setup below have commands and locations (paths) tied to my username, <code>amins</code> and our HPC cluster at The Jackson Laboratory (JAX), namely Sumner and Winter HPC, one each of CPU and GPU-based computing. Please ensure that you edit paths such that it reflects your username and paths that are available for HPC at your institute.</p>","tags":["hpc","setup","conda","bash","programming"]},{"location":"hpc/cpu/sumner_1/#option-1-start-from-scratch","title":"Option 1: Start from scratch","text":"<pre><code>ssh userid@login.sumner.jax.org\n\n## Know OS and kernel version\ncat /etc/redhat-release\nuname -a\n</code></pre> <p>Running CentOS Linux release 7.7.1908 (Core) Linux sumner-log1 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux </p> <ul> <li>First, login to sumner with a clean env, i.e., as it ships with default profile from HPC team, and nothing added in following files. Default bash configuration for sumner looks similar to following files. See example files in the source at  confs/hpc/initials/.</li> </ul> <pre><code>~/.bashrc\n~/.bash_profile\n~/.bash_aliases # if it exists\n~/.profile # if it exists\n</code></pre> <p>If you had custom bash configs (linuxbrew, previous conda, etc.), disable those by commenting out from above files. If you'd linuxbrew installed, make sure to disable it unless you are confident that conda and brew can work in harmony! Same goes for <code>~/.local/</code> directory which should not exist at the fresh startup. If it does, you may have installed some tools using python, perl, or other non-root based setup scripts. For clean setup, ~/.local directory needs to be removed from user environment, i.e., either rename it to say, ~/.local_deprecated or archive it somewhere! If you have made significant changes to HPC env, it is better to follow Option 2.</p>","tags":["hpc","setup","conda","bash","programming"]},{"location":"hpc/cpu/sumner_1/#option-2-override-existing-setup","title":"Option 2: Override existing setup","text":"<p>Here, we essentially revert from our existing custom setup to fresh HPC env that we had on the day one of login to HPC, and then we start setting up a fresh HPC env that includes conda env and other space-occupying (julia, custon apps, etc.) softwares to resolve disk quota issue.</p> <p>In order to reset to the fresh HPC env that we had on day one of login, we need to reset the entire bash login env to that of day one. Most of custom HPC env is managed by series of dot files (.bashrc, .Renviron, .Rprofile, etc.) and directories (.local, .config, etc.) in your home directory. So, in order to reset to day one env, I will archive these dot files and directories away from home directory (say ~/legacy_setup/), and replace with dot files from HPC default env that we had on the day one of login<sup>1</sup>. Please note that if you are using non-bash shell, e.g., <code>zsh</code> or other, you do need to make sure to reset login env to the HPC default bash env.</p> <p>\u2620\ufe0f Be careful moving dotfiles \u2620\ufe0f</p> <p>Moving some of dotfiles is tricky as some of those files are needed for login to sumner, e.g., files within ~/.ssh/ directory,   If you are doing this, make sure NOT to logout of sumner and at the end of executing this code block on sumner, make sure that you can login from another  terminal to sumner.</p> <ul> <li>Following script should be run manually unless you know for sure that it will exit without any error and will move all except essential dotfiles and dot directories to an archived directory. Also, make sure to check exit code by running <code>echo $?</code> immediately after running critical commands to make sure you had no error running those commands.</li> </ul> <pre><code>ssh userid@login.sumner.jax.org\ncd \"${HOME}\" &amp;&amp; \\\necho \"You are in home directory at $(pwd)\"\n\n## make an empty archived directory\nmkdir -p \"${HOME}\"/legacy_env\n\n## list files and directories that we will archive\nls -alh \"${HOME}\"/.[^.]* | tee -a \"${HOME}\"/legacy_env/list_dotfiles_dirs_\"$(date +%d%b%y_%H%M%S_%Z)\".txt\necho $?\n\n\n## list HPC env that we will archive\nenv | tee -a \"${HOME}\"/legacy_env/hpc_sumner_env_\"$(date +%d%b%y_%H%M%S_%Z)\".log\necho $?\n\n## Moving all dot files and dot directories\n## Make sure to check exit code\nmv \"${HOME}\"/.[^.]* legacy_env/\necho $?\n</code></pre> <p>Copy essential files and directories back to home directory</p> <p>Do not forget to copy back following files to \"${HOME}\" else you may get locked out of sumner. You may not have all of following files/directories but at least run each command once to ensure that essential files/directories, e.g., ~/.ssh/ are in the home directory. Ideally, confirm with your HPC staff on which files and directories are essential as HPC env may vary across institutes.</p> <pre><code>cd \"${HOME}\" &amp;&amp; \\\necho \"You are in home directory at $(pwd)\"\n\n## sumner ssh dir\nrsync -avhP legacy_env/.ssh ./\n\n## sumner login tokens, if any\ncp legacy_env/.vas_* ./\ncp legacy_env/.ksh* ./\ncp legacy_env/.k5login ./\nrsync -avhP legacy_env/.pki ./\n\n## optional files, if any\n## singularity may take a larger space \nrsync -avhP legacy_env/.singularity ./\nrsync -avhP legacy_env/.terminfo ./\nrsync -avhP legacy_env/.subversion ./\ncp legacy_env/.emacs ./\ncp legacy_env/.viminfo ./\ncp legacy_env/.screenrc ./\n</code></pre> <ul> <li>Make following empty dirs. These are unix specific configuration directories where some of softwares we install at later will keep their respective configurations. Note that we already backed up previous configurations in these directories under ~/legacy_env</li> </ul> <pre><code>cd \"${HOME}\" &amp;&amp; \\\necho \"You are in home directory at $(pwd)\"\n\nmkdir -p \"${HOME}\"/.cache\nmkdir -p \"${HOME}\"/.config\nmkdir -p \"${HOME}\"/.local\n\n## list dotfiles and dot dirs after reset\nls -alh \"${HOME}\"/.[^.]*\n</code></pre> <p>Notice that we now have a fewer (and essential) dot files and dot directories right under home directory.</p> <p>Also, notice that we are missing essential dotfiles for loading bash login env, e.g., ~/.bashrc and ~/.bash_profile. These two files should originally be present when we logged to HPC on day one. Let's copy those original files back to home directory. I am using following default bash dotfiles but it may vary across different HPC env. Contact your HPC staff for more.</p> <ul> <li>You can copy following dotfile and use text editor like <code>nano</code> or <code>vi</code> to paste contents to respective dotfiles.</li> </ul> ~/.bash_profile~/.bashrc <pre><code># .bash_profile\n\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n. ~/.bashrc\nfi\n\n# User specific environment and startup programs\n\nPATH=$PATH:$HOME/.local/bin:$HOME/bin\n\nexport PATH\n</code></pre> <pre><code># .bashrc\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\n\n# Uncomment the following line if you don't like systemctl's auto-paging feature:\n# export SYSTEMD_PAGER=\n\n# User specific aliases and functions\n# This may vary across different HPC\nmodule load gcc\n</code></pre> <ul> <li>Now, you should ensure that you can login to HPC from a separate  terminal. Do not logout from an existing terminal yet!</li> </ul> <pre><code>ssh userid@login.sumner.jax.org\nenv\n</code></pre> <ul> <li>If above command succeeds and <code>env</code> looks similar (PATH in particular) to outputs of default env variables (set by HPC staff) below, you're good! You can then exit old sumner session and install anaconda3 from a new (with a fresh or reset env) terminal session.</li> </ul> bash commandsexpected output <pre><code>## paths where all executables can be found\necho $PATH\n</code></pre> <pre><code>## paths where shared libraries are available to run programs\necho $LD_LIBRARY_PATH\n</code></pre> <pre><code>## Used by gcc before compiling program\n## Read https://stackoverflow.com/a/4250666/1243763 \necho $LIBRARY_PATH\n</code></pre> <pre><code>## default loaded modules\nmodule list\n</code></pre> <pre><code>/cm/local/apps/gcc/8.2.0/bin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.:/home/amins/.local/bin:/home/amins/bin\n</code></pre> <pre><code>/cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64:/cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64\n</code></pre> <pre><code>/cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64\n</code></pre> <pre><code>Currently Loaded Modules:\n  1) shared           3) dot                   5) slurm/18.08.8\n  2) DefaultModules   4) default-environment   6) gcc/8.2.0\n</code></pre> Know what changes <code>module load</code> command can do <p>When you load a module, it configures one or more of PATH, LD_LIBRARY_PATH, and other env variables. Command: <code>module show &lt;module name&gt;</code> can show you list of changes that a module makes during loading, e.g.,</p> <pre><code>module show gcc/8.2.0\n</code></pre> <p>In this case, loading gcc module will change PATH and LD_LIBRARY_PATH variables by perpending following respective paths. <code>module unload gcc</code> should remove these paths.</p> <pre><code>--------------------------------------------------------------------------------------------\n   /cm/local/modulefiles/gcc/8.2.0:\n--------------------------------------------------------------------------------------------\nwhatis(\"adds GNU Cross Compilers to your environment variables \")\nprepend_path(\"PATH\",\"/cm/local/apps/gcc/8.2.0/bin\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64\")\nhelp([[ Adds GNU Cross Compilers to your environment variables,\n]])\n</code></pre> <ul> <li>Make sure to logout and login to sumner again for a clean env to take an effect. Once you login, your <code>env</code> should look something similar to above. Note that PATH and LD_LIBRARY_PATH variables should default to Cent OS 7 standard paths with no user-defined paths except <code>/home/amins/.local/bin:/home/amins/bin</code> if those directories are present.</li> </ul> <p>Careful with user-defined paths</p> <p>For error-free setup, these - <code>/home/amins/.local/bin:/home/amins/bin</code> - user-defined directories should be empty to begin with and must not take precedence over system-default paths in PATH and LD_LIBRARY_PATH. Once we install anaconda3 and other tools, we will modify ~/.bash_profile and loading of bash login env such that user-defined paths override system-default paths.</p> <pre><code>exit #from sumner\n\n## login again\nssh login.sumner.jax.org\n</code></pre> <ul> <li>Store default hpc configuration</li> </ul> <p>Useful to fall back to HPC defaults if something goes awry!</p> <pre><code>mkdir -p ~/bkup/confs/hpc_default_env/\n\ncp .bashrc bkup/confs/hpc_default_env/\ncp .bash_profile bkup/confs/hpc_default_env/\n\n## export global env\nenv | tee -a \"${HOME}\"/bkup/confs/hpc_default_env/hpc_sumner_env_\"$(date +%d%b%y_%H%M%S_%Z)\".log\n</code></pre>","tags":["hpc","setup","conda","bash","programming"]},{"location":"hpc/cpu/sumner_1/#configure-defaults","title":"Configure defaults","text":"<p>Following configuration can vary across HPC env. Specifically, I prefer to keep minimal modules (tools that HPC staff installs as default) that are critical for login to HPC and compiling certain packages, e.g., CUDA libraries for Winter GPU-based HPC (detailed later in the setup).</p> <ul> <li> <p><code>dot</code> module only appends <code>.</code> to PATH variable (see <code>module show dot</code>), so that you do not need to prefix <code>./</code> to run an executable file under present/current working directory. Since I do not need <code>dot</code> module, I will override default module loading by doing <code>module unload dot</code> in my bash configuration (later). </p> </li> <li> <p>For now, I do not need system gcc and will rely on conda-installed gcc and other devtools <code>x86_64-conda_cos6-linux-gnu-*</code>. More on that later but let's unload dot and gcc first.</p> </li> </ul> <pre><code>module unload dot\nmodule unload gcc\nmodule list\n</code></pre> <pre><code>Currently Loaded Modules:\n  1) shared   2) DefaultModules   3) default-environment   4) slurm/18.08.8\n</code></pre> <ul> <li>For now, you may add following cmd to your ~/.bash_profile to unload dot and gcc at each login to HPC. Eventually it will go to ~/.profile.d/ setup detailed below.</li> </ul> <pre><code>module unload dot\nmodule unload gcc\n</code></pre> <p>Make sure that gcc is unloaded</p> <p>While settings in ~/.bash_profile should be respected during login to HPC, sometimes starting a pseudo-terminal, e.g., <code>screen</code> or <code>tmux</code> session may not source ~/.bash_profile due to directives related to interactive versus non-interactive session. Then, you may notice that <code>gcc</code> module is not unloaded and still present under <code>module list</code> output. If so, manually unload <code>gcc</code> by <code>module unload gcc</code> after each time you enter into <code>screen</code> or <code>tmux</code> session or interactive HPC session (detailed below).</p> <ul> <li>If you do not add unload command to ~/.bash_profile and rather rely on manually running unload command prior to anaconda installation, you do need ensure that these modules are unloaded in your current terminal, especially when starting a pseudo-terminal like screen, tmux, or slurm interactive job. In summary, make sure to do <code>module unload gcc</code> before running setup further.</li> </ul> <pre><code>exit #from sumner\n\n## login again\nssh login.sumner.jax.org\n</code></pre>","tags":["hpc","setup","conda","bash","programming"]},{"location":"hpc/cpu/sumner_1/#install-conda","title":"Install conda","text":"<ul> <li>Download and install anaconda3. I am using a variant of anaconda3 called, Mambaforge. It is based on Miniforge, a minimal conda installer (similar to minioconda) with an added support for conda-forge as a default channel and use of Mamba instead of default conda command to manage packages.</li> </ul> <pre><code>cd \"$HOME\" &amp;&amp; \\\nmkdir -p Downloads/conda &amp;&amp; \\\ncd Downloads/conda &amp;&amp; \\\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh &amp;&amp; \\\nmd5sum Mambaforge-Linux-x86_64.sh &gt; Mambaforge-Linux-x86_64.sh.md5\n</code></pre> <p>md5: ab95d7b4fb52c299e92b04d7dc89fa95  Mambaforge-Linux-x86_64.sh</p> <ul> <li>Prefer running setup on a dedicated interactive node instead of login node. Some of compute/memory-intensive conda install/update steps may get killed on a login node.</li> </ul> <pre><code>## start screen session prior to running interactive session\n## doing so will keep remote interactive session alive if your connection \n## from local computer to HPC is lost.\nscreen\n\n## run interactive job\n## options may vary across HPC\nsrun -p compute -q batch -N 1 -n 3 --mem 10G -t 08:00:00 --pty bash\n\n## unload gcc if loaded\nmodule unload gcc\n</code></pre> <p>You may not notice a change in login env except that your login prompt may change from a login node: <code>user@sumner-log1</code> to one of compute nodes: <code>user@sumner50</code>.</p> <ul> <li>By default, conda will setup ~/anaconda3 (or ~/mambaforge if using mambaforge) under home directory. Since conda env can grow over time and home directories are typically capped at 50 GB or so (at least with our HPC env), we will setup conda env on tier 1 space at /projects/verhaak-lab/amins/hpcenv/mambaforge</li> </ul> <pre><code>cd \"$HOME\" &amp;&amp; \\\nbash \"${HOME}\"/Downloads/conda/Mambaforge-Linux-x86_64.sh\n</code></pre> <ul> <li>Accept to license agreement and then set installation path to tier 1 space, e.g., /projects/verhaak-lab/amins/hpcenv/mambaforge in my case.<ul> <li>Note that this will vary based on your username and available location on your HPC where you can store large amount of data. Conda env and related setup can grow over time and may exceed typical 50 GB quota for a user home directory. So, prefer installing conda and related env at location where you can store more data. For JAX, it's called tier 1 space under <code>/projects/&lt;lab_name&gt;/&lt;user_name&gt;/</code> path.</li> <li>Installer will start installing conda env and towards the end, it will prompt you for initializing conda env. Say yes! If you say no, you can follow instructions that installer outputs to ensure that you have a working conda env each time you login to HPC. To do so, conda needs to write a few lines of code to ~/.bashrc file, so that HPC login env will always start with a valid (by modifying PATH and a several other <code>env</code> variables) conda env.</li> </ul> </li> </ul> <pre><code>Do you wish the installer to initialize Mambaforge\nby running conda init? [yes|no]\n[no] &gt;&gt;&gt; no\n\nYou have chosen to not have conda modify your shell scripts at all.\nTo activate conda's base environment in your current shell session:\n\neval \"$(/projects/verhaak-lab/amins/hpcenv/mambaforge/bin/conda shell.YOUR_SHELL_NAME hook)\" \n## where YOUR_SHELL_NAME is bash or zsh or other shells.\n\nTo install conda's shell functions for easier access, first activate, then:\n\nconda init\n\nIf you'd prefer that conda's base environment not be activated on startup, \n   set the auto_activate_base parameter to false: \n\nconda config --set auto_activate_base false\n\nThank you for installing Mambaforge!\n</code></pre> <ul> <li>Since I typed <code>no</code> above, I need to manually activate conda by following steps.</li> </ul> <pre><code>cd \"${HOME}\"\n\n## replace shell.bash with shell.zsh or other shells you may be using by now.\n## know which shell you are using in HPC\necho \"$(basename ${SHELL})\"\n\n## activate conda base env in the current terminal\neval \"$(/projects/verhaak-lab/amins/hpcenv/mambaforge/bin/conda shell.bash hook)\"\n</code></pre> <p>Once conda base env has been activated, you will notice your login prompt changing from <code>user@sumner50</code> to <code>(base) [userid@sumner50]</code>. You can also check which conda env you are in by running <code>echo $CONDA_DEFAULT_ENV</code>. Since we have not installed additional conda env yet, we only have base env to begin with. You can also run <code>echo $CONDA_PREFIX</code> to confirm that conda has been installed on non-default, tier 1 path and not under ~/mambaforge.</p> <ul> <li>Now, let conda edit ~/.bashrc file so conda can load base env each time we login to HPC.</li> </ul> <pre><code>conda init\n\n## if using mamba, also run mamba init\n## to enable mamba activate/deactivate env\nmamba init\n\n## Check what code has been added to ~/.bashrc\ncat ~/.bashrc\n</code></pre> <p>You will notice that conda has now added initialization code to ~/.bashrc</p> <ul> <li>Our minimal conda installation is now complete. Logout from interactive session, and then logout from HPC. </li> </ul> <pre><code># exit from interactive session\nexit\n\n## exit from HPC\nexit\n</code></pre> <p>In Part 2, we will login and start an interactive session again to customize conda and HPC env.</p> <ol> <li> <p>There were no default dot directories on the day one.\u00a0\u21a9</p> </li> </ol>","tags":["hpc","setup","conda","bash","programming"]},{"location":"hpc/cpu/sumner_2/","title":"Setting up CPU env - Part 2","text":"<p>Following up from Part 1: Initial HPC setup, we now start installing essential softwares or (in conda dictionary) packages, e.g., R, Jupyter, etc.</p> <pre><code>## login back to HPC\nssh userid@login.sumner.jax.org\n\n## start screen\nscreen\n\n## start interactive session\nsrun -p compute -q batch -N 1 -n 4 --mem 10G -t 08:00:00 --pty bash\n\n## unload gcc if loaded\nmodule unload gcc\n</code></pre> <p>\u2620\ufe0f Keep a note of walltime \u2620\ufe0f</p> <p>Make sure you do not run over alloted walltime while configuring conda env, especially when you are in the middle of installing via <code>mamba install</code> or <code>mamba update</code> command, and walltime dies out! (speaking from an experience) and that may break an ongoing conda setup.</p> <p>conda is good in a way that it locks process files for package(s) it is trying to install or update. If you are lucky, you should be able to save your ongoing setup else start again! Just read errors you may encounter during such issue and it should resolve such issue.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#configuring-conda-env","title":"Configuring conda env","text":"<p>Typically, we use <code>conda</code> command to manage all of conda env, e.g., <code>conda install package</code>, <code>conda update package</code>, <code>conda list</code>, etc. However, we will now use <code>mamba</code> instead of <code>conda</code>. Mamba is a faster drop-in replacement for <code>conda</code> to manage packages.</p> <p>Use <code>mamba</code> instead of <code>conda</code></p> <p>Moving forward, do not forget to use <code>mamba</code> instead of <code>conda</code> for all of available commands. See <code>mamba --help</code> to list available commands. At present, following commands from <code>conda</code> are supported by <code>mamba</code>: install, create, list, search, run, info and clean. For rest of commands, e.g, config, activate, deactivate, etc., use <code>conda</code> command.</p> <p>Turns out <code>mamba activate &lt;env_name&gt;</code> or <code>mamba deactivate</code> also works following one-time command: <code>mamba init</code> which will ensure sourcing mamba shell variable at the bash startup by writing a few lines towards the end of ~/.bashrc file. Please note to execute <code>mamba init</code> after <code>conda init</code> as in Part 1: Initialize conda. This will ensure at the bash startup sequence to load conda setup prior to mamba setup. If you end up running <code>mamba init</code> now (I ended up running it late in the setup), prefer activating or deactivating conda env using <code>mamba</code> and not <code>conda</code> command.</p> <p>It is important to install packages only from a single channel and not do mix-and-match install. Read more at conda-forge page on <code>channel_priority: strict</code> which is enabled as default for conda v4.6 or higher. We are using conda v4.10.3 and anaconda v2021-11. We can check that using <code>conda --version</code> and <code>conda list anaconda</code> respectively.</p> <ul> <li>Add Bioconda and conda-forge channels to get updated and compbio related packages. Do not change the order of following commands and a command with <code>--add channels conda-forge</code> must be the last one else other channels may take a priority over default <code>conda-forge</code> channel.</li> </ul> <pre><code>conda config --add channels bioconda\nconda config --add channels conda-forge\n</code></pre> <p>Above command will generate <code>~/.condarc</code> file and sets priority for channels, i.e., when same package is available from more than one channels, we prioritize installation per ordered channel list in <code>~/.condarc</code> file as below. This file should be present after above commands and no need to edit unless changing priority of channels.</p> <p>precedence of condarc file</p> <p>Since we are using Mambaforge and not anaconda3, it already ships with conda-forge as a default channel, as specified in <code>cat \"${CONDA_PREFIX}\"/.condarc</code>. Note that ~/.condarc will take precedence over default \"${CONDA_PREFIX}\"/.condarc file, so ensure that <code>conda-forge</code> is the most preferred channel (first channel) in both files.</p> <ul> <li>Besides channel priority, I have added a few other custom settings to my ~/.condarc file. For more on these settings, please refer to condarc documentation before applying to your own HPC env.</li> </ul> <p>~/.condarc is a yml format file, so take care of preceding spaces (and not tabs) before and after <code>-</code> while editing this file.</p> <pre><code>channels:\n- conda-forge\n- bioconda\nauto_update_conda: False\nalways_yes: False\nadd_pip_as_python_dependency: True\nssl_verify: True\nallow_softlinks: True\nuse_only_tar_bz2: False\nanaconda_upload: False\n\nrepodata_threads: 4\nverify_threads: 4\nexecute_threads: 4\n</code></pre> <p>Do not exceed requested resources for an interactive job</p> <p>If you increase number of threads for conda operations, make sure that you request required threads for an interactive HPC job else you may consume more resources than requested threads, and HPC workload manager may kill your interactive job - conda env can potentially break if interrupted during install or update command.</p> <ul> <li>You can verify custom set ~/.condarc configurations using <code>conda config --get</code> command. If some of key:value pairs are showing warnings saying unknown key, you should remove those entries from ~/.condarc.</li> </ul> <p>If channel priority has been set as above, you will notice <code>--add channels 'conda-forge'   # highest priority</code> from the output of <code>conda config --get</code> command.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#update-conda-package","title":"Update conda package","text":"<ul> <li>Since we used Mambaforge, anaconda3 variant from a third-pary conda-forge open-source community, let's make sure that the base <code>conda</code> package is the most up-to-date or not.</li> </ul> <pre><code>conda --version\n</code></pre> <p>conda 4.10.3</p> <ul> <li>To check if this is the current version, we will update <code>conda</code> package. </li> </ul> <pre><code>mamba update conda\n</code></pre> <p>You may notice that conda update is available else no further action needed. Notice source of updated conda package from conda-forge/linux-64. That is because we set conda-forge with the highest channel priority in ~/.condarc. </p> <pre><code>  - conda       4.10.3  py39hf3d152e_2  installed                      \n  + conda       4.11.0  py39hf3d152e_0  conda-forge/linux-64      17 MB\n</code></pre> <ul> <li>Confirm that conda package is now updated with an updated version, if any.</li> </ul> <pre><code>conda --version\n</code></pre> <p>conda 4.11.0</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#jupyterlab","title":"JupyterLab","text":"<p>For most packages, e.g., R, snakemake workflow, etc., we will use dedicated conda env and avoid installing into base env. That is to keep base env clean and without much of dependencies. Unlike base env, additional env can be recreated without a risk of breaking conda setup. However, we will require a few packages, e.g., JupyterLab and Notebook, that typically ships with regular (and not miniconda or mambaforge) anaconda3 installation.</p> Install JupyterLab in its own conda env <p>If you prefer, you can skip installing JupyterLab in base env and instead use its own dedicated env. This is perhaps a preferred way to keep base env minimal and also allows you to update JupyterLab from time to time without worrying about breaking base env. However, when you start jupyterlab session, you need to switch (activate) to the respective conda env from base or other envs.</p> <p>To install jupyterlab in its dedicated env, do following:</p> <pre><code>mamba create -c conda-forge -n jlab jupyterlab nodejs jupyterthemes jupytext dos2unix jupyter_http_over_ws jupyterlab-link-share\n\nmamba activate jlab\n\n## check installed extensions, if any\njupyter lab extension list\njupyter server extension list\n</code></pre> <p>Read install guide for extensions, if any, e.g. some extensions like jupyter_http_over_ws are not enabled by default for good (saftey) reasons.</p> <ul> <li>JupyterLab is similar to RStudio IDE and provides richer interface to several programming languages, including python, R, julia, and many more. To install jupyterlab, please read installation guide.</li> </ul> <pre><code>## core package\nmamba install -c conda-forge jupyterlab\n</code></pre> <p>Even though conda-forge is set as the highest priority channel in ~/.condarc, I am explicitly specifying to use the same channel while running install or update command.</p> <p>This will install jupyterlab and series of its dependencies. You can check version of related packages using <code>mamba list | grep -E \"jupyter\"</code> although versions may differ as they get updated over time.</p> <pre><code># packages in environment at /projects/verhaak-lab/amins/hpcenv/mambaforge:\n#\n# Name                    Version                   Build  Channel\njupyter_client            7.1.0              pyhd8ed1ab_0    conda-forge\njupyter_core              4.9.1            py39hf3d152e_1    conda-forge\njupyter_server            1.12.1             pyhd8ed1ab_0    conda-forge\njupyterlab                3.2.4              pyhd8ed1ab_0    conda-forge\njupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge\njupyterlab_server         2.8.2              pyhd8ed1ab_0    conda-forge\n</code></pre> <ul> <li>Optional: I have also installed following set of packages in the base env for my own needs or convenience for not switching to other conda environments. If you do so, try to limit installing packages in base env to bare minimum, and avoid installing packages that require multiple dependencies, e.g., R, tensorflow, node, julia, GO library, etc.</li> </ul> <pre><code>mamba install -c conda-forge git rsync vim globus-cli tmux screen\n</code></pre> <p>We will setup rest of jupyterlab settings and initialize it later after we install R in a separate conda env.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#installing-r","title":"Installing R","text":"<p>We will install R and other routinely used tools in a separate conda env for reasons explained above. You should read official documentation on installing R to familiarize with steps that I am going to follow below.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#create-a-new-env","title":"Create a new env","text":"<p>I am going to name a new env as yoda. Ideally, naming should be such that you should not have difficulty finding which env to switch to for the type of packages and analysis you may end up running at the later date, e.g., in my case, while far from ideal, I have used nomenclature based on  the Jedi members for conda env:</p> env id intended use yoda hosts all packages, including R and others that I use on daily basis luke serves as an env for background processes using databases leia a standalone env for running snakemake workflows obiwan fallback to yoda when I require to use another mature version of R or other packages windu conda env using legacy Python 2 over Python 3 anakin dev or beta env for testing: Optimized for CPU-based HPC rey Similar to yoda but optimized for GPU-based HPC ben dev or beta env for testing: Optimized for GPU-based HPC grogu toy env for everything else: for experimental purpose <p>rey and ben env are optimized for GPU-based, Winter HPC at JAX and should not be used while working in the CPU-based, Sumner HPC at JAX. However, all of CPU-based envs, e.g., yoda, luke, leia, etc. will work on both, Sumner and Winter HPCs.</p> <ul> <li>Let's create the first env, yoda and install base R package and a several essential R packages for routine analysis.</li> </ul> <pre><code>mamba create -c conda-forge -n yoda r-base r-essentials\n</code></pre> <p>Note that most up-to-date R version may be available in the conda-forge or sometimes in the other conda channels, like r or bioconda. However, it is preferable to install R from the first priority channel, i.e., conda-forge in our case.</p> <ul> <li>Activate a new env. Note that we use <code>conda</code> instead of <code>mamba</code> command as the latter (at least for now) only accepts following sub commands: install, create, list, search, run, info and clean.</li> </ul> <pre><code>conda activate yoda\n</code></pre> <p>You will notice bash prompt changing from <code>(base) [userid@sumner50]</code> to <code>(yoda) [userid@sumner50]</code>.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#pin-r-and-conda-auto-updates","title":"Pin R and conda auto-updates","text":"<ul> <li>Before moving further, let's pin R version to 4.1.1 (at the time of this writing) and also disallow conda auto-updates. That way, we have lesser chances of breaking conda env when we do <code>mamba install &lt;pkg&gt;</code> in future, and carefully install/update packages without breaking existing setup. For more on pinning packages, read official documentation.</li> </ul> <p>Technical note</p> <p>Typically, I avoid installing or updating package if <code>mamba install</code> throws a message or warning about removing or downgrading existing packages. In such cases, I fall back to compiling package using available devtools in conda. Also, I load compiled package using Modulefile when needed, and not integrate it in my default bash environment as this may give errors while running some random program due to conflicts in shared library versions.</p> <pre><code>## we already set auto update to False above\n## under ~/.condarc settings\n# conda config --set auto_update_conda False\n\n# Find package version using\nmamba list | grep -Ei 'r-base'\n</code></pre> <ul> <li>We can notice that R version is 4.1.1 (or higher). You can also check R version by <code>R --version</code>. Remember this version and add it to following newly created file:</li> </ul> <pre><code>nano \"${CONDA_PREFIX}\"/conda-meta/pinned\n</code></pre> <p>Note that <code>echo ${CONDA_PREFIX}</code> points to conda-meta/ directory under yoda and not the base env because we are within yoda env. In other words, pinned packages env specific and you can update R package in other environment(s), if present.</p> <ul> <li>Add following as a new line entry:</li> </ul> <pre><code>r-base ==4.1.1\n</code></pre> Check a valid line break <p>Since we are creating a new file and only adding a single line of text, when we save this text file, we should confirm that it is the end of the line. This is usually recognized by pressing the Enter. Unix systems recognizes such line break using an invisible <code>$</code> sign which you can confirm by running <code>cat -e \"${CONDA_PREFIX}\"/conda-meta/pinned</code></p> <pre><code>r-base ==4.1.1$\n$\n</code></pre> <p>With each line break, you will notice <code>$</code> sign, e.g., two lines in my case. You may remove a second line by editing file again but make sure to run <code>cat -e \"${CONDA_PREFIX}\"/conda-meta/pinned</code> to check a valid line break.</p> <p>You may pin only part of the major and minor version, i.e., to allow updates from 4.1.1 to 4.1.2 or 4.1.3 but not from 4.1.1 to 4.2.*. However, I rather freeze the specific version and update to builds at the later date, if need arises. To do so, remove the pinned entry from <code>\"${CONDA_PREFIX}\"/conda-meta/pinned</code> and do <code>mamba update r-base=4.1.2</code> or other version. Since R developer team updates major.minor version of R every quarter or so, I try to keep those R versions in a separate env rather updating as certain R packages may throw an error with such major updates.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#install-r-libraries","title":"Install R libraries","text":"<p>You may now install additional R libraries (or packages - it's alias!) or any other packages, e.g., git, bedtools, samtools, etc. in the new env using <code>mamba install</code> command. Just make sure that you are in the new env by <code>conda activate yoda</code>. You can check which env you are in by <code>echo \"$(basename ${CONDA_PREFIX})\"</code></p> <p>Some of packages, e.g., git, rsync, etc. are already installed in the base env. However, they may not be recognized in the new env: yoda. Ideally, you should install the same package in the new env. Conda will usually link package files (which takes much of space) from the central package directory, so installing the same package in different env should not take significant additional space.</p> <p>To search for packages, prefer using anaconda website and look for packages that are under conda-forge/ or bioconda channels, i.e., the first and second preference, respectively in our ~/.condarc file.</p> <p>Avoid installing packages from non-standard channels</p> <p>For a stable and error-free conda env, avoid installing conda packages from non-standard channels, i.e., a channel other than conda-forge and bioconda or ones specified in ~/.condarc file. Installing packages from non-standard channels will unnecessarily increase complexity of package dependencies in conda env and will increase likelihood of slowing or breaking down one or more conda env. Note that we have yet to create additional conda env besides a default base env.</p> <ul> <li>Install R packages: You can tie up all of your packages in a single command or break it down to smaller chunks. The former appraoch may take longer and difficult to debug if package installation fails due to conflicting dependencies with one or more packages.</li> </ul> <pre><code>mamba install -c conda-forge r-tidyverse r-tidymodels r-devtools r-biocmanager\nmamba install -c conda-forge gnupg git rsync vim openjdk r-rjava\nmamba install -c conda-forge bedtools pybedtools bedops\nmamba install -c conda-forge matplotlib scikit-learn\n</code></pre> <ul> <li>Reticulate and rpy2 packages will allow us to use R and python interchangeably in the same R script or python notebook, respectively! Read details about reticulate on RStudio website and rpy2 here.</li> </ul>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#reticulate-r-package","title":"reticulate R package","text":"<pre><code>mamba install -c conda-forge r-reticulate\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#rpy2-python-package","title":"rpy2 python package","text":"<pre><code>mamba install -c conda-forge rpy2\n</code></pre> <p>If above command works without forcing you to downgrade python, R, or other major packages, good for you! If not, it is due to strict requirements of rpy2 package which can conflict with other core packages in yoda env. So, <code>mamba install -c conda-forge rpy2</code> may not work.</p> <p>\u2620\ufe0f Beware of using <code>pip install</code> in conda env \u2620\ufe0f</p> <p>You should be comfortable compiling and installing packages using <code>pip install</code> and knowing how to manually install python package requirements. If not, it is safer to skip installing rpy2 (or any other) package by steps detailed below. Wait until conda-forge developers make rpy2 package available. conda-forge developer community is pretty good and active, so patience should pay off then risking to break an otherwise functioning yoda env!</p> <ul> <li>From rpy2 setup.py and requirements.txt file, list number of packages required as dependencies for rpy2.</li> </ul> <p>If we do <code>pip install rpy2</code>, pip will automatically install these requirements. However, I prefer to let conda manage all package versions because pip may not necessarily evaluate if certain package versions are compatible with other conda-installed packages. That's why I prefer installing requirements by myself using <code>mamba install</code> and then do <code>pip install</code>, so only minimal set of requirements will be installed by pip command. That way, I can minimize chances of breaking conda env due to conflicting package dependencies.</p> <pre><code>## required packages by rpy2\n## see how many of these are already installed\nconda list | grep -E \"cffi|pytest|pandas|numpy|jinja2|pytz|tzlocal\"\n</code></pre> <ul> <li>Install missing packages using <code>mamba</code>.</li> </ul> <pre><code>## ok to write packages which are already installed\n## conda will take care of version conflict, if any.\nmamba install -c conda-forge cffi pytest pandas numpy jinja2 pytz tzlocal\n</code></pre> <ul> <li>Now try (fingers crossed!) installing rpy2 using pip. Make sure that rpy2 pip website is showing same version (3.4.5 in my case) as on rpy2 github website else you need to download source file from pip website, and check respective setup.py and requirements.txt file to ensure dependencies are identical and satisfied or installed via <code>mamba install</code> command.</li> </ul> <pre><code>## in yoda env\nmkdir - ~/logs &amp;&amp; \\\npip install rpy2 |&amp; tee -a logs/pip_install_rpy2.log\n</code></pre> Success installing rpy2 <p>Great! <code>pip install</code> did not end up installing any dependencies (as we already installed those using <code>mamba install</code>), and rpy2 is now successfully installed.</p> <pre><code>Collecting rpy2\n  Downloading rpy2-3.4.5.tar.gz (194 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: cffi&gt;=1.10.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (1.15.0)\nRequirement already satisfied: jinja2 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (3.0.3)\nRequirement already satisfied: pytz in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (2021.3)\nRequirement already satisfied: tzlocal in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (4.1)\nRequirement already satisfied: pycparser in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from cffi&gt;=1.10.0-&gt;rpy2) (2.21)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from jinja2-&gt;rpy2) (2.0.1)\nRequirement already satisfied: pytz-deprecation-shim in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from tzlocal-&gt;rpy2) (0.1.0.post0)\nRequirement already satisfied: tzdata in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from pytz-deprecation-shim-&gt;tzlocal-&gt;rpy2) (2021.5)\nBuilding wheels for collected packages: rpy2\n  Building wheel for rpy2 (setup.py): started\n  Building wheel for rpy2 (setup.py): finished with status 'done'\n  Created wheel for rpy2: filename=rpy2-3.4.5-cp310-cp310-linux_x86_64.whl size=300939 sha256=136144e165e2b0f156fdb7f525547ae073dda06346a893de398aa6836e625af1\n  Stored in directory: /home/amins/.cache/pip/wheels/ba/d8/8b/68fc240578a71188d0ca04b6fe8a58053fbcbcfbe2a3cbad12\nSuccessfully built rpy2\nInstalling collected packages: rpy2\nSuccessfully installed rpy2-3.4.5\n</code></pre> <ul> <li>Since we just installed JAVA (openjdk) and rJava R library, for sanity check, run <code>R CMD javareconf</code> (to print and update java configuration for R) and <code>echo $JAVA_HOME</code> (to confirm that bash variable is correctly set).</li> <li>If you also have installed git, you can copy boilerplate  ~/.gitconfig, and make changes under line starting with name, email, and excludesfile. Make sure to read about gitconfig too.</li> </ul> <p>Best practices using <code>mamba install</code> or <code>mamba update</code></p> <p>Keep a habit of checking following when using <code>mamba install</code> command:</p> <ul> <li> Prefer conda-forge channel, followed by bioconda, and avoid installing packages from other channels. You can do so in dev or beta env, e.g., env grogu in my case to see how it pans out.</li> <li> Before hitting <code>Y/Yes</code> to install packages, ensure that installing or updating packages does not force downgrading of one or more major packages, like python, R, and any other packages that you deem it as major package in your routine analysis and downgrading it may break reproducibility of your analysis. You can always use dev env, like grogu to play packages showing such downgrade warnings.</li> </ul> <p>While doing <code>mamba install -c conda-forge samtools bcftools htslib</code>, I noticed <code>openssl</code> dependency being downgraded from v3.0.0 to v1.1.1. I find it a major downgrade to one of the core package and so I skipped installing samtools and  related packages using <code>mamba</code>. I would rather install these packages by compiling from their respective source tar balls. You can read installation details for samtools family packages at author's website.</p> <p>Similarly for <code>mamba install -c conda-forge bedtools pybedtools bedops</code>, I got following error as yoda env is using python 3.10 and it is pinned by default. So, it can not be downgraded! Similar to samtools, I will compile pybedtools or use a separate env, like grogu for such packages with unique requirements! For <code>bedops</code>, it was asking me to install older version of samtools which I was not ok with. I will rather compile those tools from the current version. So, finally I ended up installing only bedtools with clean requirements! <code>mamba install -c conda-forge bedtools</code>.</p> <pre><code>package pybedtools-0.8.2-py39h39abbe0_1 requires python &gt;=3.9,&lt;3.10.0a0, but none of the providers can be installed\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#setup-rprofile-and-renviron","title":"Setup Rprofile and Renviron","text":"<p>Rprofile and Renviron files provide additional configuration option for R, similar to ~/.condarc file to manage several conda-related configurations. You can read more about these files at R developer website and RStudio website. For detailed notes, Jennifer Bryan and Jim Hester has written an excellent resources, titled What they forgot to teach you about R with a chapter on R Startup.</p> <ul> <li>Setup R library directory path for R 3.6</li> </ul> <p>Before setting up R startup, I will make a dedicated package directory that will store R libraries or packages. By default, R will store all libraries at conda env specific path, i.e., for my case, it is at \"${CONDA_PREFIX}\"/lib/R/library/. You can check this path using <code>Rscript -e '.libPaths()'</code>. This default path is ideally intended for R libraries managed via <code>mamba install</code> or <code>mamba update</code> command. However, I also compile R libraries using <code>install.packages()</code> R command when I find conflicting dependencies in installing R libraries using <code>mamba install</code> command. In such cases, I prefer to use a separate R library directory than a default R library path.</p> <p>I am making an empty library directory for installing libraries or R packages that I may compile using R <code>install.packages()</code> command. R usually defaults to making such user-package directory in the user's home path but I will use tier 1 space again to avoid filling up my home directory with a limited quota.</p> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/4.1\n</code></pre> <ul> <li>Create ~/.Renviron file</li> </ul> <pre><code>nano ~/.Renviron\n</code></pre> <p>Add following to ~/.Renviron. Notice the order in which R will store newly compiled packages. It will use an empty directory we just created to store new packages. If for some reasons, this directory is not accessible (file permission errors), R will fallback to the second path, and so on. The second path is an expanded path of a default R package directory, i.e., output of <code>echo _\"${CONDA_PREFIX}\"/lib/R/library/_</code> command.</p> <pre><code>R_LIBS=\"/projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/4.1:/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/library\"\n</code></pre> <p>You can confirm precedence of R library paths in R using <code>.libPaths()</code> R command or from bash using <code>Rscript -e '.libPaths()'</code>.</p> <ul> <li>Create ~/.Rprofile file</li> </ul> <pre><code>nano ~/.Rprofile\n</code></pre> <p>Add following contents. Please read more about configuring R startup from links shared earlier in this section. Comment out lines using <code>#</code> if you do not require one or more of following options.</p> <pre><code>## set user specific env variables, e.g., GITHUB_PAT here\nSys.setenv(\"GITHUB_PAT\"=\"my_github_secret_token\")\n\n## Default source to download packages\nlocal({\nr &lt;- getOption(\"repos\")\nr[\"CRAN\"] &lt;- \"https://cran.rstudio.com\"\noptions(repos = r)\n})\n</code></pre> Protect secret tokens, passwords, etc. <p>If your ~/.Rprofile file contains any secret tokens, it's a best practice to make it read/write only by file owner (you) using <code>chmod 600 ~/.Rprofile</code>. Same goes for similar files in your home directory, e.g., .gitconfig, .netrc, etc. Equivalent command for directory, e.g., for .ssh/ is <code>chmod 700 ~/.ssh</code>. Read more about linux file permissions.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#compiling-r-libraries","title":"Compiling R libraries","text":"<p>Since we already have setup yoda env for R, you can now install additional R libraries using <code>mamba install r-PackageName</code> (see above) as long as it is available in anaconda repository, preferably in conda-forge or bioconda sources AND it does not significantly downgrades essential packages like python, R, and other core libraries, e.g., zlib, openssl, etc. The latter is subjective and depends on what you will consider as an essential. In any case where I have doubts of breaking conda env, I fall back to compiling R library using native R command: <code>install.packages()</code>. However, I am going to wait using this command until I have finished my HPC setup, specifically, bash startup using ~/.profile.d configuration. We are not there yet but not far from it too!</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#install-essentials","title":"Install essentials","text":"<p>This can again vary per user's need and optional. If you find errors compiling packages, you may end up installing respective dev libraries, e.g., libiconv, zlib, etc. if they are not already installed in the current (yoda in this case) conda env.</p> <pre><code>mamba install -c conda-forge wget curl rsync libiconv parallel ipyparallel\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#setup-jupyterlab","title":"Setup JupyterLab","text":"<p>We have installed JupyterLab earlier but did not finish complete setup. Let's do that!</p> <p>In this section, I will setup jupyter notebook and kernels to launch python, R, and bash based notebooks. I will also harden notebook server with several security settings.</p> <p>Remember that we will be using JupyterLab installation from the conda base env even though JupyterLab could have been installed in yoda and other conda env. Managing jupyterlab server from base env is convenient and allows us to interact with all other env. Also, if we end up resetting or deleting other env, jupyter configuration in base env will remain intact. </p> <p>First, we install language-specific kernels. By default, jupyterlab ships with python kernel, named ipykernel which is backend when we interact with jupyter python notebook. Jupyter can allow us to use other language-specific kernels to interact with R, Julia, bash, and many more languages. Accordingly, I will install kernels for R and bash in specific envs, e.g., yoda and then switch back to base env to configure jupyterlab, such that we can connect to kernels in yoda env from conda base env.</p> <ul> <li>Make sure you are in yoda env. We will switch to base env after we configure all kernels in yoda env.</li> </ul> <pre><code>conda activate yoda\n</code></pre> <ul> <li>Install kernels</li> </ul> <pre><code>mamba install -c conda-forge ipykernel r-irkernel bash_kernel\n</code></pre> <p>Note that I have also installed python kernel, ipykernel in yoda env as we may not have jupyterlab installed in yoda as we only need to install jupyterlab in the base env.</p> <p>Also, some of kernels may already have been downloaded as part of dependency for other packages we installed earlier. You can check <code>conda list | grep kernel</code> output to confirm which kernels are already installed.</p> <p>Now we can link each of these kernels to jupyterlab in base env. </p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#python-kernel","title":"python kernel","text":"<pre><code>python -m ipykernel install --user --name yoda_py310 --display-name \"yoda_py310\"\n# confirm that installation exited without any error\necho $?\n# this should return 0 for successful installation\n</code></pre> <p>--name and --display-name will show up as kernel file location at ~/.local/share/jupyter/kernels/ and icon name in the jupyterlab launcher page, respectively. You can name as you like but without any spaces or special characters. I am following naming format that uses env followed by language and its major and minor version.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#r-kernel","title":"R kernel","text":"<ul> <li>Start R session</li> </ul> <pre><code>R\n</code></pre> <ul> <li>Install kernel while in yoda env</li> </ul> <p>Read available options for IRkernel installation. Also, consider installing jupytertext-text-shortcuts but not now and we can install this along with other extensions towards the end of configuring jupyterlab.</p> <pre><code>library(IRkernel)\ninstallspec(name = \"yoda_r41\", displayname = \"yoda_r41\", user = TRUE)\n\n## quit R session\nq(save = \"no\")\n</code></pre> <p>Similar to python kernel, r kernel should now be at ~/.local/share/jupyter/kernels/.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#bash-kernel","title":"bash kernel","text":"<p>We will now install bash kernel in yoda env. Unlike other (R and python) kernels, bash kernel do not need to be installed in all of conda env because we can always switch between conda env specific bash env using <code>mamba activate anakin</code> or <code>mamba activate rey</code>, etc.</p> <p>Here, I will install bash kernel in yoda and not base env as I intend to use yoda as my primary go-to env when I login to HPC.</p> <p>Keep use of base env to bare minimum</p> <p>Please remember that we keep use of base to bare minimum for maintaining core of codna packages, and should avoid installing (and populating dependencies) packages in base env. You can always delete secondary conda env and restart but you cannot do so with base env!</p> <pre><code>## in yoda env\nmamba list | grep -E \"bash_kernel\"\n## if this does not show bash_kernel installed, redo install\n# mamba install -c conda-forge ipykernel r-irkernel bash_kernel\n## and the install kernel into jupyter env\npython -m bash_kernel.install\n</code></pre> <ul> <li>Unlike python and R kernels, I could not find overriding default name and display_name for bash kernel. So, I will rename bash kernel manually else if I end up installing similar kernel from other env, it will override kernel with the same default name: bash. Typically, you do not need to install bash kernels in all conda env as jupyterlab ships with a powerful terminal that allows switching from one to other env using same <code>conda activate</code> command. So, I hardly use bash kernel.</li> </ul> <pre><code>## go to kernel base dir\ncd ~/.local/share/jupyter/kernels/\n\n## rename bash dir to yoda_bash\nmv bash yoda_bash\n\n## edit bash kernel.json to rename display name\ncd yoda_bash\nnano kernel.json\n</code></pre> <p>Rename <code>\"display_name\": \"Bash\"</code> to <code>\"display_name\": \"yoda_bash\"</code></p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#kernel-loading","title":"kernel loading","text":"<p>Following completion of the entire setup, we are going to run JupyterLab from the base env. However, on daily basis, we like to access Python and R from yoda and not base<sup>1</sup>. Default kernel setup above should let jupyterlab handle conda env specific python but not so for other kernels.</p> <p>However, I have noticed issues running Python and R from a non base conda env as sometimes packages requiring shared libraries may throw an error as such shared libraries are either missing in base env or have a different version than one in the current env, i.e., yoda env where package was originally installed or compiled.</p> <p>I mitigate such issues by loading a valid bash env prior to initializing kernel, e.g., I will wrap a default jupyter kernel settings into a bash script (wrapper) and will activate a valid conda env, e.g., yoda in this case prior to initializing yoda specific Python or R kernel. That way, kernel will consistently inherit a valid login (bash) env for the respective conda env.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#yoda-python","title":"yoda python","text":"<ul> <li>Create a new kernel wrapper matching name of kernel we like to edit, e.g., /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310</li> </ul> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/kernels\ntouch /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310\n\n# make file executable\nchmod 700 /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310\n</code></pre> <ul> <li>Add following to wrap_yoda_py310 file. Change user paths where applicable.</li> </ul> <pre><code>#!/bin/bash\n\n## Load env before loading jupyter kernel\n## @sbamin\n\n## https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425\n\n#### Activate CONDA in subshell ####\n## Read https://github.com/conda/conda/issues/7980\n# I am using conda instead of mamba to activate env\n# as somehow I notices warnings/errors sourcing\n# mamba.sh in sub-shells.\nCONDA_BASE=$(conda info --base) &amp;&amp; \\\nsource \"${CONDA_BASE}\"/etc/profile.d/conda.sh &amp;&amp; \\\nconda activate yoda\n#### END CONDA SETUP ####\n\n# this is the critical part, and should be at the end of your script:\nexec /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/bin/python -m ipykernel_launcher \"$@\"\n\n## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/&lt;kernel_name&gt;/kernel.json\n\n#_end_\n</code></pre> <ul> <li>Now, adjust kernel settings.</li> </ul> <pre><code>## go to kernel base dir\ncd ~/.local/share/jupyter/kernels/\n\n## there should be yoda_py310 directory or\n## one matching --name yoda_py310 argument\n## we used above when installing python kernel\ncd yoda_py310\n\n## edit kernel.json\nnano kernel.json\n</code></pre> <ul> <li>Replace contents of kernel.json with following:</li> </ul> <pre><code>{\n\"argv\": [\n\"/projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310\",\n\"-f\",\n\"{connection_file}\"\n],\n\"display_name\": \"yoda_py310\",\n\"language\": \"python\",\n\"metadata\": {\n\"debugger\": true\n}\n}\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#yoda-r","title":"yoda R","text":"<p>Now, we can reconfigure R kernel for yoda same as above but with a few changes in the wrapper script.</p> <ul> <li>Create a new kernel wrapper for R, e.g., /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41</li> </ul> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/kernels\ntouch /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41\n\n# make file executable\nchmod 700 /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41\n</code></pre> <ul> <li>Add following to wrap_yoda_r41 file. Change user paths where applicable.</li> </ul> <pre><code>#!/bin/bash\n\n## Load env before loading jupyter kernel @sbamin https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425\n\n#### Activate CONDA in subshell ####\n## Read https://github.com/conda/conda/issues/7980\nCONDA_BASE=$(conda info --base) &amp;&amp; \\\nsource \"${CONDA_BASE}\"/etc/profile.d/conda.sh &amp;&amp; \\\nconda activate yoda\n#### END CONDA SETUP ####\n\n## this is the critical part, and should be at the end of your script:\n## path to R and arguments come from original kernel.json under\n## ~/.local/share/jupyter/kernels/yoda_r41/ directory.\n\n## In some cases, path to R may differ and may originate from\n## .../envs/yoda/lib64/R/bin/R instead of .../envs/rey/lib64/R/bin/R\n\n## If so, adjust path to R here accordingly.\nexec /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/bin/R --slave -e \"IRkernel::main()\" --args \"$@\"\n\n## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/&lt;kernel_name&gt;/kernel.json\n\n#_end_\n</code></pre> <ul> <li>Now, adjust kernel settings.</li> </ul> <pre><code>## go to kernel base dir\ncd ~/.local/share/jupyter/kernels/\n\n## there should be yoda_py310 directory or\n## one matching --name yoda_py310 argument\n## we used above when installing python kernel\ncd yoda_r41\n\n## edit kernel.json\nnano kernel.json\n</code></pre> <ul> <li>Replace contents of kernel.json with following:</li> </ul> <pre><code>{\n\"argv\": [\n\"/projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41\",\n\"{connection_file}\"\n],\n\"display_name\": \"yoda_r41\",\n\"language\": \"R\"\n}\n</code></pre> <p>Done! Next time you run jupyter, you should have a new julia kernel in JupyterLab.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#configure-jupyterlab","title":"Configure JupyterLab","text":"<p>Once we have installed env specific kernels as in yoda (and other envs, if a ny), now it's a time configure JupyterLab in the base env.</p> <pre><code>## deactivate yoda env\nconda deactivate\n</code></pre> <p>You should now be in conda base env</p> <p>If you were jumping across more than one conda envs, then each instance of <code>conda deactivate</code> command will bring you back to previously active env. So, make sure to return to base env which you can confirm using <code>echo $CONDA_PREFIX</code> output. That should point to base path of conda (mambaforge in my case) installation: /projects/verhaak-lab/amins/hpcenv/mambaforge/. Also, notice change in bash prompt to <code>(base) userid@sumner50</code>.</p> <ul> <li>Once in the base env, generate skeleton for default jupyter configuration.</li> </ul> <pre><code>## return to home dir\ncd \"${HOME}\" &amp;&amp; \\\njupyter server --generate-config\n</code></pre> <p>Writing default config to: /home/userid/.jupyter/jupyter_notebook_config.py</p> <p>Secure Jupyter Server</p> <p>It is critical that you harden security of jupyterlab server. Default configuration is not good enough (in my view) for launching notebook server over HPC, especially without SSL (or https) support. Setting up individual security steps is beyond scope of this documentation. However, I strongly recommend reading official documentation on running a public Jupyter Server and security in the jupyter server.</p> <p>Checkout this example guide on creating self-signed SSL certificates in case you do not have SSL certificates from Research IT department.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#self-signed-ssl-certificates","title":"self-signed SSL certificates","text":"<p>Ideally you should have SSL certificates signed by a verified certificate authority (CA) else most of modern web browsers will issue a warning or an error regarding SSL secutity. CA-signed SSLs are typically paid unless issued via let's encrypt or your DNS providers.</p> <p>Securing a website (Jupyter Env in our case) is beyond scope of this guide but I will suggest to inquire to your research IT department to see if they can help secure your Jupyter env. It's better to have CA-signed SSL over self-signed SSL, and always better to have SSL (https) over insecure (http) connection to your Jupyter env. This is true even when you are working within a secure firewall of your work network.</p> <p>Here, I am creating a self-signed SSL which most of you can generate and at least have self-signed SSL. See this post for details.</p> <pre><code>## switch to base conda env\nmamba activate base\n\n## create dir to save SSL certificates\nmkdir ~/ssl_cert &amp;&amp; \\\nchmod 700 ~/ssl_cert &amp;&amp; \\\ncd ~/ssl_cert\n\n## generate a private key\nopenssl genrsa -out hpc_cert.key 2048\n\n## secure your private key\nchmod 600 hpc_cert.key\n\n## create a signed certificate using a key we generated above\nopenssl req -new -key hpc_cert.key -out hpc_cert.csr\n\n## finally self-sign this certicate\nopenssl x509 -req -days 365 -in hpc_cert.csr -signkey hpc_cert.key -out hpc_cert.pem\n</code></pre> <p>While creating a signed certificate, hpc_cert.csr, you will be prompted to enter issuer's country, city, etc., including Common Name (CN). You should put some info related to use of this certificate into respective fields, e.g., Common Name can be <code>hpcjupyter</code>.</p> <p>SSL certificate - unable to verify SSL connection</p> <p>Regardless of what you put in CN, say <code>hpcjupyter.mywebsite.com</code>, most of modern web browsers so either SSL warning or error of not verifying your self-signed SSL. This is because you have self-signed this SSL and not using an approved certificate authority (CA) provider for signing your SSL. Read this post for details. In essense, your self-signed SSL is acting as it is an authentic SSL for a CN you provided above, <code>hpcjupyter.mywebsite.com</code>. Since that's a clearly a security risk, most modern browsers will throw a warning before you are allowed to visit a site (Jupyter server in this case) or even may not allow at all to load that website! Hence, prefer asking your Research IT to make a signed SSL available for a specific intranet subdomain, e.g., <code>jupyter.company.com</code> for users to use.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#cookie-file","title":"cookie file","text":"<ul> <li>After creating SSL certificates, also create a cookie file for jupyter server.</li> </ul> <pre><code>openssl rand -hex 32 &gt; /home/foo/ssl_cert/jp_cookie\nchmod 600 /home/foo/ssl_cert/jp_cookie\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#config-file","title":"config file","text":"<ul> <li>Example config for /home/userid/.jupyter/jupyter_server_config.py. Please do not copy and paste these options without knowing underlying details.</li> </ul> <p>Jupyter token - provide a strong token string</p> <p>While editing jupyter config file below, please read inline comments carefully, especially for <code>c.ServerApp.token</code>. Do not use the default token as that token is used for login to your running jupyter server. Provide a secure (a longer string, 32 characters or more) string generated using <code>uuidgen</code> and after removing dashes.</p> <p>Do not worry of remembering this token. Jupyter does allow you to have an alternate way of login using a custom, user-generated password (see further below).</p> <pre><code>## leave commented out portion of default config as it is.\n## then you can add your custom config\n## Do not copy these configurations without knowing what they do!\n\n#### NOTEBOOK CONFIGS ####\n## SSL settings\n## read documentation for details\nc.ServerApp.certfile = u'/home/foo/ssl_cert/hpc_cert.csr'\nc.ServerApp.keyfile = u'/home/foo/ssl_cert/hpc_cert.key'\n\nc.JupyterHub.cookie_secret_file = '/home/foo/ssl_cert/jp_cookie'\nc.ServerApp.open_browser = False\n\n## token used to programmatically login to jupyter,\n## e.g., via VS Code Jupyter extension.\n## This is essentially a password to login to jupyter\n## Provide an alphanumeric secret string - longer the better.\n## you can create one using uuidgen command: Remove dashes.\nc.ServerApp.token = 'PLEASE_REPLACE_THIS_TOKEN_c8syb2lnd89g2fosyyfskdfy02h'\nc.ServerApp.allow_password_change = False\n\n## should be set to False\n## unsafe to set True from https security point of view\nc.ServerApp.disable_check_xsrf = False\n\n## use one of available options: See documentation\nc.Application.log_level = 'INFO'\n\n## use login shell\nc.ServerApp.terminado_settings={'shell_command':['bash', '-l']}\n## END ##\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#jupyter-password","title":"jupyter password","text":"<ul> <li> <p>Once you customize config file above, make sure to generate a secret and strong password using <code>jupyter server password</code> command. Your password then will be written in an encrypted format in /home/userid/.jupyter/jupyter_server_config.json file.</p> </li> <li> <p>Make both files read/write-only by you.</p> </li> </ul> <pre><code>## for directory, we use permission 700\nchmod 700 ~/.jupyter\n\n# For files, we use permission 600\nchmod 600 ~/.jupyter/jupyter_server_config.py\nchmod 600 ~/.jupyter/jupyter_server_config.json\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#customizing-user-interface","title":"Customizing user interface","text":"<p>If you need to add custom themes, fonts, shortcuts, etc. for Jupyter, you may follow this section, else safe to skip to Start Jupyter section.</p> <p>Before installing themes or customizing jupyterlab further, I will install node js package to base env.</p> <p>Ideally, base env should not be cluttered with packages except bare mininmum that comes with original conda installation (mambaforge in my case). However, node js is required to setup and manage jupyterlab extensions and how jupyterhub can interact with a few kernels, e.g., <code>jupyterlab-sql</code> extension to interact with sql databases that I will end up installing in the future.</p> <pre><code>mamba install -c conda-forge nodejs\nnpm --version\n</code></pre> <p>using v8.1.2</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#themes","title":"Themes","text":"<p>Themes provide custom user interface and is optional for setup. See example themes at dunovank/jupyter-themes</p> <pre><code>mamba install -c conda-forge jupyterthemes\n</code></pre> <p>Note: This may downgrade node js. Since I do not use node js in base and it is installed in base to manage jupyter extensions, I was ok downgrading it as it changed only a build and not major or minor version.</p> <ul> <li>setup theme, see details here</li> </ul> <pre><code>jt -t solarizedl -T -N -f firacode -fs 12 -tf ptserif -tfs 11 -nf ptsans -nfs 12 -dfs 11 -ofs 10 -cellw 90% -lineh 170\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#keyboard-shortcuts","title":"keyboard shortcuts","text":"<p>If you are familiar with RStudio shortcuts for R pipe <code>%&gt;%</code> and assignment <code>&lt;-</code> operator, you can enable those in JupyterLab too<sup>2</sup> by first starting a jupyterlab session. You can then go to <code>Advanced Settings Editor</code> either by pressing Cmd+, on a mac or go to <code>Settings</code> from a top menubar, and then clicking <code>Keyboard Shortcuts</code> option. There, under <code>User Preferences</code> pane, you can paste following to enable keyboard shortcuts, i.e., Alt+- for <code>&lt;-</code> and Shift+Cmd+M for <code>%&gt;%</code> operator.</p> <pre><code>{\n    \"shortcuts\": [\n        {\n            \"command\": \"apputils:run-first-enabled\",\n            \"selector\": \"body\",\n            \"keys\": [\"Alt -\"],\n            \"args\": {\n                \"commands\": [\n                    \"console:replace-selection\",\n                    \"fileeditor:replace-selection\",\n                    \"notebook:replace-selection\",\n                ],\n                \"args\": {\"text\": \" &lt;- \"}\n            }\n        },\n        {\n            \"command\": \"apputils:run-first-enabled\",\n            \"selector\": \"body\",\n            \"keys\": [\"Accel Shift M\"],\n            \"args\": {\n                \"commands\": [\n                    \"console:replace-selection\",\n                    \"fileeditor:replace-selection\",\n                    \"notebook:replace-selection\",\n                ],\n                \"args\": {\"text\": \" %&gt;% \"}\n            }\n        }\n    ]\n}\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#gpg-signatures","title":"gpg signatures","text":"<p>Import gpg keys, if any for code signing. More at https://unix.stackexchange.com/a/392355/28675</p> <p>Earlier I installed required gpg packages, gpg and python-gnupg but they ended up conflicting with <code>gpg-agent</code> that is running by the system gpg at <code>/usr/bin/gpg</code>. So, I have to remove both conda packages in order to use system gpg at <code>/usr/bin/</code>.</p> <pre><code>mamba remove -c conda-forge gnupg python-gnupg\n</code></pre> <p>Removing packages using <code>mamba remove</code></p> <p>This also removed several other packages which were required by gpg packages but not by other packages still present in conda env. However, over time, you may end up compiling softwares outside of conda env but still using certain dependencies installed via conda. If so, be careful running <code>mamba remove</code> command as it can not check dependencies for softwares installed outside conda env, and removing packages like below may break your compiled tools.</p> <pre><code>  - gnupg           2.3.3  h7853c96_0     installed         \n  - libassuan       2.5.5  h9c3ff4c_0     installed         \n  - libgcrypt       1.9.4  h7f98852_0     installed         \n  - libgpg-error     1.42  h9c3ff4c_0     installed         \n  - libksba         1.3.5  hf484d3e_1000  installed         \n  - npth              1.6  hf484d3e_1000  installed         \n  - ntbtls          0.1.2  hdbcaa40_1000  installed         \n  - python-gnupg    0.4.8  pyhd8ed1ab_0   installed \n</code></pre> <p>Careful with gpg command</p> <p>For code signing, you do not need private keys and public keys works ok. Make sure to check gpg documentation before running these commands. Incorrect use may expose your private keys (worst if you push incorrect keys to a public gpg server!) and defeats the purpose of encryption.</p> <pre><code>## list public keys, if any\n## This will setup ~/.gnupg dir if running command for the first time\n## Do chmod 700 ~/.gnupg in case dir perm are not correctly set\ngpg --list-keys\n\ngpg --allow-secret-key-import --import private_public.key\n\n## list public keys\ngpg --list-keys\n\n## set trust level\n## set a valid trust level after reading documentation\ngpg --edit-key {KEY} trust quit\n\n## list secret keys\ngpg --list-secret-keys\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#rmate","title":"rmate","text":"<p>I user <code>rmate</code> command to open remote files on HPC in the text editor like Atom or SublimeText on my macbook.</p> <ul> <li>Prefer installing standalone binary over ruby-based (<code>gem install rmate</code>) command. If you prefer ruby based installation, better to add ruby installation in a separate conda env, e.g., in luke or other backend env.</li> </ul> <pre><code># in base env\n\n## download standalone binary and save as rmate in ~/bin/\nmkdir -p ~/bin\ncurl -Lo ~/bin/rmate https://raw.githubusercontent.com/textmate/rmate/master/bin/rmate\nchmod 700 ~/bin/rmate\n</code></pre> <p>Read usage instructions for more on using <code>rmate</code> command.</p>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#backup-conda-env","title":"Backup conda env","text":"<p>Let's backup conda setup we have done so far. I will backup configurations for each of conda env we created above. I have created a small wrapper,  conda_bkup.sh - using <code>conda env export</code> and <code>conda list</code> commands - to backup conda env. </p> <ul> <li>Base or root env</li> </ul> <p>~/conda_env_bkup/sumner/base// <pre><code>conda_bkup.sh\n</code></pre> <ul> <li>dev env</li> </ul> <p>~/conda_env_bkup/sumner/yoda// <pre><code>conda activate yoda &amp;&amp; \\\nconda_bkup.sh\n\n## return to base env\nconda deactivate\n</code></pre>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_2/#start-jupyterlab","title":"Start JupyterLab","text":"<ul> <li>Optional: Install several tools in base env.</li> </ul> <pre><code># I use dos2unix often to fix line endings for\n# files created from windows (dos2unix) or mac (mac2unix)\n\n## A few other packages for jupyterlab extensions\nmamba install -c conda-forge jupyter_http_over_ws jupyterlab-link-share dos2unix\n\n## check for successful install\necho $?\n\n# Run only if you have installed jupyter_http_over_ws AND\n# you are familiar with managing jupyter server backend.\n# jupyter server extension enable --py jupyter_http_over_ws\n</code></pre> <ul> <li>Test jupyterlab run.</li> </ul> <p>\u2620\ufe0f Use SSL and password protection \u2620\ufe0f</p> <p>Avoid running notebook server without SSL and proper password and token configuration as detailed above) else you may encounter a significant data security risk.</p> <pre><code>mkdir -p ~/tmp/jupyter/sumner\n\n## capture LAN IP for a login or compute node\n## https://stackoverflow.com/a/3232433\nREMOTEIP=\"$(hostname -I | head -n1 | xargs)\"\n\n## test run from a login or compute node\n## SSL related settings will be inherited from jupyter config file that \n## we already have created as above.\njupyter lab --no-browser --ip=\"${REMOTEIP}\" |&amp; tee -a ~/tmp/jupyter/sumner/runtime.log\n</code></pre> <ul> <li>Once a jupyter session begins and assuming you are on a secure local area network, you can open URL: <code>https://&lt;REMOTEIP&gt;:&lt;PORT&gt;/lab</code> to launch jupyter lab. Here, <code>&lt;PORT&gt;</code> is randomly assigned when you start a server and URL will be displayed on the terminal or in a log file at ~/tmp/jupyter/sumner/runtime.log.</li> </ul> <p>Run jupyterlab from a compute and not login node</p> <p>Avoid running JupyterLab server on a login node. It will most likely be killed by HPC admins. For longer running and compute-intensive jupyterlab sessions, it is preferable to run jupyterlab from a compute and not a login node. This requires series of secure port forwarding which is beyond the scope of current documentation. However, your HPC may already have support for running JupyterLab on a compute node, e.g, similar to this one at Univ. of Bern or Princeton Univ.. Talk to your HPC staff for policies on running JupyterLab server.</p> <p>Before continuing setup (not over yet!), let's logout and login first from interactive job and exit HPC.</p> <pre><code>exit # from interactive session\nexit # from sumner\n\nssh sumner\n</code></pre> <p>In Part 3, I will finalize setting up Sumner (or CPU-based) HPC and also install a dedicated conda env for Winter (GPU-based) HPC. If you like to stop here, you may except I prefer that you follow bash startup section in Part 3, so that conda and program-specific (R and python) environment variables are consistently loaded across HPC bash user env.</p> <ol> <li> <p>Notice that there is no R in the base env. So, hitting R will not start R session unless you do <code>mamba activate yoda</code>!\u00a0\u21a9</p> </li> <li> <p>Based on a reply from @krassowski at JupyterLab forums \u21a9</p> </li> </ol>","tags":["hpc","setup","conda","kernels","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/","title":"Setting up CPU env - Part 3","text":"<p>Continuing the setup from the Part 2, now we will finalize setup for Sumner or CPU-optimized HPC with following key configurations:</p> <ul> <li> Install additional tools and conda envs for Sumner (CPU-based) HPC:<ul> <li> Julia in yoda env</li> <li> a new env, luke for installing sql related backend tools, and</li> <li> a new env, leia for running snakemake workflows.</li> </ul> </li> <li> Configure Modules to load tools that either I use infrequently or require multiple dependencies that may break my stable env, yoda</li> <li> Finalize bash startup env using ~/.profile.d/ configuration.</li> </ul> <p>Let's start with a fresh  terminal session:</p> <pre><code>exit # from prior interactive session, if any\nexit # from sumner\n\nssh sumner\n\nscreen\n## start interactive session\nsrun -p compute -q batch -N 1 -n 4 --mem 10G -t 08:00:00 --pty bash\n\n## activate env\nconda activate yoda\n\n## unload gcc if loaded\nmodule unload gcc\n</code></pre> <p>Confirm that <code>echo $PATH</code> output should now have paths related to conda env prefixed but nothing else related to modules, LD_LIBRARY_PATH, etc.</p> <p>Also, make sure that module: <code>gcc</code> is unloaded and should not show up in <code>module list</code> output. We do not want system gcc (or any other devtools), and instead rely on conda-installed devtools when compiling R libraries or any other softwares, e.g., samtools, bcftools, etc.</p> <pre><code>## example PATH variable\n/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/bin:/projects/verhaak-lab/amins/hpcenv/mambaforge/condabin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/amins/.local/bin:/home/amins/bin\n\n## example LD_LIBRARY_PATH variable\n/cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64\n</code></pre> Duplicate entries in PATH and LD_LIBRARY_PATH <p>If you are using <code>screen</code>, you may notice duplicate entries for one or more paths, typically for those which are loaded by the system defaults, e.g., modules and <code>/usr/local/...</code>. We will take care of this later when setting up our bash startup using ~/.profile.d/ configurations.</p>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#julia","title":"Julia","text":"<p>Since I use Julia alongside R and Python, I will install it under a primary env, yoda. I will prefer using a long-term support (LTS) version over the latest version. You can install julia via conda-forge channel. However, I'd trouble running conda installed julia in jupyter notebooks with kernel unable to start and connect to console<sup>1</sup>.  Hence, I ended up installing pre-compiled version from julia downloads page and then adjusting PATH variable to setup <code>julia</code> command using ~/.bashrc or preferably ~/.profile.d/ configuration (explained later).</p> Removing conda installed version of julia <p>If you have previously installed julia in conda env, yoda and now like to remove it, you can run <code>mamba remove -n yoda julia</code> to remove julia and all of dependencies which are NOT shared by other conda installed packages. Before removing dependencies, it's good to check if any of removed packages (shared libraries in particular) are requirements or not for other packages by listing package dependencies.</p> <p>After ensuring no other conda packages require julia-related dependencies, I used <code>mamba remove -n yoda julia libunwind</code> to remove conda-installed julia and its dependencies.</p> <ul> <li>Create an empty path to store compiled packages. Later in the setup (Modules section), I will end up loading most of these packages as <code>module load package</code> as I use them less often. For julia, I will rather use bash startup to assign it to PATH variable and load it as a routine package.</li> </ul> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia\n</code></pre> <ul> <li>Install LTS version of julia: Go to julia downloads page and download 64-bit LTS version (and not one with musl which is statically linked libraries).</li> </ul> <pre><code>cd /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia\nwget https://julialang-s3.julialang.org/bin/linux/x64/1.6/julia-1.6.4-linux-x86_64.tar.gz\n\ntar xvzf julia-1.6.4-linux-x86_64.tar.gz\n\n# This is to standardize package naming for module definitions, if any.\nmv julia-1.6.4 1.6.4\n</code></pre>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#reconfigure-julia-base-directory","title":"Reconfigure julia base directory","text":"<p>By default, julia will store packages (which takes much of space) into ~/.julia. Since my home directory is capped at 50 GB, I will reconfigure julia to use non-home path (on tier 1 space) for storing packages. Before reconfiguring, please read Environment Variable section in julia documentation and following post on stackoverflow.</p> Why not to simply move ~/.julia/ to a new place? <p>Perhaps the easiest solution would be to move ~/.julia/ to a new path and symlink it from there. While it looks easy, I usually avoid such hack as several softwares, including conda and some of commands of python and R rely on hardlinks over symnlinks and can throw errors. Besides, it is always good to understand how softwares set default configurations.</p> <ul> <li>Since pre-compiled julia is not in bash PATH, for now, we will just run following command to manually make julia available in PATH for the current terminal session. Once setup is complete, we will set julia path in PATH permanently using bash startup, so julia can load anytime we login to HPC.</li> </ul> <pre><code>export PATH=\"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/bin:${PATH}\"\necho \"$PATH\"\n</code></pre> <ul> <li>Start julia prompt.</li> </ul> <pre><code># show location of julia\ncommand -v julia\n\n# start julia prompt\njulia\n</code></pre> segmentation fault running julia <p>If you have libunwind library for some other conda package(s) and if it is a version higher than 1.5.0, running <code>julia</code> command may throw an error saying segmentation fault. More at conda-forge/julia-feedstock#135. In that case, you may need to tweak PATH and LD_LIBRARY_PATH such that conda installed libunwind does not take precedence when running julia. This is usually achieved using <code>module load julia</code> prior to running julia and thus, tweaking required PATH and LD_LIBRARY_PATH. Alternately, you may downgrade conda installed libunwind only if conda does not throw a warning. <code>mamba install -c conda-forge libunwind=1.5.0</code></p> <ul> <li>Once inside julia terminal, notice existing paths from an output of julia command: <code>DEPOT_PATH</code></li> </ul> <pre><code>3-element Vector{String}:\n\"/home/amins/.julia\"\n\"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia\"\n\"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\"\n</code></pre> <ul> <li>Quit julia by Ctrl+D</li> </ul> <p>We will now update PATH variable and also set a new bash env variable, <code>JULIA_DEPOT_PATH</code> in ~/.bashrc/ Once we finalize bash startup, we will move most of custom configurations from ~/.bashrc to a dedicated ~/.profile.d/ directory.</p> <p>Since we have not installed any julia packages, only packages that are shipped with julia are at /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia/, specifically under base/ subdirectory. We like to keep this path unaltered and hence, we will keep it at the last in <code>JULIA_DEPOT_PATH</code> to assign the lowest priority to install new packages.</p> <ul> <li>First, make an empty package directory similar to R package directory we created earlier.</li> </ul> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6\n</code></pre> <ul> <li>Now copy all of ~/.julia/ contents to this new path. Since we have not installed any new packages, we will be copying only a skeleton of directory and files to a new package path.</li> </ul> <pre><code>rsync -avhP ~/.julia/ /projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/\n</code></pre> <p>Do not delete ~/.julia/ directory</p> <p>Do not delete ~/.julia/ after copying it to a new path. Julia will still use this path to store user-defined configurations, primarily under ~/.julia/config/startup.jl file. Please read Environment Variable in julia documentation if you have not read that yet!</p> <ul> <li>Add following line to ~/.bashrc, preferably above the <code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</code> line because we like to take conda setup precedence over rest of configurations during bash startup.</li> </ul> <pre><code>#### custom configs ####\nexport PATH=\"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/bin:${PATH}\"\n\nexport JULIA_DEPOT_PATH=\"/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\"\n\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n</code></pre> <p>Notice that we have now purposefully kept ~/.julia/ path out of default paths to store packages. Of course, julia will still use ~/.julia/ for storing user configurations among other things but those should not take up much space as installing packages do. If otherwise, we will know later and come up with a better solution!</p> <ul> <li>Before we start julia again to check new <code>DEPOT_PATH</code>, ensure that an updated bash startup has been loaded from ~/.bashrc file. Ideally, you should exit from a current session and login again same as we did in the beginning of this page. Alternately, you may run above <code>export JULIA_...</code> command in the current terminal to update julia env variable.</li> </ul> Why not we do <code>source ~/.bash_profile</code> or <code>source ~/.bashrc</code>? <p>You may do <code>source ~/.bash_profile</code> but do note that this may have unwanted effects on PATH and LD_LIBRARY_PATH variables depending on how ~/.bashrc is loading bash startup env, including from <code>/etc/bashrc</code>. In a nutshell, better to run a single command as follows or the best is to log out and login again to have updated bash startup to take an effect.</p> <ul> <li>Update current terminal env with an updated depot path variable. Make sure to run subsequent <code>julia</code> command in the same terminal (and not in any other terminal sessions in screen you may have already opened) else julia may fall back to old depot path.</li> </ul> <pre><code>export JULIA_DEPOT_PATH=\"/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\"\n</code></pre> <ul> <li>Start <code>julia</code> and type <code>DEPOT_PATH</code> inside julia prompt.</li> </ul> <pre><code>3-element Vector{String}:\n\"/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6\"\n\"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia\"\n\"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\"\n</code></pre> <p>You should now see an updated DEPOT_PATH with our custom path taking the highest priority.</p>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#install-julia-kernel","title":"Install julia kernel","text":"<p>Similar to python and r kernel, we will install julia kernel to interact with julia from JupyterLab console.</p> <p>Running Jupyter notebook from julia prompt</p> <p>IJulia kernel package in julia also allows you to run jupyter notebook or JupyterLab from julia prompt. It does so by installing a separate conda env inside julia package directory. Since we already have installed conda env outside julia package paths, I prefer not to install conda env via julia. So, if you ever come across Running IJulia guide, please tread carefully installing two different conda env on your system! Unless you know what you are doing, avoid running commands like <code>notebook()</code> or <code>jupyterlab()</code> inside julia prompt.</p> <p>Run <code>julia</code> command under yoda env</p> <p>Since we have installed julia as a standalone package and path to <code>julia</code> is fixed in PATH variable, we can run <code>julia</code> independent of which conda env we are in. However, I am using yoda env as my routine env where R and other tools are installed. So, as the best practice, I will run <code>julia</code> after activating yoda env, including running julia as a kernel.</p> <pre><code>mamba activate yoda\n\n# enter julia prompt\njulia\n</code></pre> <ul> <li>Install IJulia kernel</li> </ul> <pre><code>using Pkg\nPkg.add(\"IJulia\")\n</code></pre> <p>Notice where julia is now installing new packages to!</p> <pre><code>julia&gt; Pkg.add(\"IJulia\")\n  Installing known registries into `/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6`\n  ...Resolving and installing dependency packages...\n\n    Building Conda \u2500\u2192 `/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/6cdc8832ba11c7695f494c9d9a1c31e90959ce0f/build.log`\n    Building IJulia \u2192 `/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/d8b9c31196e1dd92181cd0f5760ca2d2ffb4ac0f/build.log`\nPrecompiling project...\n  11 dependencies successfully precompiled in 8 seconds (4 already precompiled)\n</code></pre> <ul> <li>Exit julia prompt by Ctrl+D</li> </ul> <p>For consistency with naming kernels and ensuring that we load a valid yoda env prior to running kernel, let's adjust kernel settings. For rationale, see kernel loading section in Part 2.</p> <ul> <li>Create a new kernel wrapper, /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16</li> </ul> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/kernels\ntouch /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16\n\n# make file executable\nchmod 700 /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16\n</code></pre> <p>Add following to wrap_yoda_jl16</p> <pre><code>#!/bin/bash\n\n## Load env before loading jupyter kernel @sbamin https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425\n\n#### Activate CONDA in subshell ####\n## Read https://github.com/conda/conda/issues/7980\n# I am using conda instead of mamba to activate env\n# as somehow I notices warnings/errors sourcing\n# mamba.sh in sub-shells.\nCONDA_BASE=$(conda info --base) &amp;&amp; \\\nsource \"${CONDA_BASE}\"/etc/profile.d/conda.sh &amp;&amp; \\\nconda activate yoda &amp;&amp; \\\necho \"Env is $(basename ${CONDA_PREFIX})\"\n#### END CONDA SETUP ####\n\n# this is the critical part, and should be at the end of your script:\nexec /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/bin/julia -i --color=yes --project=@. /projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/packages/IJulia/e8kqU/src/kernel.jl \"$@\"\n\n## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/&lt;kernel_name&gt;/kernel.json\n\n#_end_\n</code></pre> <ul> <li>Now, adjust kernel settings.</li> </ul> <pre><code>## go to kernel base dir\ncd ~/.local/share/jupyter/kernels/\n\n## rename julia kernel dir to yoda_jl16\nmv julia-1.6 yoda_jl16\n\n## edit kernel.json to rename display name to yoda_jl16\ncd yoda_jl16\nnano kernel.json\n</code></pre> <p>Replace contents of kernel.json with following:</p> <pre><code>{\n\"argv\": [\n\"/projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16\",\n\"{connection_file}\"\n],\n\"display_name\": \"yoda_jl16\",\n\"language\": \"julia\",\n\"env\": {},\n\"interrupt_mode\": \"signal\"\n}\n</code></pre> <p>Done! Next time you run jupyter, you should have a new julia kernel in JupyterLab.</p>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#database","title":"Database","text":"<p>This is an optional setup. If you are using database like postgresql, you may end up needing similar setup to install required postgresql drivers in conda env. Database driver(s) vary based on type of database, e.g., postgresql, mongodb, etc., and supported programming language.</p>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#postgresql-driver","title":"postgresql driver","text":"<ul> <li>Create a new env, luke to host database related drivers among other backend tools.</li> </ul> <pre><code>mamba create -c conda-forge -n luke psqlodbc\n\n# activate _luke_ env\nmamba activate luke\n</code></pre> <ul> <li>Add database connection entries to ~/.odbc.ini and make it <code>chmod 600 ~/.odbc.ini</code> as it contains login credentials in plain characters!</li> </ul> <p>We use postgresql database in our lab and it is hosted internally on a dedicated linux node. I can programmatically (via R, python, jupyterlab kernel) connect to this database by declaring following connection definition in ~/.odbc.ini file. Notice path to database driver, psqlodbcw.so that we have just installed, and will be used to connect to a remote database.</p> <pre><code>[db1]\nDriver              = /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/luke/lib/psqlodbcw.so\nDatabase            = db1\nServername          = db1.example.com\nUserName            = user1\nPassword            = password1\nPort                = &lt;db1 port&gt;\nsslmode             = require\n[db2]\nDriver              = /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/luke/lib/psqlodbcw.so\nDatabase            = db2\nServername          = db2.example.com\nUserName            = user2\nPassword            = password2\nPort                = &lt;db1 port&gt;\n</code></pre> <ul> <li>If database connection requires SSL (preferable), then you will need to put required SSL configuration files into ~/.postgresql or similar database-specific directory.</li> </ul>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#jupyterlab-sql-deprecated","title":"jupyterlab-sql (deprecated)","text":"<p>I installed jupyterlab-sql in base env thinking it is supported for jupyterlab v3 or higher. However, jupyterlab-sql seems to be outdated and may not function in jupyterlab v3+.</p> <p>You can ignore following setup for jupyterlab-sql (and jump to postgresql-kernel) unless source wbesite confirms that it is supported in an updated jupyrterlab env. Since I already installed jupyterlab-sql, I am going to remove it from base env.</p> <ul> <li>Remove deprecated package.</li> </ul> <pre><code>pip uninstall jupyterlab-sql\n</code></pre> <pre><code>pip uninstall jupyterlab-sql\nFound existing installation: jupyterlab-sql 0.3.3\nUninstalling jupyterlab-sql-0.3.3:\n  Would remove:\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/python3.9/site-packages/jupyterlab_sql-0.3.3.dist-info/*\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/python3.9/site-packages/jupyterlab_sql/*\nProceed (Y/n)? Y\n</code></pre> <ul> <li>Remove a related extension.</li> </ul> <pre><code>jupyter server extension disable jupyterlab_sql\njupyter lab extension disable jupyterlab-sql\n</code></pre> <ul> <li>Rebuild existing jupyter extensions.</li> </ul> <pre><code>jupyter lab build\n\n# check enabled extensions\njupyter lab extension list\njupyter server extension list\necho $?\n</code></pre> Error loading server extension <p>If you still notice a following warning (though <code>echo $?</code> should be 0 for an error-free configuration of jupyterlab extensions) related to jupyterlab_sql, it should not impact running jupyterlab. This seems like an open issue with jupyterlab extension configuration. jupyter/notebook#2584</p> <pre><code>Error loading server extension jupyterlab_sql\n      X is jupyterlab_sql importable?\n</code></pre> <p>Before installing third-party packages, check project page for compatibility</p> <p>Turns out that jupyterlab-sql is no longer actively being maintained and may not be compatible with an updated jupyterlab v2+. pbugnion/jupyterlab-sql#147</p> <p>Before installing third-party packages, prefer packages that are being actively maintained by looking into last commit date, release history, if any, and list of open issues.</p> <p>jupyterlab-sql is a GUI extension in the base env because we run jupyter from base.</p> <ul> <li>From jupyterlab-sql page, check setup.py requirements.</li> </ul> <pre><code>## return to base env from _yoda_\nmamba deactivate\n## confirm that login prompt is showing (base) [userid@sumner]\n## else run mamba deactivate or mamba activate base\n\n## check if requirements are already installed or not\nmamba list | grep -E \"sqlalchemy|jsonschema\"\n\n## install required dependencies\nmamba install -c conda-forge sqlalchemy jsonschema\n</code></pre> <pre><code>pip install jupyterlab-sql\n</code></pre> <pre><code>Successfully built jupyterlab-sql\nInstalling collected packages: jupyterlab-sql\nSuccessfully installed jupyterlab-sql-0.3.3\n</code></pre> <ul> <li>Build required jupyterlab extension for SQL</li> </ul> <pre><code>jupyter server extension enable jupyterlab_sql --py --sys-prefix\n\n## Rebuild all of jupyterlab extensions\n## this may take a while (~5 minutes)\njupyter lab build\n\n# check enabled extensions\njupyter lab extension list\njupyter server extension list\n</code></pre> <ul> <li>Read on how-to use SQL GUI </li> </ul>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#postgresql-kernel","title":"postgresql kernel","text":"<p>Similar to other kernels, let's install postgresql kernel, so that jupyter notebooks can interact with postgres database. If using non-postgres system, follow driver installation based on sqlalchemy-based compatible drivers.</p> <ul> <li>Install postgres driver.</li> </ul> <p>This is not available in conda-forge channel. So, installing using <code>pip install</code> after ensuring that all of dependencies for the drivers are satisfied in base env and if not, prefer installing dependencies first by <code>mamba install</code> and then do <code>pip install</code>.</p> <pre><code>pip install py-postgresql\n</code></pre> <pre><code>Successfully built py-postgresql\nInstalling collected packages: py-postgresql\nSuccessfully installed py-postgresql-1.2.2\n</code></pre> <ul> <li>Install ipython-sql kernel and psycopg2 - a popular postgresql driver for python.</li> </ul> <pre><code>## install core package, sqlalchemy and related dependencies, if any.\nmamba install -c conda-forge sqlalchemy jsonschema\nmamba install -c conda-forge ipython-sql psycopg2\n</code></pre> <p>Since I will be using yoda env for most times, I have also installed core sql drivers into yoda env without any conflicts with an existing setup.</p> <pre><code>mamba activate yoda\nmamba install -c conda-forge sqlalchemy ipython-sql psycopg2\nmamba deactivate\n</code></pre> <ul> <li>If applicable, restart jupyterlab server to enable SQL integration in jupyterlab env. Here is a good tutorial on using jupyter magic commands with SQL.</li> </ul>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#python-2","title":"Python 2","text":"<p>Python comes in two major flavors: Python 2 and Python 3. Since January 1, 2020, the official python developer community  have stopped supporting further development and bug fixes for Python 2. So, it's ideal to use Pythonh 3 over Python 2 unless for a few (or more!) softwares in computational biology that depend on Python 2.</p> <p>Since conda env is bound to Python (and the major version), let's create a separate conda env, windu that will host Python 2.</p> <p>Optional: Along with python 2, I will also install a software, PhyloWGS which requires python 2 as a core dependency.</p> <pre><code>## python 2.7 was the last major release.\nmamba create -n windu python=2.7 phylowgs\n</code></pre> <ul> <li>Let's check briefly if it works ok!</li> </ul> <pre><code>mamba activate windu\npython --version\n\n## if you have also installed phylowgs\n# evolve.py --help\n</code></pre> <p>Python 2.7.15</p> <ul> <li>Deactivate and return to base env.</li> </ul> <pre><code>mamba deactivate\n</code></pre>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#ruby","title":"Ruby","text":"<p>Tools installed under dev or beta env, anakin</p> <pre><code>mamba create -c conda-forge -n anakin httpie\nmamba activate anakin\n</code></pre> <pre><code># install ruby and github cli, https://github.com/cli/cl\nmamba install -c conda-forge ruby gh\n# additional gist utility, https://github.com/defunkt/gist\ngem install gist\n</code></pre> <p>Installing gem for the first time will prepend gem bin path, <code>/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/anakin/share/rubygems/bin</code> to <code>$PATH</code> variable.</p> <p>Use <code>mamba deactivate</code> prior to switching to a new conda env</p> <p>When switching to an another conda env, always prefer using <code>mamba deactivate</code> first and then run <code>mamba activate &lt;other_env&gt;</code> instead of directly running <code>mamba activate &lt;other_env&gt;</code>. By doing deactivation first, conda will reset conda-related paths in <code>$PATH</code>, i.e., to remove paths from a deactivated env and fall back to the conda env (or default base env) that was active prior to a deactivated env. However, if you run <code>mamba activate &lt;other_env&gt;</code> without prior <code>mamba deactivate</code>, some of non-standard paths, i.e., paths other than .../envs//bin/, e.g., ruby gem path above, may remain in the <code>$PATH</code> and even takes the precedence over paths from a switched (and now active) conda env. Such Such invalid ordering of paths in <code>$PATH</code> variable may create issues when you either compile softwares by inadvertently using devtools from a wrong conda env or run softwares with a dynamically linked shared libraries from a wrong conda env.","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#perl","title":"Perl","text":"<p>I rarely use perl language except when it is part of a software, e.g., vcf2maf. If you have configured linux env using setup detailed above, including in previous two parts, you should already have a working perl setup under both, base and yoda env with an identical version (<code>perl --version</code> 5.32.1).</p>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#setup-perl5lib","title":"setup PERL5LIB","text":"<p>Optional: Similar to setting up version-specific custom R and julia package path, let's do the same for perl too using a bash env variable, <code>PERL5LIB</code>.</p> <p>Prefer setting up PERL5LIB at the runtime</p> <p>Please know that it is a better to set PERL5LIB at the run time, i.e., using <code>module load &lt;some program&gt;</code> (see Modules) of a specific package over hardcoding it in the bash startup (as I am doing below!). Hardcoding PERL5LIB with the same perl packages but built using two different perl versions may fail to run your program. Read this post at IBM website</p> <p>Since we have an identical perl version in base and yoda env, we will only create a common path for both env. If you have a different perl version across conda env, you need to source custom bash startup to update <code>PERL5LIB</code> at the time you activate or deactivate conda env. See notes under Tips on compiling packages. Also, read more on <code>PERL5LIB</code> path at stackoverflow and following guide on setting up custom PERL5LIB path.</p> <p>Setting up PERL5LIB path</p> <p>HPC job schedulers, like slurm and moab may use their respective perl libraries. Typically, installing a different version of perl in your (user) conda env should not conflict with running slurm or moab commands as the latter is built using system-installed perl (at <code>/usr/bin/</code>). If it does, you may need to debug specific errors and resolve issue(s) mostly related to PERL5LIB path.</p> <ul> <li>First, check default perl library paths by checking tail portion of <code>perl -V</code> output under <code>@INC:</code>, e.g., for yoda env, it shows following paths:</li> </ul> <pre><code>@INC:\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/site_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/5.32/vendor_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/vendor_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/5.32/core_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/core_perl\n</code></pre> <ul> <li>Create an empty path to store user-installed perl packages. We will only need perl standard library path and one for site_perl. Paths for vendor and core library are designed for system-only perl packages.</li> </ul> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32\nmkdir -p /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32\n</code></pre> <p> Update PERL5LIB to respective major.minor perl version in case you update perl program.</p> <ul> <li>For now, add following to ~/.bashrc above <code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</code> line. Again, we will move most of custom bash startup settings to a dedicated ~/.profile.d/ path.</li> </ul> <pre><code>export PERL5LIB=\"/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl\"\n</code></pre> <p>Unlike setting up other library paths, e.g., LD_LIBRARY_PATH, LIBPATH, etc. where we need to append existing system paths (else bash env may fail to find those libraries), in PERL5LIB, we only need to specify user-level library paths and perl will pick up system path based at the run-time.</p> <ul> <li>Logout and login again to HPC and activate yoda env.</li> <li><code>perl -V</code> should now show a new <code>%ENV</code> variable and perl library paths should now show updated paths with precedence for user-level paths over system paths.</li> </ul> <pre><code>%ENV:\n    PERL5LIB=\"/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl\"\n@INC:\n    /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32\n    /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32\n    /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/5.32/site_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/site_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/5.32/vendor_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/vendor_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/5.32/core_perl\n    /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/core_perl\n</code></pre>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#snakemake","title":"Snakemake","text":"<p>I use snakemake for running computational workflows. Snakemake is a well maintained python package with frequent releases for new features and bugfixes. Hence, I prefer to install it in a dedicated conda env, leia. This will allow me to update snakemake periodically without worrying about conflicting dependencies for other packages installed in yoda or other envs. Also, I can execute complex workflows using snakemake and can activate other conda envs, like yoda (CPU-otpimized) or rey (GPU-optimized) using one or more of snakemake-based rules. To install snakemake, please read installation guide</p> <pre><code># for clarity (and sanity): Better to name this env as snakemake unless\n# being loyal to Star Wars saga!\nmamba create -c conda-forge -c bioconda -n leia snakemake\n</code></pre> <ul> <li>To update snakemake to its latest release:</li> </ul> <pre><code>mamba activate snakemake\nmamba update -c conda-forge -c bioconda snakemake\n</code></pre>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#additional-setup","title":"Additional Setup","text":"<p>Following setup is optional.</p> <ul> <li>Additional packages in yoda env.</li> </ul> <pre><code>## in yoda env\n# depmap was not available in conda-forge\n# biconda version had no conflict with existing setup\nmamba install -c bioconda bioconductor-depmap\nmamba install -c conda-forge r-odbc r-dbi\nmamba install -c conda-forge r-ggthemes r-cowplot r-ggstatsplot r-hrbrthemes\n## OpenCL support for CPU\nmamba install -c conda-forge pocl\n</code></pre>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#jupytext","title":"JupyText","text":"<p>Optional: Jupytext allows running jupyter notebooks as text or markdown files similar to running scripts for R, Python, and Julia. </p> <ul> <li>Install jupytext in base env if if is not installed before. Note compatibility for related jupyterlab extension at mwouts/jupytext Current version of jupytext (1.13.3) is only compatible with JupyterLab 3+ (I have v3.2.4 and so all good!)</li> </ul> <pre><code>mamba deactivate\nmamba activate base\n\nmamba install -c conda-forge jupytext\n</code></pre> <ul> <li>After installing or updating a new jupyterlab extension, good to update and list all extensions.</li> </ul> <pre><code>## list enabled extension\njupyter lab extension list\njupyter server extension list\n\n## update all extensions\njupyter lab extension update --all\njupyter server extension update --all\n\njupyter lab extension list\njupyter server extension list\n</code></pre> <p>I have also installed jupyter_contrib_nbextensions extension earlier in Part 2 which allows additional configuration for jupyter notebook. This is a beta extension and an optional setup.</p>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#modules","title":"Modules","text":"<p>Similar to managing conda env using <code>mamba activate</code> or <code>mamba deactivate</code>, we can also load/unload a specific package via <code>module load</code> or <code>module unload</code> commands. Modules allows us to manage compiled packages or softwares which otherwise are either not available via conda-forge or bioconda channel or installing those with conda is creating dependencies conflict with core conda packages like R and python. Besides overcoming conflicting dependencies, Modules is a better way to organize the same software with multiple versions in case we need to occasionally use an older version for some legacy (and complex) workflow while prefer using an updated version otherwise.</p> <p>Here, I need to assume you have a working knowledge of using Modules. If not, no worries and here are a few tutorials on using modules in HPC env. Also, talk to your Research IT staff as module configuration may vary across HPC envs.</p> <p>Resources on Modules</p> <ul> <li>HPC at NIH</li> <li>Sherlock at Stanford</li> <li>Official documentation also provides an excellent overview of working with Modules. </li> </ul> <ul> <li>Your HPC staff may already have installed a few modules. To view those, run <code>module list</code>.</li> <li>Besides these default modules, you may also want to load packages that you have compiled by yourself. While buding packages, you may find tips on compiling packages useful.</li> <li>Once packages are built, create an empty directory to package-specific directory and respective Modulefiles - one file each for a version, e.g.,</li> </ul> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/modules\ncd /projects/verhaak-lab/amins/hpcenv/modules\n\n## subdir that will host compiled packages in version specific manner\nmkdir apps\n\n## subdir that will host Modulefiles\nmkdir def\n</code></pre> <ul> <li>Now, if I have compiled multiple versions of a package, e.g., samtools v1.11, v1, 14, etc. or a pre-built packages, e.g., GATK v4.1.9.0 and GATK 4.2, I can do that as follows:</li> </ul> <pre><code>cd /projects/verhaak-lab/amins/hpcenv/modules\n\n## bash syntax, {1.11,1.14} will create multiple directories\n## for each entry separated by comma\nmdkir -p apps/samtools/{1.11,1,14}\nmkdir -p def/samtools/{1.11,1.14}\n</code></pre> <ul> <li>In /def/ directory, you will need to put Modulefile as detailed in official documentation. For advanced configuration, refer to following documentation if your HPC is using Lua module system. Following are example Modulefile for samtools v1.14</li> </ul> Example Modulefile Tcl formatLua format <pre><code>#%Module1.0\n## Example format in Tcl langugae\n## samtools\n## Author: Samir Amin\n\n## Read about Modulefile manpage\n## https://modules.readthedocs.io/en/latest/modulefile.html\n\n## Substitute version number and app name below\n# for Tcl script use only\nset VERSION 1.11\nset MODULEDIR /projects/verhaak-lab/amins/hpcenv/modules/apps\nset NAME samtools\n## create a new modulefile for a different version, e.g., 1.14\nset INSTALL_DIR ${MODULEDIR}/${NAME}/${VERSION}\n\nproc ModulesHelp { } {\nglobal version\n        puts stderr \"\\nLoads ${NAME}\\n\"\n}\n\nmodule-whatis   \"${NAME}\"\n\n## check available commands in documentation\nappend-path PATH ${INSTALL_DIR}/bin\n\nif { [ module-info mode load ] } {\nputs stderr \"\\nLoaded ${NAME} ${VERSION} from ${INSTALL_DIR}\\n\"\n}\n\nif { [ module-info mode remove ] } {\nputs stderr \"\\nUnloaded ${NAME} ${VERSION}\\n\"\n}\n\n## END ##\n</code></pre> <pre><code>--[[\n## Example format in Lua langugae\n## samtools\n## Author: Samir Amin\n\n## Read about Lmod\n## https://lmod.readthedocs.io/en/latest/015_writing_modules.html\n## https://lmod.readthedocs.io/en/latest/050_lua_modulefiles.html\n## https://lmod.readthedocs.io/en/latest/020_advanced.html\n--]]\n\n--################################ INTERNAL VARS #################################\n--Module Name and Version are parsed by Lmod from dir/version string in module path\n--Make sure to have exact version numbering when naming respective\n-- app directroy and Modulefile\nlocal pkgName = myModuleName()\nlocal version = myModuleVersion()\nlocal pkgNameVer = myModuleFullName()\n\nlocal approot = \"/projects/verhaak-lab/amins/hpcenv/modules/apps\"\nlocal appbase = \"samtools\"\nlocal pkgdir = pathJoin(approot,appbase,version)\nlocal pkgbin = pathJoin(pkgdir,\"bin\")\n\n--################################# MODULE INFO ##################################\nwhatis(\"Name: \".. pkgName)\nwhatis(\"Version: \" .. version)\n\n--################################## ENV SETUP ###################################\n--## check available commands in documentation\nprepend_path(\"PATH\", pkgbin)\n\n--################################# MODULE LOAD ##################################\nhelp(\n\"Loads \" .. pkgNameVer .. '\\nCheck env change, if any by\\nmodule show ' .. pkgNameVer\n)\n\nif (mode() == \"load\") then\n  LmodMessage(\"## INFO ##\\nLoading \" .. pkgName .. version .. \"from \" .. pkgdir)\nend\n\nif (mode() == \"unload\") then\n  LmodMessage(\"## INFO ##\\nUnloading \" .. pkgName .. version .. \"from \" .. pkgdir)\nend\n\n--## END ##\n</code></pre>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#containers","title":"Containers","text":"<p>Containers are a greay way to enclose project-specifig set of packages and workflow(s) to promote portablity and reproducibility of the same workflow across different HPC environments. Here is a brief intro on two of most popular container systems: Docker and Singularity. HPC at NIH provides a few examples on building singularity based containers, including packages that require GPU-based configuration, e.g., Keras/Tensorflow and Theano. Your HPC may already have a container system like Singularity. If so, you can take full advantage of container system.</p> <ul> <li>Optional: In my bash startup, I have tweaked singularity env variables to store cache data on tier 1 space over default ~/.singularity/ path.</li> </ul> <pre><code>### singularity ###\n## add manpath for singularity to an existing manpath\nMANPATH=\"${SUM7ENV}/local/share/man:/cm/local/apps/singularity/current/share/man${MANPATH:+:$MANPATH}\"\n## set cache dir to non-home path\nSINGULARITY_CACHEDIR=\"/projects/verhaak-lab/amins/containers/cache/singularity\"\n## path were built SIF images are manually stored\nSINGULARITY_SIF=\"/projects/verhaak-lab/amins/containers/sifbin\"\n</code></pre> bash special syntax: <code>${VAR:+:$VAR}</code> <p>I have expanded bash env variable like, MANPATH using syntax: <code>${MANPATH:+:$MANPATH}</code>. It is a special bash syntax which will expand to existing MANPATH only if MANPATH contained any value else output of MANPATH will be an empty string without space or <code>:</code>, thereby perseving MANPATH structure.</p>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#tips-on-compiling-packages","title":"Tips on compiling packages","text":"<ul> <li>Store compiled packages under a base directory like /projects/verhaak-lab/amins/hpcenv/opt/ or */projects/verhaak-lab/amins/hpcenv/opt/apps/.</li> <li>For each of compiled packages, keep a directory structure that maintains package version, e.g., .../apps/samtools/v10.2, .../apps/samtools/v11.3, etc.</li> <li>When possible, install or compile packages using a clean terminal session, i.e., ssh to HPC, start screen, and then interactive session to do rest of installation steps.</li> <li>Avoid installing packages via <code>pip install</code> as unlike <code>mamba install or update</code> command, <code>pip install</code> does not strictly check for package dependencies of conda-installed packaegs. So, at least try installing most of package dependencies using <code>mamba install</code> first before running <code>pip install</code>.</li> <li>Avoid installing packages via Jupyter notebook as unlike terminal session, debugging for a failed installation is difficult with notebook and env session may vary per initialization sequence of jupyter kernel, i.e., whether or not appropriate conda env was loaded prior to starting kernel.</li> <li>While installing packages, prefer using <code>mamba install/update</code> first. If this forces you to downgrade core packages like R and python or several core shared libraries, e.g., libz, openssl, etc., you may fall back to compiling via language-specific functions, e.g., <code>install.packages()</code> in R, <code>pip install</code> for python, etc.</li> <li>While compiling packages, ensure that precedence of paths in PATH, LD_LIBRARY_PATH, LIBRARY_PATH, and related devtools paths are aligned to conda env AND for the respective programming env profile, e.g., if you are compiling a package with intention to use it from yoda env (say a R package), your terminal bash session should have precedence for /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/ path in PATH and if applicable, in LD_LIBRARY_PATH and LIBRARY_PATH too. The identical precedence of paths should also be present in ~/.Renviron file.</li> <li>Know that ~/.Renviron will be read by any of R session running from yoda or other conda env (say you have R 4.1 in yoda but R 5.1 in anakin). Of course, Renviron varies for R 4.1 and R 5.1, and hence, you should have a dedicated Renviron file for each of conda env. You can do that by either creating a Renviron.site file in the respective env under /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/etc path or loading/unloading a custom Renviron file each time you activate/deactivate conda env using configuration files similar to ~/profile.d/ files (see bash startup below), e.g., <code>activate.d/load_R4.1env.sh</code> and <code>deactivate/unload_R4.1env.sh</code> under /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/etc/conda/.</li> <li>For julia, similar startup env files should be ~/.julia/config/startup.jl and <code>$JULIA_BINDIR</code>/<code>$SYSCONFDIR</code>/julia/startup.jl.</li> </ul>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/cpu/sumner_3/#bash-startup","title":"bash startup","text":"<p>As I wrap up setup for our CPU-based Sumner HPC, I will finally configure bash startup - a sequence of files containing bash code that should be loaded to setup a consistent bash login environment on HPC login (interactive) and compute nodes (non-interactive). Here the consistent environment equals to order of paths in <code>PATH</code> and <code>LD_LIBRARY_PATH</code> variables especially when switching conda env and loading of custom configurations for R, Julia, and other packages, etc. that I configured earlier.</p> <p>First, understand what an interactive and a non-interactive bash session is. For most times, we deal with interactive session when we ssh to HPC. You are in the interactive session if env variable, <code>PS1</code> (called bash custom prompt) is set and <code>echo $PS1</code> is showing some output. For more on bash startup, please read official documentation, including notes on interactive shell, especially behavior of an interactive shell.</p> <p>Over years of fiddling with bash startup, my startup setup has become overly complex than needed and may not suit well for your need. At minimum, I prefer to have following in bash startup sequence:</p> <ol> <li>Keep minimal configuration (bash code) in the user ~/.bash_profile and rather keep all of custom configuration commands under ~/.profile.d/ directory.</li> <li>Under user ~/.profile.d/, organize a few bash script based on order you like your bash startup to setup, e.g., ~/.profile.d/A01.sh should load commonly used bash env variables related, e.g., <code>EDITOR</code> to define default text editor, <code>TZ</code> to set default timezone, etc. Then, ~/.profile.d/A02.sh should have additional configurations for other packages, e.g., R, Julia, etc. which otherwise we populated in ~/.bash_profile above.</li> <li>Keep minimal configuration in the user ~/.bashrc file as this may not be loaded with non-interactive, non-login session, e.g., submitting a job to compute nodes without passing bash env of a terminal session or certain commands in R and python using system command to execute bash command(s). Ideally, you like to keep aesthetic configurations in ~/.bashrc file like setting up terminal fonts and colors, managing bash history, etc. You may also optionally create ~/.bash_aliases to store bash short codes that you may use while doing command-line interactive work.</li> <li>Contrary to a default setup of conda instructions in ~/.bashrc, avoid such major env configuration in ~/.bashrc as it may not be loaded in non-interactive sessions, i.e., <code>mamba activate</code> or <code>mamba deactivate</code> may not work within a running shell script or snakemake workflows before running mamba init code. Conda setup may change in future (an active issue  conda/conda#8072) but until then, I will move conda initialization code from ~/.bashrc to ~/.bash_profile, specifically after loading ~/.bashrc but before loading ~/.profile.d/void block (see below).</li> <li>bash startup sequence also includes system default configurations, e.g., setting up default modules to run a workload manager like slurm and a singularity container. These instructions are typically located under /etc/profile file and /etc/profile.d/ directory, both of these will be sourced in the very beginning. These configurations are essential for working on HPC, including submitting jobs to compute nodes. However, sometimes default modules may alter PATH and LD_LIBRARY_PATH variables such that it may conflict with your custom env setup. If so, you can reconfigure respective variables using bash command(s) in ~/.profile.d/void/VA01.sh and similar shell files. These files under void/ will be loaded after bash has initialized system default configurations, i.e., sourced /etc/profile file and shell files under /etc/profile.d/ directory, and therefore, any of bash commands within ~/.profile.d/void/VA01.sh file will override the respective configuration set earlier.</li> <li>I emphasized reconfigure and not resetting earlier because you should be very careful of resetting PATH and LD_LIBRARY_PATH. You may get locked out of HPC login node and may need a help of sysadmin to let you in again!</li> </ol> <p>PS: I do reset PATH at the very end of ~/.bash_profile once bash startup sequence has traversed across all of files detailed above and shown in flow diagram below. By doing so, I can get a consistent bash login environment. However, this is an overkill and you can sure get a stable bash env without such reset.</p> <p>Order of precedence matters</p> <p>Be aware of how bash startup sequence will load these files. Check a flow diagram under My bash startup sequence below.</p> <p>For example, if I set <code>PATH</code> variable as <code>export PATH=/a/b/c:/p/q/r</code> in ~/.profile.d/A01.sh and then I add <code>export PATH=/p/q/r:/a/b/c</code> in ~/.profile.d/A02.sh, bash startup sequqence will overwrite previous PATH with an updated PATH from A02.sh file.</p> My bash startup sequence: flow diagram <p>Following diagram represents sequence of files (containing bash code) that gets sourced each time I login to HPC.</p> <p>If plot is not visible below, you can view plot by pasting following code to mermaid live editor.</p> <pre><code>graph TB\n  A[System &lt;br&gt;/etc/profile] --&gt; B[User&lt;br&gt;.bash_profile]\n  B --&gt; C{User .profile.d/ directory};\n  C --&gt;|Yes| D[source .profile.d/A01.sh];\n  D -.-&gt; E[source .profile.d/A02.sh];\n  E -.-&gt; F[source .profile.d/Z99.sh];\n  F --&gt; G;\n  C --&gt; |No| G{User .bashrc};\n  G --&gt; |Interactive shell&lt;br&gt;PS1 var is set| H[source /etc/bashrc]\n  H --&gt; I[User .bash_aliases]\n  I --&gt; J[bash terminal config, &lt;br&gt; e.g., colors, history, etc.]\n  J --&gt; K[Initialize&lt;br&gt;Conda Environment]\n  G --&gt; |Non-interactive shell&lt;br&gt;PS1 var is unset| K\n  L[User .profile.d/void] --&gt; M[source &lt;br&gt; .profile.d/void/VA01.sh];\n  M -.-&gt; N[source &lt;br&gt; .profile.d/void/VZ99.sh];\n  K --&gt; L;\n  N --&gt; O[Set PATH]\n  O --&gt; P[Set PS1]\n</code></pre> <p></p> <p>Example bash startup files</p> <p>Fix link - You can  download my bash startup files. It will not work by cloning into your linux env. However, each file has inline comments that should help customizing your bash startup.</p> <p>And that's all! </p> <p>My setup for our CPU-based Sumner HPC or for that matter, a generic linux-based machine is now complete. Since our HPC env at JAX shares the common home directory and base linux image (CentOS 7) between CPU (Sumner) and GPU (Winter) based HPC, the above setup will work off-the-shelf on the Winter HPC too except for tasks which require GPU-based computing, e.g., using GPU-based tensorflow and pytorch libraries. For the latter, I will setup a dedicated GPU-based conda env, rey (and ben and gorgu!) and tweak bash startup such that I get GPU-based bash env only when I login to Winter and to Sumner HPC.</p> <p>For GPU-based setup, go to Part 4.</p> <ol> <li> <p>Related discussions on julia forums and troubleshooting guide.\u00a0\u21a9</p> </li> </ol>","tags":["hpc","setup","conda","startup","database","jupyter","programming"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/","title":"Image analysis on GPU-based HPC","text":"<p>Setup CellProfiler, CellPose, StarDist on GPU-based HPC.</p> <p>CPU version of CellProfiler (CP) is installed under env grogu. However, we like to run CP based workflow in a headless HPC environment such that it can leverage CP plugins, including use of GPU-enabled CellPose and StartDist plugins. This requires manually configuring several dependencies for CP vs. those for CellPose and StarDist, such that all three programs can live in harmony within a single conda env.</p>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#initial-setup","title":"Initial Setup","text":"<p>Create a new conda env, cellprofiler because we will be using <code>pip install</code> several times and this may require changing package versions which otherwise existing and stable conda-managed env - with other packages and their dependencies - may not allow us to edit/update.</p> <p>Following setup is based on GPU-enabled HPC at The Jackson Laboratory for Genomic Medicine but it should be applicable for most other NVIDIA-based GPUs running Cent OS 7.</p>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#cellprofiler","title":"CellProfiler","text":"<ul> <li>Login to Winter HPC which is our GPU based HPC.</li> </ul> <pre><code>ssh winter\nmamba activate base\n</code></pre> Do <code>echo $?</code> after each major installation step <p>For error-free setup, check exit code for a successfully run command (should be 0) after each of major installation commands below. This can be done either using <code>echo $?</code> immediately after the major command or suffixing <code>&amp;&amp; echo \"Success\"</code> to a major command text string.</p> <ul> <li>Create a new env and install CP from the official source. Know that purpose of installing CP via conda is to install CP dependencies and not CP itself. For CP, we will later use <code>pip install</code> to install it from the source and so override conda managed CP.</li> </ul> <pre><code>mamba create -c conda-forge -c bioconda -n cellprofiler cellprofiler\n</code></pre>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#cellpose","title":"CellPose","text":"<ul> <li>Install CellPose dependencies, including CUDA drivers for GPU functionality.</li> </ul> <pre><code>mamba activate cellprofiler\nmamba install -c conda-forge -c pytorch pytorch cudatoolkit=11.3\n</code></pre> <ul> <li>Install or upgrade cellpose along with all of its dependencies.</li> </ul> <code>pip install</code> of dependencies is generally not a best practice <p>When working in a conda env, it is not a good idea to install packages using <code>pip install</code>, especially dependencies. You may rather install all of dependencies for a package using <code>mamba install</code> command to avoid version conflict with conda managed packages. You can usually find required dependencies of a package, say CellPose in setup.py and/or <code>requirements.txt</code> file under CellPose source code.</p> <p>Here, I already know that CP and CellPose have conflicting dependencies and I am attempting to fix those manually using <code>pip install</code> with trade off of potentially breaking conda managed package dependencies. Let's see what happens at the end of this setup!</p> <pre><code>pip install cellpose[all] --upgrade |&amp; tee -a ~/logs/cellpose2_envcp_install.log\n</code></pre> <p>Expected end of the installation log:</p> <pre><code>...\nInstalling collected packages: slicerator, PyQt5-Qt5, pyasn1, tqdm, rsa, pyqtgraph, pyqt5.sip, pyasn1-modules, protobuf, opencv-python-headless, natsort, llvmlite, google-crc32c, fastremap, cachetools, pyqt5, pims, numba, googleapis-common-protos, google-resumable-media, google-auth, google-api-core, cellpose, google-cloud-core, dask-image, google-cloud-storage\nSuccessfully installed PyQt5-Qt5-5.15.2 cachetools-5.2.0 cellpose-2.1.0 dask-image-2021.12.0 fastremap-1.13.2 google-api-core-2.8.2 google-auth-2.9.1 google-cloud-core-2.3.2 google-cloud-storage-2.4.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 llvmlite-0.39.0 natsort-8.1.0 numba-0.56.0 opencv-python-headless-4.6.0.66 pims-0.6.1 protobuf-4.21.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyqt5-5.15.7 pyqt5.sip-12.11.0 pyqtgraph-0.12.4 rsa-4.9 slicerator-1.1.0 tqdm-4.64.0\n</code></pre>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#stardist","title":"StarDist","text":"<ul> <li>Similarly install StarDist tensorflow 2 version. For this, we need to first install TensorFlow 2 GPU package, CUDA drivers, and related packages using <code>mamba install</code>.</li> </ul> <p>Downgrading major version for CUDA toolkit and cudnn</p> <p>Note that following command may downgrade above installed (during CellPose dependencies install) cudatoolkit from 11 to 10 and cudnn from 8 to 7. While this is not a best practice, we need to ensure that dependencies for all three - CellProfiler, CellPose, and StarDist - live in harmony under the same conda env, cellprofiler!</p> <pre><code>mamba activate cellprofiler\nmamba install -c conda-forge -c pytorch tensorflow-gpu keras pytorch torchvision cudatoolkit scikit-learn numpy scipy natsort tifffile tqdm numba torch-optimizer\n</code></pre> <ul> <li>Verify if tensorflow is using GPU. Note that tensorflow may require a valid path to CUDA toolkit in LD_LIBRARY_PATH. Since conda env does not put its lib path in LD_LIBRARY_PATH, we may need to set that explicitly as follows (and later as modulefile - See towards the end)</li> </ul> <pre><code>export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:${CONDA_PREFIX}/lib/\"\n# Verify install:\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre> <ul> <li>Test pytorch cuda support: This is critical for running Cellpose in GPU mode.</li> </ul> <pre><code>import torch\n## RELU only has one CUDA device: 0\ndevice = torch.device('cuda:0')\ntorch.zeros([1, 2, 3]).to(device)\n</code></pre> <p>This may fail and cellpose may fallback to CPU. If so, reinstall pytorch from cuda-specific conda repository.</p> <ul> <li>Cellpose may not use GPU unless pytorch package is compiled using compatible cuda toolkit. See details here. To fix, reinstall pytorch ensuring it comes from a channel starting with respective cuda major version, e.g., <code>cuda102...</code> in this case.</li> </ul> <p>For Winter HPC: GPU suport worked with previous steps, and so, I did not reinstall pytorch as pytorch was already built using <code>cuda102...</code> toolkit. See pytorch line in Check versions step below.</p> <pre><code>mamba update -c conda-forge -c pytorch pytorch\n</code></pre> <ul> <li>Check versions for GPU related libraries.</li> </ul> <pre><code>(cellprofiler) foo@winter204:/fastscratch/foo$ mamba list | grep -E \"tensor|cuda|torch\"\ncudatoolkit               10.2.89             h713d32c_10    conda-forge\npytorch                   1.12.0          cuda102py38hfdb21e3_202    conda-forge\npytorch-ranger            0.1.1              pyhd8ed1ab_0    conda-forge\ntensorboard               2.8.0              pyhd8ed1ab_1    conda-forge\ntensorboard-data-server   0.6.0            py38h2b5fc30_2    conda-forge\ntensorboard-plugin-wit    1.8.1              pyhd8ed1ab_0    conda-forge\ntensorflow                2.8.1           cuda102py38h32e99bf_0    conda-forge\ntensorflow-base           2.8.1           cuda102py38ha005362_0    conda-forge\ntensorflow-estimator      2.8.1           cuda102py38h4357c17_0    conda-forge\ntensorflow-gpu            2.8.1           cuda102py38hf05f184_0    conda-forge\ntorch-optimizer           0.3.0              pyhd8ed1ab_0    conda-forge\ntorchvision               0.13.0          cuda102py38h041733a_0    conda-forge\n</code></pre> <ul> <li>Now install or upgrade StarDist.</li> </ul> <pre><code>pip install stardist --upgrade |&amp; tee -a ~/logs/stardist_envcp_install.log\n</code></pre> StarDist: Version conflict with <code>pip install</code> <p>Notice errors below while installing stardist using <code>pip install</code>. While exit code (<code>echo $?</code>) returned zero or success status, CellProfiler command may fail given some of dependencies are mismatched between StarDist and CellProfiler. We need to resolve this later in the setup process.</p> <p>Expected end of the installation log:</p> <pre><code>...\nInstalling collected packages: numpy, h5py, csbdeep, stardist\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.1\n    Uninstalling numpy-1.23.1:\n      Successfully uninstalled numpy-1.23.1\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behavior is the source of the following dependency conflicts.\npython-bioformats 4.0.5 requires python-javabridge==4.0.3, but you have python-javabridge 4.0.0 which is incompatible.\ncentrosome 1.2.0 requires matplotlib==3.1.3, but you have matplotlib 3.5.2 which is incompatible.\ncellprofiler 4.2.1 requires docutils==0.15.2, but you have docutils 0.19 which is incompatible.\ncellprofiler 4.2.1 requires h5py==3.2.1, but you have h5py 2.10.0 which is incompatible.\ncellprofiler 4.2.1 requires matplotlib==3.1.3, but you have matplotlib 3.5.2 which is incompatible.\ncellprofiler 4.2.1 requires mysqlclient==1.4.6, but you have mysqlclient 2.0.3 which is incompatible.\ncellprofiler 4.2.1 requires python-javabridge==4.0.3, but you have python-javabridge 4.0.0 which is incompatible.\ncellprofiler 4.2.1 requires pyzmq==18.0.1, but you have pyzmq 18.1.1 which is incompatible.\ncellprofiler 4.2.1 requires sentry-sdk==0.18.0, but you have sentry-sdk 1.9.0 which is incompatible.\ncellprofiler 4.2.1 requires wxPython==4.1.0, but you have wxpython 4.1.1 which is incompatible.\ncellprofiler-core 4.2.1 requires docutils==0.15.2, but you have docutils 0.19 which is incompatible.\ncellprofiler-core 4.2.1 requires h5py==3.2.1, but you have h5py 2.10.0 which is incompatible.\ncellprofiler-core 4.2.1 requires python-javabridge==4.0.3, but you have python-javabridge 4.0.0 which is incompatible.\ncellprofiler-core 4.2.1 requires pyzmq==18.0.1, but you have pyzmq 18.1.1 which is incompatible.\nSuccessfully installed csbdeep-0.7.2 h5py-2.10.0 numpy-1.22.4 stardist-0.8.3\n</code></pre> <ul> <li>Also install including OmniPose which I missed installing earlier with CellPose.</li> </ul> <pre><code>pip install omnipose --upgrade |&amp; tee -a ~/logs/omnipose_envcp_install.log\n</code></pre> Omnipose: Version conflict with <code>pip install</code> <p>Similar to StarDist setup, we need to resolve these conflict issues later.</p> <p>Expected end of the installation log:</p> <pre><code>Successfully built ncolor\nInstalling collected packages: mahotas, edt, ncolor, omnipose\n  Attempting uninstall: mahotas\n    Found existing installation: mahotas 1.4.13\n    Uninstalling mahotas-1.4.13:\n      Successfully uninstalled mahotas-1.4.13\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncellprofiler 4.2.1 requires docutils==0.15.2, but you have docutils 0.19 which is incompatible.\ncellprofiler 4.2.1 requires h5py==3.2.1, but you have h5py 2.10.0 which is incompatible.\ncellprofiler 4.2.1 requires matplotlib==3.1.3, but you have matplotlib 3.5.2 which is incompatible.\ncellprofiler 4.2.1 requires mysqlclient==1.4.6, but you have mysqlclient 2.0.3 which is incompatible.\ncellprofiler 4.2.1 requires python-javabridge==4.0.3, but you have python-javabridge 4.0.0 which is incompatible.\ncellprofiler 4.2.1 requires pyzmq==18.0.1, but you have pyzmq 18.1.1 which is incompatible.\ncellprofiler 4.2.1 requires sentry-sdk==0.18.0, but you have sentry-sdk 1.9.0 which is incompatible.\ncellprofiler 4.2.1 requires wxPython==4.1.0, but you have wxpython 4.1.1 which is incompatible.\nSuccessfully installed edt-2.3.0 mahotas-1.4.12 ncolor-1.1.5 omnipose-0.2.1\n</code></pre>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#resolve-version-conflicts","title":"Resolve version conflicts","text":"<p>Before fixing version conflicts that we observed above, see if example headless pipeline of CellProfiler can run on HPC under cellprofiler conda env. If so, there may not be any need of resolving conflicts. I usually check CellProfiler dependencies in its source code (at setup.py and requirements.txt if any) and at their conda installation and Linux setup pages.</p> <p>I usually avoid package downgrade unless I encounter a relevant error. I also avoid upgrading packages unless there is a change in major or minor version from say <code>h5py</code> 2.10.0 to 3.2.1 as in above case or 2.10.0 to 5.4.1 to 5.5.0. First, I try using <code>mamba update</code> if it can update packages while respecting other packages dependencies that are managed via conda (but may not do so for packages installed via <code>pip</code>).</p> <pre><code>mamba update -c conda-forge -c bioconda -c pytorch python-javabridge matplotlib docutils h5py mysqlclient pyzmq sentry-sdk wxPython\n</code></pre> <p>Resulted into no version changes and so I skipped updating <code>h5py</code> at least for now.</p> <p>ToDo: Check with CellProfiler Team on upgrading <code>h5py</code> package</p> <p>Check with CP team on need for upgrading h5py - whether critical or ok to keep v 2.10.0 over 3.2.1?</p> <p>Another option is to source install CellProfiler from the CP source code and using <code>pip3 install .</code>. Read installing CP on Ubuntu 20.04. However, I have avoided this so far to manage much of my conda CP env using conda and <code>pip install</code> commands. So far, I have not seen errors that break CP workflow with or without use of GPU-based CellPose and StarDist plugins. If I do notice errors that can only be fixed by installing CP from its source, I will update this page accordingly.</p> <ul> <li>At the end of this setup, you should have all three tools installed in cellprofiler conda env.</li> </ul> <pre><code>mamba list | grep -E \"cellpose|stardist|cellprofiler\"\n</code></pre> <pre><code>cellpose                  2.1.0                    pypi_0    pypi\ncellprofiler              4.2.1            py38h779adbc_0    bioconda\ncellprofiler-core         4.2.1              pyhdfd78af_0    bioconda\nstardist                  0.8.3                    pypi_0    pypi\n</code></pre>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#jupyter-support","title":"Jupyter Support","text":"<p>To use cellprofiler conda env in jupyter kernel, e.g., in VScode or elsewhere, install <code>ipykernel</code> (and related jupyter dependencies)</p> <pre><code>mamba activate cellprofiler\nmamba install -c conda-forge ipykernel\n## install env specific kernel at ~/.local/share/jupyter/kernels\npython -m ipykernel install --user --name cellprofiler_py --display-name \"cellprofiler_py\"\n</code></pre>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#kernel-setup","title":"Kernel setup","text":"<p>This setup - using a wrapper script - will first activate CP conda env and then launch python kernel for jupyter connection. That way, I can be certain that all of cellprofiler conda env configurations, including GPU configs, are loaded before launching jupyter session.</p> <p>To do so, first create a wrapper script like following and save it somewhere on your filesystem, e.g., /projects/foo/hpcenv/opt/kernels/wrap_cellprofiler_py38. </p> <pre><code>#!/bin/bash\n\n## Load env before loading jupyter kernel\n\n## https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425\n\n## restrict loading kernel only to GPU (and not CPU) based HPC\nif [[ \"$(hostname)\" != *\"winter\"* ]]; then\necho -e \"ERROR: Invalid hostname\\nThis kernel works only on winter HPC\\n\" &gt;&amp;2\nexit 1\nelse\n#### Activate CONDA in subshell ####\n## Read https://github.com/conda/conda/issues/7980\n# I am using conda instead of mamba to activate env\n# as somehow I notices warnings/errors sourcing\n# mamba.sh in sub-shells.\nCONDA_BASE=$(conda info --base) &amp;&amp; \\\nsource \"${CONDA_BASE}\"/etc/profile.d/conda.sh &amp;&amp; \\\nconda activate cellprofiler\n    #### END CONDA SETUP ####\n\n## Load additional CUDA drivers, toolkit, etc.\n## if applicable prior to initializing kernel\n\n# this is the critical part, and should be at the end of your script:\n# replace with path to python under cellprofiler conda env\nexec /projects/foo/hpcenv/mambaforge/envs/cellprofiler/bin/python -m ipykernel_launcher \"$@\"\n\n## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/&lt;kernel_name&gt;/kernel.json\nfi\n#_end_\n</code></pre> <ul> <li>Now go to <code>~/.local/share/jupyter/kernels</code> and locate at <code>kernel.json</code> file under cellprofiler_py (or whichever name you used when configuring ipykernel above). Open </li> </ul> <pre><code>{\n\"argv\": [\n\"/projects/foo/hpcenv/opt/kernels/wrap_cellprofiler_py38\",\n\"-f\",\n\"{connection_file}\"\n],\n\"display_name\": \"cellprofiler_py\",\n\"language\": \"python\",\n\"metadata\": {\n\"debugger\": true\n}\n}\n</code></pre> Change icons to show CellProfiler icon in Jupyter <p>You can also replace icon images under <code>~/.local/share/jupyter/kernels/cellprofiler_py</code> to show cellprofile env specific icon in Jupyter Launcher session.</p> <p> </p> Example Jupyter Launcher after kernel setup","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#backup-conda-env","title":"Backup Conda Env","text":"<p>At this stage, you should backup configurations for cellprofiler conda env, similar to one detailed here.</p> <p>At the end of this setup, including commands that I ran below for GPU and CP plugins setup, my conda cellprofiler env was identical to an env export file:  cellprofiler_gpu_env_export_pkgs.yml. Some of package versions may differ for you though and that should be ok. Anaconda provides an option to recreate an env similar to an exported env file which I have not tested personally.</p>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#gpu-configuration","title":"GPU configuration","text":"<p>Following setup as above, GPU env should be configured and managed via conda. You may however see bash variable <code>CUDA_HOME</code> as undefined. This variable may be required by some tools and should point to path where cellprofiler conda env is installed, e.g., <code>/projects/foo/mambaforge/envs/cellprofiler</code>.</p> Optional: load required CUDA drivers as a modulefile <p>Here an example module file I have in my env. This is optional (and not loaded in my HPC env) and only needed for some cases where one or more tools fail to recognize GPU drivers. Talk to your HPC team to configure a valid CUDA driver setup that matches GPU hardware on the HPC.</p> <pre><code>(cellprofiler) foo@winter204:/fastscratch/foo/tmp$ module show gpu/10.2_cellprofiler\n-----------------------------------------------------------------------------------------------------------------------------------------\n   /projects/foo/hpcenv/opt/modules/def/gpu/10.2_cellprofiler:\n-----------------------------------------------------------------------------------------------------------------------------------------\nwhatis(\"adds user-defined NVIDIA CUDA 11.1 Toolkit to your environment variables \")\nconflict(\"gpu/11.1.1\")\nconflict(\"gpu/11.1.1_ben\")\nconflict(\"gpu/10.2_ben\")\nsetenv(\"CUDA_INSTALL_PATH\",\"/projects/foo/hpcenv/mambaforge/envs/cellprofiler\")\nsetenv(\"CUDA_PATH\",\"/projects/foo/hpcenv/mambaforge/envs/cellprofiler\")\nsetenv(\"CUDA_ROOT\",\"/projects/foo/hpcenv/mambaforge/envs/cellprofiler\")\nsetenv(\"CUDA_HOME\",\"/projects/foo/hpcenv/mambaforge/envs/cellprofiler\")\nprepend_path(\"PATH\",\"/projects/foo/hpcenv/opt/modules/apps/gpu/10.2/local/bin\")\nprepend_path(\"LIBRARY_PATH\",\"/projects/foo/hpcenv/opt/modules/apps/gpu/10.2/local/lib\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/projects/foo/hpcenv/opt/modules/apps/gpu/10.2/local/lib\")\nprepend_path(\"INCLUDEPATH\",\"/projects/foo/hpcenv/opt/modules/apps/gpu/10.2/local/include\")\nprepend_path(\"MANPATH\",\"/projects/foo/hpcenv/opt/modules/apps/gpu/10.2/local/share/man\")\nprepend_path(\"LIBRARY_PATH\",\"/projects/foo/hpcenv/mambaforge/envs/cellprofiler/lib\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/projects/foo/hpcenv/mambaforge/envs/cellprofiler/lib\")\nprepend_path(\"INCLUDEPATH\",\"/projects/foo/hpcenv/mambaforge/envs/cellprofiler/include\")\nprepend_path(\"CUDA_INC_PATH\",\"/projects/foo/hpcenv/mambaforge/envs/cellprofiler\")\nsetenv(\"CUDA_INSTALL_DIR\",\"/projects/foo/hpcenv/mambaforge/envs/cellprofiler\")\nhelp([[ Adds user-defined NVIDIA CUDA 11.1 Toolkit to your environment variables,\n]])\n</code></pre>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#test-gpu","title":"Test GPU","text":"<p>Following commands in bash and python should test GPU functionality. Ideally, you should run all three - CellProfiler, CellPose, and StarDist - to ensure that GPU is in-use during runtime.</p> <pre><code>echo $CUDA_HOME # (1)\nnvcc --version # (2)\nnvidia-smi # (3)\n</code></pre> <ol> <li>CUDA_HOME should point to conda env home path, e.g., /foo/mambaforge/envs/cellprofiler. If not, you may get an error for certain tools.</li> <li><code>nvcc</code> is located at <code>*/foo/mambaforge/envs/cellprofiler/bin/nvcc</code></li> <li><code>nvidia-smi</code> is a system-managed binary at <code>/usr/bin/nvidia-smi</code></li> </ol> <pre><code>export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:${CONDA_PREFIX}/lib/\"\n# Verify install:\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre> <pre><code>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n</code></pre> <pre><code>2022-07-29 14:08:14.276017: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-29 14:08:15.656565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30976 MB memory:  -&gt; device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n</code></pre> <p>As long as it says using <code>device:GPU:0</code>, TensorFlow 2 is setup to use GPU.</p> <pre><code>import torch\n## RELU only has one CUDA device: 0\ndevice = torch.device('cuda:0')\ntorch.zeros([1, 2, 3]).to(device)\n\nx = torch.rand(5, 3)\nprint(x)\n</code></pre> <p>Should show a tensor with random numbers.</p> <pre><code>tensor([[[0., 0., 0.],\n         [0., 0., 0.]]], device='cuda:0')\n</code></pre> <pre><code>tensor([[0.8770, 0.5362, 0.5748],\n        [0.4587, 0.8161, 0.3703],\n        [0.0056, 0.8461, 0.0816],\n        [0.6864, 0.8047, 0.5323],\n        [0.2400, 0.3549, 0.7756]])\n</code></pre>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#cellprofiler-plugins","title":"CellProfiler Plugins","text":"<p>Following guide is based on documentation from CellProfiler Plugin page.</p> <pre><code>mkdir -p \"${HPCAPPS}\"/cellprofiler\ncd \"${HPCAPPS}\"/cellprofiler\ngit clone https://github.com/CellProfiler/CellProfiler-plugins.git\n</code></pre> <p>I have used CP plugin version with commits upto: CellProfiler/CellProfiler-plugins@714d4e7 dated 2022-08-18.</p> <ul> <li>Open CellProfiler in GUI mode. <code>Preferences &gt; CellProfiler plugins directory</code>, update path to cellprofiler plugins dir. Restart CP from commandline in GUI mode as <code>cellprofiler</code>. If running CP on HPC, this may not be feasible. If so, run CP in headless mode with <code>--plugins-directory</code> flag pointing to plugin path we installed above.</li> </ul> <p>Observe errors on command line and install missing packages as long as 1) updates do not break existing setup of cellprofiler env, and 2) we can run CellPose and StarDist as plugins. We may not need to install dependencies for other plugins, if any. Check plugin-specific dependencies at CellProfiler-plugins website.</p> <pre><code>mamba activate cellprofiler\nmamba install -c conda-forge pandas jpype1 pyimagej\n</code></pre> <ul> <li>Restart CP and check for errors on terminal if any.</li> </ul> <pre><code>cd /fastscratch/foo/cellprofiler/toyset\ncellprofiler -c -r -p pipeline/hcs_toyrun.cppipe -o output -i input -L 20  --plugins-directory \"${HPCAPPS}\"/cellprofiler/CellProfiler-plugins\n</code></pre> <p>Hurray! CP works in headless mode with GPU-based CellPose as a plugin.</p>  Error in sys.excepthook <p>I was able to run CP with GPU-enabled CellPose workflow in headless mode and was able to get expected output of the workflow. However, CP ended with Error in sys.excepthook message but with a bash exit code (<code>$?</code>) 0. This is likely a harmless error and a possible bug in the upstream code. See related Image.sc forum post</p>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/image_analysis_cellprofiler_cellpose/#questions","title":"Questions","text":"<p>For questions specific to above setup, please use GitHub based comments system below. For questions related to cellprofiler and related plugins, please use an excellent community forum at Images.sc, including cellprofiler based questions.</p>","tags":["gpu","hpc","imaging","segmentation","cellprofiler","cellpose","stardist"]},{"location":"hpc/gpu/winter_1/","title":"Setting up GPU env","text":"<p>Winter HPC at JAX is a GPU-based computing cluster and it is powered by NVIDIA\u00ae V100 series GPU cards. If you are working on GPU-based HPC or linux env, following page should guide you on setting up commonly used GPU libraries, e.g., Tensorflow 2, Keras, and PyTorch. GPU setup involves several technical jargon related to hardware compliant libraries, e.g., CUDA toolkit if using NVIDIA marketed GPU cards. I will not go into details of each step here and instead link to installation guide for further details. Knowing such details should be useful while working with deep learning tools and debugging runtime errors.</p> <p>Before starting GPU setup, I expect that you have finished CPU setup, up until Part 3, mainly installing yoda env and bash startup sequence.</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#login-to-gpu-hpc","title":"Login to GPU HPC","text":"<p>First, let's move away from CPU HPC, aka Sumer HPC at JAX, and instead login to GPU HPC, i.e., Winter HPC at JAX. We have a common linux base operating system (OS), i.e., Cent OS 7 and a user home directory for both HPCs, hence I will have an identical bash env - via bash startup sequence - in Winter as of Sumner.</p> home directory and operating system <p>If you have a separate OS and a home directory for CPU and GPU HPCs, you need to start from a scratch in setting up GPU HPC, i.e., initial setup is identical to CPU setup, preferably all three parts or at least installing yoda env and bash startup sequence.</p> <p>If you have an identical home directory but different OS, e.g., Cent OS 6 on CPU HPC and Cent OS 7 on GPU HPC, that's a bad system design in my view as it will be challenging - at least to me - to separate software compilation libraries by modifying PATH, LD_LIBRARY_PATH, etc. and configuration locations, e.g., ~/.local and ~/.config under the shared bash env.</p> <ul> <li>Login to Winter HPC</li> </ul> <pre><code>ssh winter\n</code></pre> <p>If you have set bash startup sequence earlier, you should expect ~identical<sup>1</sup> bash env, including ordering of paths (output of <code>echo $PATH</code>) between CPU and GPU HPCs.</p> <ul> <li>Start an interactive job, so that we can use compute and not login node for setup/ This is to avoid our setup being potentially killed on the login node due to compute and/or memory intensive commands we will run during setup.</li> </ul> <pre><code>## interactive job command may vary across HPCs\n## requesting partition: gpu with one gpu core\nsrun --job-name=gpusetup --qos=dev --time=08:00:00 --mem=8G --nodes=1 --ntasks=2 --mail-type=FAIL --export=all --gres gpu:1 --pty bash --login\n</code></pre> <p>Notice that I now use <code>bash --login</code> over <code>bash</code> to force interactive login. Details under bash startup sequence.</p> <p>Once you are in the interactive session, bash prompt will change to user@winter200 or some other number than the original login node. We are going to create a new and dedicated conda env for GPU HPC, named rey. That said, we can use previously setup conda env for CPU HPC here too!</p> <ul> <li>For example, to start R session on GPU HPC:</li> </ul> <pre><code>mamba activate yoda\nR\n</code></pre> <p>You should be able to interact with R same as you do on CPU HPC, as long as both HPCs have shared storage paths and an identical OS.</p> <p>Avoid managing conda env across HPCs</p> <p>While it should not matter if you are managing conda env on CPU or GPU HPC, e.g., installing or upgrading conda packages, I prefer to manage all of CPU optimized conda envs, i.e., base, yoda, leia, etc. from Sumner (CPU) HPC. Accordingly, I will use Winter GPU HPC to manage GPU-optimized conda env, i.e., rey and ben. This is particularly important for managing GPU env as CUDA and other GPU-specific libraries are not available on CPU HPC and so, installing or upgrading GPU packages may fail if you use CPU HPC to manage GPU env.</p> <p>Let's deactivate yoda and return to base env in the Winter HPC.</p> <pre><code>mamba deactivate\n</code></pre>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#create-a-gpu-env-rey","title":"Create a GPU env rey","text":"<p>Now, we are going to install sizable (3 GB or more) worth of packages into a new conda env, rey. These packages form core of deep learning or specifically provide set of algorithms to employ artificial neural network based machine learning.</p> <pre><code>mamba create -c conda-forge -c pytorch -n rey python=3.9 tensorflow-gpu keras pytorch torchvision torchaudio cudatoolkit=11.1.1 cudatoolkit-dev=11.1.1 scikit-learn xgboost r-base=4.1.1 r-tensorflow r-keras r-tfdatasets tensorflow-hub cupy dask dask-ml pyopencl pocl ## lazy way to check if above command had any errors.\n## should return 0, meaning successful execution of\n## the most recent previous command.\necho $?\n</code></pre> <p>Before running above command, please know what we are installing here:</p> <ul> <li>Create a new conda env, rey for GPU-based HPC.</li> <li>Install all packages with the highest preference from conda-forge channel followed by pytorch. PyTorch is a a commonly used deep learning library and PyTorch team distributes some of dependencies with their own conda channel.</li> <li>For all practical purposes, we will try to keep rey env similar to yoda env while adding GPU support in rey. Accordingly, we need to ensure that we are using similar <code>major.minor</code> version for Python and R - two major programming languages that I use on daily basis.<ul> <li>I am using python 3.10 in yoda. However, python 3.10 support is not yet available, a commonly used deep learning library. Hence, I am specifying <code>python=3.9</code> to ensure that <code>mamba create</code> will do the best to keep that version. You may try using the same python version as you have in yoda env, and check if <code>mamba create ...</code> command above throws an error regarding conflicting versions. If it does, update <code>=3.XX</code> to a one lower minor version until mamba allows you to create a new env, rey.</li> <li>Same logic for R by using <code>r-base=4.1.1</code> to match R version in yoda env. Please read important notes below on using R packages across two or more conda envs.</li> </ul> </li> <li>Install popular ML frameworks with GPU support: Tensorflow 2, Keras, and PyTorch. Keras now ships with Tensorflow 2 and so specifying keras in command above is optional.<ul> <li>Notice that unlike restricting version for Python and R (to match with that in yoda env), we are not specifying versions for any of ML libraries. Doing so has one drawback that in rare cases, conda may end up installing an older but compatible version (with our Python and R) of one or more ML libraries. If this happens and you are in need of the latest ML library, you have an option to create yet another conda env to install the most recent ML library at the cost of possibly installing a different version of Python and R than one in yoda env.</li> </ul> </li> <li> <p>CUDA toolkit is the heart of leveraging GPU support on Winter HPC. Using conda, we are installing CUDA toolkit and related development kit to install NVIDIA\u00ae cuDNN library. However, we will ensure - by appending, <code>=11.1.1</code> to a package - that CUDA toolkit version must match that of system installed CUDA drivers by HPC staff. If there is a mismatch, GPU hardware (NVIDIA V100 card) may fail to recognize our instructions (commands) to perform machine learning analysis.</p> <ul> <li>For Winter HPC at JAX, I am using CUDA 11.1.1 based on available drivers in Winter HPC. These drivers are typically configured using HPC modules and you can list those using <code>module available</code> and then list details for a specific drivers, e.g, <code>module show cuda11.1/toolkit/11.1.1</code>. Besides toolkit, HPC staff also provides following other core drivers which may be required for installing or running a specific GPU-compiled package. For now, I am not loading any of these default modules and instead relying on conda-managed (and minimal) packages.</li> </ul> <pre><code>cuda11.1/blas/11.1.1\ncuda11.1/fft/11.1.1\ncuda11.1/nsight/11.1.1\ncuda11.1/profiler/11.1.1\ncuda11.1/toolkit/11.1.1 \n</code></pre> </li> <li> <p>We will also install scikit-learn and XGBoost, two popular libraries for machine learning at-large. </p> </li> <li>Then, I will install GPU support for R language and a few packages for using Tensorflow for R.<ul> <li>We have earlier installed R in yoda but we cannot use it in rey env. To leverage R for machine (and deep) learning, I will install the identical R version, i.e. 4.1.1 in conda env, rey. That way, I can use most of R packages from yoda (CPU-based) env for use in rey (GPU-based) env.</li> </ul> </li> </ul> <p>Careful sharing R packages across two or more conda envs</p> <p>Do note that some R packages requires additional packages (libraries) to be installed in the respective conda env, e.g., rJava requires <code>java</code> from yoda env and may not work with rey env. In such cases, use <code>mamba install</code> to install such packages in rey while on GPU env but avoid running <code>install.packages</code> command from a R session running in rey env. Why?</p> <p>I have briefly touched on this issue in Part 3: Tips on compiling packages. If we use <code>install.packages</code> from rey env, it will end up installing the same package, e.g., <code>rJava</code> and perhaps, all of its dependencies into the same library path as of yoda env, i.e., as defined by first entry of <code>.libPaths()</code>. That is a recipe for warnings and errors because doing so will inevitably mix up library dependencies between two different conda envs, each optimized for CPU and GPU.</p> <p>There is a solution though! We can update <code>.libPaths()</code> for rey on-the-fly when we activate or deactivate rey env (explained later) and that way, we can ensure that <code>install.packages</code> should install packages in rey specific R package path. I say \"should\" as R may end up updating packages in any of user-writable paths, even if the path is set as a second or lower preference.</p> <p>In nutshell, to avoid breaking R in multiple conda env, use <code>mamba install</code> or <code>mamba update</code> over R <code>install.packages()</code> when possible.</p> <ul> <li>I will also install a few additional packages for image classification and related machine learning purpose. These are: cupy, dask, dask-ml, pyopencl, and pocl. A notable exception is that I am not installing Theano which is not under active development but now forked as Aesara.</li> <li>I am installing all major packages at once to ensure package dependencies are not in conflict and we have a stable GPU env.</li> <li>While some of packages, e.g., r-tensorflow may work on CPU-based HPC too, I would recommend to use this conda env only on the GPU-based HPC as most of packages require GPU support.</li> </ul>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#install-essentials","title":"Install essentials","text":"<p>I use following tools in routine and have installed these tools into yoda env - a default for CPU-based HPC<sup>2</sup>. Similarly, I am installing in these tools here in rey env too for GPU-based HPC.</p> <pre><code>mamba install -c conda-forge wget curl rsync libiconv parallel ipyparallel git rsync vim globus-cli tmux screen openjdk=11.0 r-rjava matplotlib r-reticulate rpy2\n</code></pre> <p>For java (openjdk), prefer using the same version as in yoda, e.g., restrict java major.minor version to be 11.0 but allow a different patch (11.0.1 or 11.0.2,...).</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#loading-gpu-env","title":"Loading GPU env","text":"<p>Once we have rey env ready, we can check type of GPU, CUDA drivers, etc. We should also ensure that we configure CUDA drivers and related env variables correctly, so future installations of GPU tools, like TensorRT recognize CUDA related variables and work correctly.</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#check-cuda-drivers","title":"Check CUDA drivers","text":"<ul> <li>Let's activate rey and power up GPU!</li> </ul> <pre><code>mamba activate rey\n</code></pre> <ul> <li>Check NVIDIA driver version</li> </ul> <pre><code>nvcc --version\n</code></pre> <pre><code>nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Mon_Oct_12_20:09:46_PDT_2020\nCuda compilation tools, release 11.1, V11.1.105\nBuild cuda_11.1.TC455_06.29190527_0\n</code></pre> <ul> <li>Check GPU usage activity on the compute node using <code>nvidia-smi</code>. This command is from a system-installed cuda libraries, typically under <code>/usr/bin</code> or <code>/usr/local/bin</code>. On Winter HPC at JAX, it is only available on compute nodes and not on a login node.</li> </ul>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#setup-gpu-env-as-modulefile","title":"Setup GPU env as Modulefile","text":"<p>Unfortunately, conda installed CUDA toolkit is not a full CUDA installation and it does not set any of CUDA related bash env variables, especially <code>CUDA_PATH</code>, <code>CUDA_HOME</code>, <code>CUDNN_PATH</code> variables. conda/conda#7757 Since I have installed the identical cuda toolkit (v11.1.1) in rey conda env to the one managed by out HPC admins, i.e., <code>module show cuda11.1/toolkit/11.1.1</code>, I will create a module file that includes combination of env variables from both of these toolkits. That way, I can load this module during bash startup such that the module will configure GPU env only on the Winter (GPU) HPC and not on the Sumner (CPU) HPC.</p> <ul> <li>Create an empty local directory structure to store user-installed GPU libraries, e.g., configs related to CUPTI, etc.</li> </ul> <pre><code>cd \"${HPCAPPS}\" &amp;&amp; \\\nmkdir -p gpu/11.1.1/local\n</code></pre> <ul> <li>Following command will create directory scaffold similar to /usr/local env</li> </ul> <pre><code>cd \"${HPCAPPS}\"/gpu/11.1.1/local &amp;&amp; \\\nmkdir -p {bin,etc,include,lib,lib64,libexec,share/{doc,info,locale,man/{man1,man3}}}\n</code></pre> <ul> <li>Create a module file at \"${HPCMODULES}\"</li> </ul> <pre><code>mkdir -p \"${HPCMODULES}\"/gpu\ncd \"${HPCMODULES}\"/gpu\n\n## create a module file that matches CUDA version.\ntouch 11.1.1\n</code></pre> <ul> <li>I have placed GPU configurations from both, admin installed CUDA drivers and GPU packages that I just have installed above. You may need to consult your HPC team to get an idea on configurations that you may able override with conda installed cuda toolkit.</li> </ul> <p>Example modulefiles for GPU HPC</p> <p>My gpu modulefile are at  /confs/hpc/modules/def</p> <ul> <li>Once we have a modulefile ready, we can load custom gpu env using <code>module load gpu/11.1.1</code>.</li> <li>Notice change in PATH, LD_LIBRARY_PATH, and related env variables. For now, you will notice that <code>\"${CONDA_PREFIX}\"/bin</code> is pushed behind other cuda related paths we have configured using modulefile. Since I prefer to have <code>\"${CONDA_PREFIX}\"/bin</code> take precedence over rest of <code>$PATH</code> contents, I will reset PATH such that <code>\"${CONDA_PREFIX}\"/bin</code>, i.e., <code>../envs/rey/bin</code> in Winter HPC, will take precedence over other paths that we are loading via above modulefile. See bash startup section for details.</li> </ul>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#jupyter-kernels","title":"Jupyter kernels","text":"<p>Let's install Python and R jupyter kernels for rey with configuration similar to that for yoda env.</p> <ul> <li>First, we will install required packages. Here, I am not interested in installing bash_kernel as I did with yoda as I rarely use bash kernel and rather prefer terminal. If installing reticulate and rpy2 packages throw warnings about potential upgrade or downgrade of existing packages in rey env, please do not ignore warnings and instead follow steps under installing respective packages in yoda env.</li> </ul> <pre><code>mamba install -c conda-forge ipykernel r-irkernel r-reticulate rpy2\n</code></pre> <ul> <li>Setup Python jupyter kernel for rey</li> </ul> <pre><code>python -m ipykernel install --user --name rey_py39 --display-name \"rey_py39\"\n\n## confirm that installation exited without any error\n## this should return 0 for successful installation\necho $?\n</code></pre> <ul> <li>Setup R jupyter kernel for rey</li> </ul> <pre><code>library(IRkernel)\ninstallspec(name = \"rey_r41\", displayname = \"rey_r41\", user = TRUE)\n\n## quit R session\nq(save = \"no\")\n</code></pre> <p>Configure Python and R kernel loading for rey</p> <p>Don't forget to tweak kernel loading as we did for yoda env else you may encounter issues running GPU-based packages in JupyterLab env.</p> <p>Pro Tip: You can use conditional expression in kernel wrapper, so kernel can only load on GPU-enabled HPC.</p> <pre><code>if [[ \"$(hostname)\" != *\"winter\"* ]]; then\necho -e \"ERROR: Invalid hostname\\nThis kernel works only on winter HPC\\n\" &gt;&amp;2\nexit 1\nelse\n#### Activate CONDA in subshell ####\n## Read https://github.com/conda/conda/issues/7980\nCONDA_BASE=$(conda info --base) &amp;&amp; \\\nsource \"${CONDA_BASE}\"/etc/profile.d/conda.sh &amp;&amp; \\\nconda activate rey\n    #### END CONDA SETUP ####\n\n## Load additional CUDA drivers, toolkit, etc.\n## if applicable prior to initializing kernel\n# module load cuda11.1/toolkit/11.1.1\n\n##... rest of kernel setup as explained earlier.\n</code></pre>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#renviron-setup","title":"Renviron setup","text":"<p>As explained above in the warning box, be careful installing packages using R from more than one conda envs and instead prefer using <code>mamba install</code> or <code>mamba update</code> to manage R packages.</p> <p>When we start R, it reads ~/.Renviron file or takes precedence based on order as detailed on CRAN - startup webpage. Accordingly, we will create a rey env-specific R Renviron file such that loading R in rey env will use rey specific library path to install new packages<sup>3</sup>, and will not install those under a default library path for yoda that we specified earlier in the setup.</p> <ul> <li>Let's create an empty directory to store rey env specific user R packages.</li> </ul> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/rey4.1\n</code></pre> <ul> <li>Create an env specific config directory at the place you like and create a Renviron file inside it.</li> </ul> <pre><code>mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/R/confs/rey\ncd /projects/verhaak-lab/amins/hpcenv/opt/R/confs/rey\n\n## create a Renviron file\nnano Renviron\n</code></pre> <ul> <li>Add following to Renviron file, i.e., we take the <code>R_LIBS</code> path from ~/.Renviron file we created earlier, and then prefix rey env specific paths to it. Here, rey env path consist of two parts:<ul> <li>First, /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/rey4.1 is a newly created custom path where <code>install.packages</code> command can install new packages while working in rey but not yoda env.</li> <li>Second, rey env default R library path that we got from <code>.libPaths()</code> output: /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/R/library. This path is used by <code>mamba install</code> or <code>mamba update</code> for managing R packages.</li> </ul> </li> </ul> <pre><code>R_LIBS=\"/projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/rey4.1:/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/R/library:/projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/4.1:/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/library\n</code></pre> <ul> <li>Now setup a custom loading of Renviron for rey env. This will make sure that R environ will switch/revert every time conda env, rey is loaded/unloaded via <code>mamba activate/deactivate</code> command.</li> </ul> <p>Example activate.d or deactivate.d scripts to manage conda envs</p> <p>You can view example scripts per respective conda env at  /confs/hpc/mambaforge/envs.</p> <pre><code>cd /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/etc/conda\n\n## create a new file\nnano activate.d/activate-r-env.sh\n</code></pre> <ul> <li>Add following to activate.d/activate-r-env.sh</li> </ul> <pre><code>#!/usr/bin/env sh\n\n## Define R_HOME from rey env\nR_HOME=\"$CONDA_PREFIX/lib/R\"\n## override ~/.Renviron which otherwise point to R from yoda env\nR_ENVIRON_USER=\"/projects/verhaak-lab/amins/hpcenv/opt/R/confs/rey/Renviron\"\n\nexport R_HOME R_ENVIRON_USER\n</code></pre> <p>load custom user setup after default setup</p> <p>bash startup reads file in alphanumeric order. There could be other R setup files, e.g., activate.d/activate-r-base.sh. So, make sure to name custom file(s), e.g., activate-r-env.sh file such that it loads after R specific default files. </p> <ul> <li>Similarly create a deactivate.d/deactivate-r-env.sh to unload custom changes when we do <code>mamba deactivate</code> to turn off rey env.</li> </ul> <pre><code>nano deactivate.d/deactivate-r-env.sh\n</code></pre> <ul> <li>and add following:</li> </ul> <pre><code>#!/usr/bin/env sh\n\n## fall back to pre-existing R env\nunset R_HOME R_ENVIRON_USER\n</code></pre> <p>Prefer <code>mamba deactivate</code> followed by <code>mamba activate &lt;env name&gt;</code></p> <p><code>unset</code> command will erase and not restore the matching custom env variables, if any. So, ideally you should do <code>mamba deactivate</code> to turn off rey and then do <code>mamba activate yoda</code> to properly activate yoda env to restore custom set env variables and all of env specific bash startup under respective activate.d/ directory. </p> <ul> <li>Now logout and login to winter HPC again. Do <code>mamba activate rey</code> and start R, and type <code>.libPaths()</code>. Now, exit R and type do <code>mamba deactivate</code> followed by <code>mamba activate yoda</code>. Start R and type <code>.libPaths()</code>. Notice difference in R library paths under two R sessions!</li> </ul>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#optional-setup","title":"Optional Setup","text":"<p>Following packages are optional for setup.</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#tensorboard","title":"Tensorboard","text":"<p>Tensorboard graphical user interface (GUI) ships with Tensorflow 2 and so does not require additional configuration.</p> <ul> <li>Check version for tensorflow and tensorboard</li> </ul> <pre><code>## in rey env\npython -c 'import tensorflow as tf; print(tf.__version__)' #2.6.2 or higher\npython -c 'import tensorboard as tb; print(tb.__version__)' #2.6.0 or higher\n</code></pre> <ul> <li>Checkout getting started guide for more on how to use GUI app. If tensorboard python notebook extension, <code>%load_ext tensorboard</code> fails to initialize tensorboard within notebook, you can manually initialize tensorboard using a terminal command as follows: </li> </ul> <pre><code>tensorboard serve --logdir logs --host &lt;IP address to bind to&gt;\n</code></pre> <p>where IP address can be a localhost or <code>hostname -I</code> as long as it is on the secure network. Tensorboard should be accessible at an unsecure http address shown in the output of above command.</p> <ul> <li>To quit tensorboard web server on the terminal, press Ctrl+C.</li> </ul>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#tensorrt","title":"TensorRT","text":"<p>NVIDIA\u00ae TensorRT\u2122 a software development kit (SDK) for NVIDIA compliant GPU cards. Conda does not provide TensorRT package, so we need to install it using getting started guide and installation using tarball instructions. This requires membership into NVIDIA developer program.</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#installation-steps","title":"Installation steps","text":"<ul> <li>Download tarball specific to CUDA and cuDNN version as determined by following commands. </li> </ul> <pre><code>## CUDA version, 11.1\nnvcc --version\n## cuDNN version 8.2\ncat ${CONDA_PREFIX}/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\n</code></pre> <ul> <li> <p>Accordingly, I have downloaded following tarball: </p> <ul> <li>TensorRT-8.2.2.1.Linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz</li> </ul> </li> <li> <p>Extract tarball to apps folder and rename path to extracted contents, so that we can load TensorRT as a module.</p> </li> </ul> <pre><code>cd \"${HPCAPPS}\"\n\nmkdir -p tensorrt\ncd tensorrt\n\n## place tarball in tensorrt directory and then extract it.\ntar xvzf TensorRT-8.2.2.1.Linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz\n\n## rename extracted directory for consistency on naming modules.\nmv TensorRT-8.2.2.1 8.2.2.1\n</code></pre> <ul> <li>To install TensorRT, we need to temporarily export TensorRT library path to LD_LIBRARY_PATH. For future logins to GPU HPC, we can then load this path as and when needed using modulefile or permanently insert this into LD_LIBRARY_PATH for GPU HPC using GPU-specific bash startup.</li> </ul> <pre><code>export LD_LIBRARY_PATH=\"${HPCAPPS}/tensorrt/8.2.2.1/lib:${LD_LIBRARY_PATH}\"\n</code></pre> <ul> <li>Install Python TensorRT wheel file. There are more than one file with different <code>cp3x</code>. I could not figure out what it means and so ended up installing the most recent one, i.e., <code>cp39</code>.</li> </ul> <pre><code>cd \"${HPCAPPS}\"/tensorrt/8.2.2.1/python &amp;&amp; \\\npip install tensorrt-8.2.2.1-cp39-none-linux_x86_64.whl |&amp; tee -a tensorrt_8.2.2.1_install.log\n</code></pre> Expected output: <pre><code>Processing ./tensorrt-8.2.2.1-cp39-none-linux_x86_64.whl\nInstalling collected packages: tensorrt\nSuccessfully installed tensorrt-8.2.2.1\n</code></pre> <ul> <li>Install Python UFF wheel file which is required for working with TensorFlow.</li> </ul> <pre><code>cd \"${HPCAPPS}\"/tensorrt/8.2.2.1/uff &amp;&amp; \\\npip install uff-0.6.9-py2.py3-none-any.whl |&amp; tee -a tensorrt_8.2.2.1_install.log\n</code></pre> Expected output: <pre><code>Processing ./uff-0.6.9-py2.py3-none-any.whl\nRequirement already satisfied: numpy&gt;=1.11.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from uff==0.6.9) (1.19.5)\nRequirement already satisfied: protobuf&gt;=3.3.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from uff==0.6.9) (3.18.1)\nInstalling collected packages: uff\nSuccessfully installed uff-0.6.9\n</code></pre> <p>When to use bash startup versus module file</p> <p>Above installation step should include <code>convert-to-uff</code> in PATH which you can check within output of <code>which convert-to-uff</code>. Since installation has already inserted binaries, e.g., <code>convert-to-uff</code> into bash PATH variable, I will now prefer to setup TensorRT related PATH and LD_LIBRARY_PATH using bash startup instead of loading TensorRT using module file. Module file works better if installation setup does not alter core bash startup variables like PATH and LD_LIBRARY_PATH.</p> <ul> <li>Install the Python graphsurgeon wheel file.</li> </ul> <pre><code>cd \"${HPCAPPS}\"/tensorrt/8.2.2.1/graphsurgeon &amp;&amp; \\\npip install graphsurgeon-0.4.5-py2.py3-none-any.whl |&amp; tee -a tensorrt_8.2.2.1_install.log\n</code></pre> Expected output: <pre><code>Processing ./graphsurgeon-0.4.5-py2.py3-none-any.whl\nInstalling collected packages: graphsurgeon\nSuccessfully installed graphsurgeon-0.4.5\n</code></pre> <ul> <li>Install the Python onnx-graphsurgeon wheel file.</li> </ul> <pre><code>cd \"${HPCAPPS}\"/tensorrt/8.2.2.1/onnx_graphsurgeon &amp;&amp; \\\npip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl |&amp; tee -a tensorrt_8.2.2.1_install.log\n</code></pre> Expected output: <pre><code>Processing ./onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl\nRequirement already satisfied: numpy in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx-graphsurgeon==0.3.12) (1.19.5)\nCollecting onnx\n  Downloading onnx-1.10.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.7 MB)\nRequirement already satisfied: protobuf in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx-&gt;onnx-graphsurgeon==0.3.12) (3.18.1)\nRequirement already satisfied: six in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx-&gt;onnx-graphsurgeon==0.3.12) (1.15.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.2.1 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx-&gt;onnx-graphsurgeon==0.3.12) (3.7.4.3)\nInstalling collected packages: onnx, onnx-graphsurgeon\nSuccessfully installed onnx-1.10.2 onnx-graphsurgeon-0.3.12\n</code></pre>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#test-gpu-functionality","title":"Test GPU functionality","text":"<p>Once we have rey env ready, we can test GPU functionality of installed packages. This is not a required step but I like to make sure that I am using GPU and not CPU for computation, e.g., tensorflow and r-keras package may fall back to CPU if it finds missing or badly configured support for GPU.</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#test-tensorflow-and-keras","title":"Test Tensorflow and Keras","text":"<p>I have followed beginner scripts from tensorflow tutorials to test GPU functionality. Similarly, RStudio section on Tensorflow for R provides beginners tutorials for testing machine learning using GPU.</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#test-pytorch","title":"Test PyTorch","text":"<p>See details on PyTorch website.</p> <pre><code>import torch\nx = torch.rand(5, 3)\nprint(x)\n</code></pre>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#test-tensorrt","title":"Test TensorRT","text":"<pre><code>cd \"${HPCAPPS}\"/tensorrt/8.2.2.1\n\n## ensure that gpu module is loaded\nmodule load gpu/11.1.1\n\ncd samples/sampleMNIST &amp;&amp; \\\nmake &amp;&amp; \\\necho \"make OK\"\n\ncd ../../data/mnist &amp;&amp; \\\n## Download MNIST dataset\nwget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz &amp;&amp; \\\nwget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz &amp;&amp; \\\nwget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz &amp;&amp; \\\nwget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n\nls *ubyte.gz | parallel -j2 gunzip {}\n\ncd ../.. &amp;&amp; \\\n./bin/sample_mnist -h &amp;&amp; \\\n./bin/sample_mnist --datadir=data/mnist\n</code></pre> <p>If all goes well, you will see tests passed ok and a predicted digit in ASCII art.</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#dask","title":"Dask","text":"<p>Read docs at http://distributed.dask.org/en/stable/client.html</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#image-classification","title":"Image Classification","text":"<p>Optional: Libraries specific to cell segmentation.</p> <ul> <li>I am creating a new conda env, ben for installing tools related to cell segmentation analysis. These tools require additional set of packages (including installing using <code>pip</code>) and are updated often which together can make rey env unstable over long run. Most of packages are based on package requirements for CellPose tool: setup.py and requirements.txt file.</li> </ul> <pre><code>mamba create -c conda-forge -c pytorch -n ben python=3.9 tensorflow-gpu keras pytorch torchvision cudatoolkit=11.1.1 cudatoolkit-dev=11.1.1 scikit-learn numpy scipy natsort tifffile tqdm numba torch-optimizer\n</code></pre> <ul> <li> <p>Before activating ben env, duplicate modulefile, <code>gpu/11.1.1</code> that we created earlier to <code>gpu/11.1.1_ben</code>. Replace conda env name from rey to ben in <code>gpu/11.1.1_ben</code>. This will allow to load a valid GPU env and avoid potential danger of putting rey paths in PATH and LD_LIBRARY_PATH while we work in ben env.</p> </li> <li> <p>Activate ben env</p> </li> </ul> <pre><code>mamba activate ben\n</code></pre> <p>Check for a valid bash env</p> <p>If you notice any of rey env related paths, especially taking precedence over ben env paths, something is wrong and you should check modulefiles above to load conda env specific valid env. Installing cellpose and related package dependencies with invalid bash env will invariably break the core, rey env.</p> <pre><code>module load gpu/11.1.1_ben\n\n## These should point to paths related to ben and not rey env.\necho $PATH\necho $LD_LIBRARY_PATH\n</code></pre>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#cellpose","title":"Cellpose","text":"<p>A generalized algorithm for cellular segmentation. MouseLand/cellpose</p> <ul> <li>Not recommended but given many dependencies for cellpose are not available or of conflicting nature using <code>mamba install</code>, I am falling back to <code>pip install</code>.</li> </ul> <pre><code>pip install cellpose[all] |&amp; tee -a ~/logs/cellpose_install.log\n</code></pre> <p>In case of errors or unstable env, I can always purge ben env without any impact on rey conda env.</p> Installation log and warnings, if any <pre><code>Installing collected packages: googleapis-common-protos, pyparsing, numpy, google-crc32c, google-api-core, PyWavelets, pyqt5.sip, PyQt5-Qt5, packaging, opencv-python-headless, networkx, imageio, google-resumable-media, google-cloud-core, fastremap, edt, scikit-image, pyqtgraph, pyqt5, google-cloud-storage, cellpose\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.19.5\n    Uninstalling numpy-1.19.5:\n      Successfully uninstalled numpy-1.19.5\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n\ntensorflow 2.6.2 requires numpy~=1.19.2, but you have numpy 1.21.5 which is incompatible.\n\nSuccessfully installed PyQt5-Qt5-5.15.2 PyWavelets-1.2.0 cellpose-0.7.2 edt-2.1.1 fastremap-1.12.2 google-api-core-2.4.0 google-cloud-core-2.2.1 google-cloud-storage-2.0.0 google-crc32c-1.3.0 google-resumable-media-2.1.0 googleapis-common-protos-1.54.0 imageio-2.13.5 networkx-2.6.3 numpy-1.21.5 opencv-python-headless-4.5.5.62 packaging-21.3 pyparsing-3.0.6 pyqt5-5.15.6 pyqt5.sip-12.9.0 pyqtgraph-0.11.0rc0 scikit-image-0.19.1\n</code></pre> <ul> <li>Turns out tensorflow 2 (GPU) works with updated numpy and should not be throw an error.</li> </ul> <pre><code>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n</code></pre> <ul> <li>CellPose should now be all set for running in ben env.</li> </ul> <pre><code>cellpose --help\n</code></pre>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#stardist","title":"Stardist","text":"<p>StarDist - Object Detection with Star-convex Shapes. stardist/stardist</p> <pre><code>## in ben env\npip install stardist |&amp; tee -a stardist_install.log\n</code></pre> Installation log and warnings, if any <pre><code>Installing collected packages: python-dateutil, kiwisolver, fonttools, cycler, matplotlib, csbdeep, stardist\nSuccessfully installed csbdeep-0.6.3 cycler-0.11.0 fonttools-4.28.5 kiwisolver-1.3.2 matplotlib-3.5.1 python-dateutil-2.8.2 stardist-0.7.3\n</code></pre> <ul> <li>To test run, follow example from stardist repo.</li> </ul>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#cellprofiler","title":"Cellprofiler","text":"<p>Tool for image analysis, cellprofiler.org</p> <p>Related bioformats2raw and raw2ometiff were downloaded as standalone binary packages and installed as modules.</p> <p>PS: Cellprofiler has a limited GPU support for now but it may change in the future. Follow Cellprofiler forum for updates. For now, I am installing it in grogu env which is a toy env!</p> <pre><code>## login to CPU HPC\nssh sumner </code></pre> <ul> <li>Create grogu conda env</li> </ul> <pre><code>mamba create -c conda-forge -c bioconda -n grogu cellprofiler\n</code></pre> <ul> <li>Run cellprofiler</li> </ul> <pre><code>mamba activate grogu\n\ncellprofiler --help\n</code></pre>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#update-bash-startup","title":"Update bash startup","text":"<p>Finally, I am tweaking bash startup sequence that we had setup earlier, such that it can allow loading GPU-specific bash env only when we login to Winter GPU HPC and not on Sumner CPU HPC. I have made following changes to bash startup. You can  download my bash startup files here.</p> <ul> <li>Update <code>SET PATH</code> block of ~/.bash_profile to reset PATH for Winter GPU. See my notes under <code>elif [[ \"$(hostname)\" == *\"winter\"* ]]; then</code> section in  an example .bash_profile file.</li> <li>Update ~/.profile.d/void/VW01_set_winter_gpu.sh to load Winter specific settings. See more into an example VW01_set_winter_gpu.sh file.</li> </ul> <p>Logout and login again to Winter HPC. You will see a near identical bash prompt like Sumner HPC, e.g., <code>user@winter-log1</code>. However, when you check <code>echo $PATH</code> output and <code>echo $CONDA_DEFAULT_ENV</code>, you will notice that a default conda env in Winter HPC is now rey while in Sumner HPC, it is base (sometimes called root).</p> <p>Of course, you can revert to base or any other conda env in Winter HPC by doing <code>mamba deactivate</code> (because we changed from base to rey during bash startup) and then <code>mamba activate base</code> (or yoda, or any other env).</p> <p>If you have also setup activate.d/deactivate.d scripts as detailed earlier, you will be able to fine tune loading and unloading of conda env specific to HPC type (CPU or GPU) as well as type of R and GPU-specific configs. See  /confs/hpc/mambaforge/envs for example scripts.</p>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"hpc/gpu/winter_1/#done","title":"Done!","text":"<p>Hope you have found this documentation helpful. I think this is more technical that I originally expected and you may have to look into stackoverflow or elsewhere to understand jargons I used across pages. Hopefully, I can go through some of sections again and put emphasis on rationale behind setting up my linux environment.</p> <p>That said, I hope this documentation, at least the CPU part, should get you started with HPC setup. For learning specific programming language and data analysis, I will post a few external resources on getting started guide to learn programming in Python, R, and more.</p> <p>Best wishes!  </p> <ol> <li> <p>There could be a difference though if both HPCs do not share common OS or they are using different system defaults, e.g., loading different bash env from <code>/etc/profile</code> which is managed by HPC staff.\u00a0\u21a9</p> </li> <li> <p>Installing common packages in yoda env and installing essentials.\u00a0\u21a9</p> </li> <li> <p>See warning box above on why our setup can still update R packages managed by yoda env despite using custom Renviron file.\u00a0\u21a9</p> </li> </ol>","tags":["hpc","setup","gpu","deep learning","imaging","programming"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#bash","title":"bash","text":"<ul> <li>Setting up CPU env - Part 1</li> </ul>"},{"location":"tags/#cellpose","title":"cellpose","text":"<ul> <li>Image analysis on GPU-based HPC</li> </ul>"},{"location":"tags/#cellprofiler","title":"cellprofiler","text":"<ul> <li>Image analysis on GPU-based HPC</li> </ul>"},{"location":"tags/#conda","title":"conda","text":"<ul> <li>Setting up CPU env - Part 1</li> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> </ul>"},{"location":"tags/#database","title":"database","text":"<ul> <li>Setting up CPU env - Part 3</li> </ul>"},{"location":"tags/#deep-learning","title":"deep learning","text":"<ul> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#gpu","title":"gpu","text":"<ul> <li>Image analysis on GPU-based HPC</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#howto","title":"howto","text":"<ul> <li>Run OnDemand App with a custom conda env</li> </ul>"},{"location":"tags/#hpc","title":"hpc","text":"<ul> <li>Run OnDemand App with a custom conda env</li> <li>Setting up CPU env - Part 1</li> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> <li>Image analysis on GPU-based HPC</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#imaging","title":"imaging","text":"<ul> <li>Image analysis on GPU-based HPC</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#jupyter","title":"jupyter","text":"<ul> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> </ul>"},{"location":"tags/#kernels","title":"kernels","text":"<ul> <li>Setting up CPU env - Part 2</li> </ul>"},{"location":"tags/#onboarding","title":"onboarding","text":"<ul> <li>Run OnDemand App with a custom conda env</li> </ul>"},{"location":"tags/#ondemand","title":"ondemand","text":"<ul> <li>Run OnDemand App with a custom conda env</li> </ul>"},{"location":"tags/#programming","title":"programming","text":"<ul> <li>Setting up CPU env - Part 1</li> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#segmentation","title":"segmentation","text":"<ul> <li>Image analysis on GPU-based HPC</li> </ul>"},{"location":"tags/#setup","title":"setup","text":"<ul> <li>Setting up CPU env - Part 1</li> <li>Setting up CPU env - Part 2</li> <li>Setting up CPU env - Part 3</li> <li>Setting up GPU env</li> </ul>"},{"location":"tags/#stardist","title":"stardist","text":"<ul> <li>Image analysis on GPU-based HPC</li> </ul>"},{"location":"tags/#startup","title":"startup","text":"<ul> <li>Setting up CPU env - Part 3</li> </ul>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/howto/","title":"HowTo","text":""}]}