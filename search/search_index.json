{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"My notes on programming, mostly using R, Python, and bash.","title":"Home"},{"location":"disclosure/","text":"Copyrights \u00b6 This website inherits copyright and disclosures details from a parent website, SBAmin.com and it is available at sbamin.com/disclosure . Source code for this website is available at sbamin/code101 under MIT license . Use of Material for MkDocs theme is under MIT LICENSE agreement from Martin Donath . Privacy \u00b6 This website uses cookies, in particular, Google Analytics to monitor your one-time or repeated visits and page visit behavior. These cookies also measure relevant sections and pages of website which are frequently or rarely visited, e.g., using user-defined search keywords, and thus help me optimize contents and relevant meta data to make this documentation better. This website also uses other third-party cookies which are shipped along with use of their respective services, e.g., content-delivery network, java scripts, custom fonts, etc. These services are required for a proper functioning (aesthetics at-large) of this website. However, disabling these cookies should not render this website non-functional. Implicit consent until cookie consent is enabled on this site For time being, your visit to this site will imply an automatic consent of yours to accept these cookie policies. Until I update user-enabled cookie consent, you are encouraged to use web browser default settings or browser add-ons that can block such cookies.","title":"Copyrights and Privacy Policy"},{"location":"disclosure/#copyrights","text":"This website inherits copyright and disclosures details from a parent website, SBAmin.com and it is available at sbamin.com/disclosure . Source code for this website is available at sbamin/code101 under MIT license . Use of Material for MkDocs theme is under MIT LICENSE agreement from Martin Donath .","title":"Copyrights"},{"location":"disclosure/#privacy","text":"This website uses cookies, in particular, Google Analytics to monitor your one-time or repeated visits and page visit behavior. These cookies also measure relevant sections and pages of website which are frequently or rarely visited, e.g., using user-defined search keywords, and thus help me optimize contents and relevant meta data to make this documentation better. This website also uses other third-party cookies which are shipped along with use of their respective services, e.g., content-delivery network, java scripts, custom fonts, etc. These services are required for a proper functioning (aesthetics at-large) of this website. However, disabling these cookies should not render this website non-functional. Implicit consent until cookie consent is enabled on this site For time being, your visit to this site will imply an automatic consent of yours to accept these cookie policies. Until I update user-enabled cookie consent, you are encouraged to use web browser default settings or browser add-ons that can block such cookies.","title":"Privacy"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: bash \u00b6 Setting up CPU env - Part 1 conda \u00b6 Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 database \u00b6 Setting up CPU env - Part 3 deep learning \u00b6 Setting up GPU env gpu \u00b6 Setting up GPU env hpc \u00b6 Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env imaging \u00b6 Setting up GPU env jupyter \u00b6 Setting up CPU env - Part 2 Setting up CPU env - Part 3 kernels \u00b6 Setting up CPU env - Part 2 programming \u00b6 Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env setup \u00b6 Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env startup \u00b6 Setting up CPU env - Part 3","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#bash","text":"Setting up CPU env - Part 1","title":"bash"},{"location":"tags/#conda","text":"Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3","title":"conda"},{"location":"tags/#database","text":"Setting up CPU env - Part 3","title":"database"},{"location":"tags/#deep-learning","text":"Setting up GPU env","title":"deep learning"},{"location":"tags/#gpu","text":"Setting up GPU env","title":"gpu"},{"location":"tags/#hpc","text":"Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env","title":"hpc"},{"location":"tags/#imaging","text":"Setting up GPU env","title":"imaging"},{"location":"tags/#jupyter","text":"Setting up CPU env - Part 2 Setting up CPU env - Part 3","title":"jupyter"},{"location":"tags/#kernels","text":"Setting up CPU env - Part 2","title":"kernels"},{"location":"tags/#programming","text":"Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env","title":"programming"},{"location":"tags/#setup","text":"Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env","title":"setup"},{"location":"tags/#startup","text":"Setting up CPU env - Part 3","title":"startup"},{"location":"hpc/","text":"Hello World! \u00b6 I work in computational cancer biology for over a decade now. While true scale of big data can vary across fields, I work on daily basis to analyze large-scale genomic data on the high-performance computing (HPC) infrastructure. Once every two years or so, I reset my HPC environment (env) so as to upgrade major version for programming languages, mainly R and Python. Here, reset as in I force my HPC login terminal window to behave as if I am login to HPC for the first time with system defaults. I also prefer doing such hard reset over patching cumulative updates and tips I get from my twitterverse over those two years. I usually write notes while I do such resets and over years, I have found it quite useful, especially for debugging and when moving to new places with new HPC envs. So, this time, I thought to make these notes available here :fontawesome-solid-globe-africa: with those starting for the first time in HPC env or even programming on a linux run machine as the primary audience . I started my journey in computational biology at Dana-Farber back in 2008. The entire programming env was quite new and daunting to me with no prior experience in programming. I recollect typing cd , chdir , etc. commands in Windows\u00ae DOS terminal for not more than an hour when our teachers introduced us about computers during my high-school years! Nonetheless, I got curious looking at that terminal and then arrival of the internet during my medical school years fueled my curiosity and helped me get familiar with computers. I was not skilled and in fact, far from comfortable the time I joined Dana-Farber but along the ride, I was helped by my colleagues and mentors, and those with whom I'd not interacted in-person: In particular Hadley Wickham and tidyverse team for sharing an excellent documentation on programming in R and countless users at stackoverflow and biostars forums for sharing their ideas and bugfixes. So, it's paying it forward time and hope some of you find this documentation useful and a step closer towards a skilled programmer. Scope of this documentation I must say though that this website is more geared towards getting you started with linux computing and an attempt to lower the resistance to working on the terminal , i.e., to set up a stable linux env so that you can focus on learning programming instead of debugging on why certain commands or program fail to work. That said, this website is not intended to teach you on how to program in R or Python or other languages. I am certain that there are several excellent online course and textbook materials available for such learning. I will add a few of my favorite materials here in the future. I have tried to provide logic behind most of commands, including external links for technical words. However, you may find several keywords or phrases as jargon, e.g., you may wonder what pip install is or what is kernel? In such cases, I suggest to at least do a web search and familiarize yourself on what those words/phrases mean and why they could be relevant as you master your programming skills. Getting Started \u00b6 To get started, I have partitioned documentation into two major sections. The first section (CPU Computing) details - in three parts - how to setup HPC env using conda based package management system. In Part 1 , we will start with HPC setup either from the scratch as in login to HPC on the day one or will reset or overhaul an existing HPC env 1 . We will install conda and set a default and minimal bash startup profile. In Part 2 , we will setup two or more dedicated conda env to host commonly used programming languages in computational biology: R along with Python 3 that ships with conda. We will also setup Jupyter Lab , a popular user interface similar to RStudio to interact with hosted programming languages and files over the secure internet browser. Finally, in Part 3 , we will charge up our HPC env by adding Julia - another popular language in computational biology. We will also install a few drivers and jupyter kernels to interact with externally hosted databases like SQL or postgresql. I will also outline setting up Modules , Snakemake -based workflows, and Singularity container system. Finally, we will launch our custom HPC env using a mighty bash startup sequence Second section (GPU computing) is an optional setup and intended for those working on GPU-enabled HPCs. We will walk through setting up commonly used GPU libraries, e.g., Tensorflow 2 , Keras , and PyTorch . I am going to update this section in the near future. Feedback \u00b6 Source code of this website is available at sbamin/code101 . I welcome user contributions, including bugfixes, enhancements, and alternative approaches. Since I update my linux env once every two years or so, I will not be updating this website often. If I encounter major bugs in my working env, I will update this website accordingly. This website is tightly linked to related github code repository at sbamin/code101 , including page comments which are powered by GitHub Discussions . Please use github issues and pull requests for a bug report and contributions, respectively. For more, you can tag/follow me at @sbamin . Acknowledgments \u00b6 Big shout-out to Martin Donath for sharing this great documentation ecosystem at Material for MkDocs . Over last three years, it has helped me a lot in documenting my daily work, progress notes, etc. Preferably requires prior working experience with HPC env and following cautionary notes I have placed to do such reset. \u21a9","title":"Getting started with HPC"},{"location":"hpc/#hello-world","text":"I work in computational cancer biology for over a decade now. While true scale of big data can vary across fields, I work on daily basis to analyze large-scale genomic data on the high-performance computing (HPC) infrastructure. Once every two years or so, I reset my HPC environment (env) so as to upgrade major version for programming languages, mainly R and Python. Here, reset as in I force my HPC login terminal window to behave as if I am login to HPC for the first time with system defaults. I also prefer doing such hard reset over patching cumulative updates and tips I get from my twitterverse over those two years. I usually write notes while I do such resets and over years, I have found it quite useful, especially for debugging and when moving to new places with new HPC envs. So, this time, I thought to make these notes available here :fontawesome-solid-globe-africa: with those starting for the first time in HPC env or even programming on a linux run machine as the primary audience . I started my journey in computational biology at Dana-Farber back in 2008. The entire programming env was quite new and daunting to me with no prior experience in programming. I recollect typing cd , chdir , etc. commands in Windows\u00ae DOS terminal for not more than an hour when our teachers introduced us about computers during my high-school years! Nonetheless, I got curious looking at that terminal and then arrival of the internet during my medical school years fueled my curiosity and helped me get familiar with computers. I was not skilled and in fact, far from comfortable the time I joined Dana-Farber but along the ride, I was helped by my colleagues and mentors, and those with whom I'd not interacted in-person: In particular Hadley Wickham and tidyverse team for sharing an excellent documentation on programming in R and countless users at stackoverflow and biostars forums for sharing their ideas and bugfixes. So, it's paying it forward time and hope some of you find this documentation useful and a step closer towards a skilled programmer. Scope of this documentation I must say though that this website is more geared towards getting you started with linux computing and an attempt to lower the resistance to working on the terminal , i.e., to set up a stable linux env so that you can focus on learning programming instead of debugging on why certain commands or program fail to work. That said, this website is not intended to teach you on how to program in R or Python or other languages. I am certain that there are several excellent online course and textbook materials available for such learning. I will add a few of my favorite materials here in the future. I have tried to provide logic behind most of commands, including external links for technical words. However, you may find several keywords or phrases as jargon, e.g., you may wonder what pip install is or what is kernel? In such cases, I suggest to at least do a web search and familiarize yourself on what those words/phrases mean and why they could be relevant as you master your programming skills.","title":"Hello World!"},{"location":"hpc/#getting-started","text":"To get started, I have partitioned documentation into two major sections. The first section (CPU Computing) details - in three parts - how to setup HPC env using conda based package management system. In Part 1 , we will start with HPC setup either from the scratch as in login to HPC on the day one or will reset or overhaul an existing HPC env 1 . We will install conda and set a default and minimal bash startup profile. In Part 2 , we will setup two or more dedicated conda env to host commonly used programming languages in computational biology: R along with Python 3 that ships with conda. We will also setup Jupyter Lab , a popular user interface similar to RStudio to interact with hosted programming languages and files over the secure internet browser. Finally, in Part 3 , we will charge up our HPC env by adding Julia - another popular language in computational biology. We will also install a few drivers and jupyter kernels to interact with externally hosted databases like SQL or postgresql. I will also outline setting up Modules , Snakemake -based workflows, and Singularity container system. Finally, we will launch our custom HPC env using a mighty bash startup sequence Second section (GPU computing) is an optional setup and intended for those working on GPU-enabled HPCs. We will walk through setting up commonly used GPU libraries, e.g., Tensorflow 2 , Keras , and PyTorch . I am going to update this section in the near future.","title":"Getting Started"},{"location":"hpc/#feedback","text":"Source code of this website is available at sbamin/code101 . I welcome user contributions, including bugfixes, enhancements, and alternative approaches. Since I update my linux env once every two years or so, I will not be updating this website often. If I encounter major bugs in my working env, I will update this website accordingly. This website is tightly linked to related github code repository at sbamin/code101 , including page comments which are powered by GitHub Discussions . Please use github issues and pull requests for a bug report and contributions, respectively. For more, you can tag/follow me at @sbamin .","title":"Feedback"},{"location":"hpc/#acknowledgments","text":"Big shout-out to Martin Donath for sharing this great documentation ecosystem at Material for MkDocs . Over last three years, it has helped me a lot in documenting my daily work, progress notes, etc. Preferably requires prior working experience with HPC env and following cautionary notes I have placed to do such reset. \u21a9","title":"Acknowledgments"},{"location":"hpc/cpu/sumner_1/","tags":["hpc","setup","conda","bash","programming"],"text":"Set up for HPC Sumner \u00b6 There are at least two scenarios with an existing HPC env: We start setting up HPC env from scratch, i.e., right after login to HPC login node for the first time or We already have an existing custom setup, e.g., using linuxbrew or even conda, and we like to start from fresh setup. This is true (as in my case) when you may have setup conda env in your home directory and conda env grew in size over time that you are now approaching disk quota of 50 GB for home directory. So, now we like to move it to tier 1 space ( /projects/ ) which has disk quota in TBs in not GBs. Following will guide setting up HPC env for both scenarios. Change user paths per your username and HPC paths Most of setup below have commands and locations (paths) tied to my username, amins and our HPC cluster at The Jackson Laboratory (JAX) , namely Sumner and Winter HPC, one each of CPU and GPU-based computing. Please ensure that you edit paths such that it reflects your username and paths that are available for HPC at your institute. Option 1: Start from scratch \u00b6 ssh userid@login.sumner.jax.org ## Know OS and kernel version cat /etc/redhat-release uname -a Running CentOS Linux release 7.7.1908 (Core) Linux sumner-log1 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux First, login to sumner with a clean env, i.e., as it ships with default profile from HPC team, and nothing added in following files. Default bash configuration for sumner looks similar to following files. See example files in the source at confs/hpc/initials/ . ~/.bashrc ~/.bash_profile ~/.bash_aliases # if it exists ~/.profile # if it exists If you had custom bash configs (linuxbrew, previous conda, etc.), disable those by commenting out from above files. If you'd linuxbrew installed, make sure to disable it unless you are confident that conda and brew can work in harmony! Same goes for ~/.local/ directory which should not exist at the fresh startup. If it does, you may have installed some tools using python, perl, or other non-root based setup scripts. For clean setup, ~/.local directory needs to be removed from user environment, i.e., either rename it to say, ~/.local_deprecated or archive it somewhere! If you have made significant changes to HPC env, it is better to follow Option 2. Option 2: Override existing setup \u00b6 Here, we essentially revert from our existing custom setup to fresh HPC env that we had on the day one of login to HPC, and then we start setting up a fresh HPC env that includes conda env and other space-occupying (julia, custon apps, etc.) softwares to resolve disk quota issue. In order to reset to the fresh HPC env that we had on day one of login, we need to reset the entire bash login env to that of day one. Most of custom HPC env is managed by series of dot files (.bashrc, .Renviron, .Rprofile, etc.) and directories (.local, .config, etc.) in your home directory. So, in order to reset to day one env, I will archive these dot files and directories away from home directory (say ~/legacy_setup/ ), and replace with dot files from HPC default env that we had on the day one of login 1 . Please note that if you are using non-bash shell, e.g., zsh or other, you do need to make sure to reset login env to the HPC default bash env. \u2620\ufe0f Be careful moving dotfiles \u2620\ufe0f Moving some of dotfiles is tricky as some of those files are needed for login to sumner, e.g., files within ~/.ssh/ directory, If you are doing this, make sure NOT to logout of sumner and at the end of executing this code block on sumner, make sure that you can login from another terminal to sumner. Following script should be run manually unless you know for sure that it will exit without any error and will move all except essential dotfiles and dot directories to an archived directory. Also, make sure to check exit code by running echo $? immediately after running critical commands to make sure you had no error running those commands. ssh userid@login.sumner.jax.org cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" ## make an empty archived directory mkdir -p \" ${ HOME } \" /legacy_env ## list files and directories that we will archive ls -alh \" ${ HOME } \" /. [ ^. ] * | tee -a \" ${ HOME } \" /legacy_env/list_dotfiles_dirs_ \" $( date +%d%b%y_%H%M%S_%Z ) \" .txt echo $? ## list HPC env that we will archive env | tee -a \" ${ HOME } \" /legacy_env/hpc_sumner_env_ \" $( date +%d%b%y_%H%M%S_%Z ) \" .log echo $? ## Moving all dot files and dot directories ## Make sure to check exit code mv \" ${ HOME } \" /. [ ^. ] * legacy_env/ echo $? Copy essential files and directories back to home directory Do not forget to copy back following files to \"${HOME}\" else you may get locked out of sumner. You may not have all of following files/directories but at least run each command once to ensure that essential files/directories, e.g., ~/.ssh/ are in the home directory. Ideally, confirm with your HPC staff on which files and directories are essential as HPC env may vary across institutes. cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" ## sumner ssh dir rsync -avhP legacy_env/.ssh ./ ## sumner login tokens, if any cp legacy_env/.vas_* ./ cp legacy_env/.ksh* ./ cp legacy_env/.k5login ./ rsync -avhP legacy_env/.pki ./ ## optional files, if any ## singularity may take a larger space rsync -avhP legacy_env/.singularity ./ rsync -avhP legacy_env/.terminfo ./ rsync -avhP legacy_env/.subversion ./ cp legacy_env/.emacs ./ cp legacy_env/.viminfo ./ cp legacy_env/.screenrc ./ Make following empty dirs. These are unix specific configuration directories where some of softwares we install at later will keep their respective configurations. Note that we already backed up previous configurations in these directories under ~/legacy_env cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" mkdir -p \" ${ HOME } \" /.cache mkdir -p \" ${ HOME } \" /.config mkdir -p \" ${ HOME } \" /.local ## list dotfiles and dot dirs after reset ls -alh \" ${ HOME } \" /. [ ^. ] * Notice that we now have a fewer (and essential) dot files and dot directories right under home directory. Also, notice that we are missing essential dotfiles for loading bash login env, e.g., ~/.bashrc and ~/.bash_profile. These two files should originally be present when we logged to HPC on day one. Let's copy those original files back to home directory. I am using following default bash dotfiles but it may vary across different HPC env. Contact your HPC staff for more. You can copy following dotfile and use text editor like nano or vi to paste contents to respective dotfiles. ~/.bash_profile ~/.bashrc # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ] ; then . ~/.bashrc fi # User specific environment and startup programs PATH = $PATH : $HOME /.local/bin: $HOME /bin export PATH # .bashrc # Source global definitions if [ -f /etc/bashrc ] ; then . /etc/bashrc fi # Uncomment the following line if you don't like systemctl's auto-paging feature: # export SYSTEMD_PAGER= # User specific aliases and functions # This may vary across different HPC module load gcc Now, you should ensure that you can login to HPC from a separate terminal. Do not logout from an existing terminal yet! ssh userid@login.sumner.jax.org env If above command succeeds and env looks similar (PATH in particular) to outputs of default env variables (set by HPC staff) below, you're good! You can then exit old sumner session and install anaconda3 from a new (with a fresh or reset env) terminal session. bash commands expected output ## paths where all executables can be found echo $PATH ## paths where shared libraries are available to run programs echo $LD_LIBRARY_PATH ## Used by gcc before compiling program ## Read https://stackoverflow.com/a/4250666/1243763 echo $LIBRARY_PATH ## default loaded modules module list /cm/local/apps/gcc/8.2.0/bin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.:/home/amins/.local/bin:/home/amins/bin /cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64:/cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 /cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 Currently Loaded Modules: 1) shared 3) dot 5) slurm/18.08.8 2) DefaultModules 4) default-environment 6) gcc/8.2.0 Know what changes module load command can do When you load a module, it configures one or more of PATH, LD_LIBRARY_PATH, and other env variables. Command: module show <module name> can show you list of changes that a module makes during loading, e.g., module show gcc/8.2.0 In this case, loading gcc module will change PATH and LD_LIBRARY_PATH variables by perpending following respective paths. module unload gcc should remove these paths. -------------------------------------------------------------------------------------------- /cm/local/modulefiles/gcc/8.2.0: -------------------------------------------------------------------------------------------- whatis(\"adds GNU Cross Compilers to your environment variables \") prepend_path(\"PATH\",\"/cm/local/apps/gcc/8.2.0/bin\") prepend_path(\"LD_LIBRARY_PATH\",\"/cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64\") help([[ Adds GNU Cross Compilers to your environment variables, ]]) Make sure to logout and login to sumner again for a clean env to take an effect. Once you login, your env should look something similar to above. Note that PATH and LD_LIBRARY_PATH variables should default to Cent OS 7 standard paths with no user-defined paths except /home/amins/.local/bin:/home/amins/bin if those directories are present. Careful with user-defined paths For error-free setup, these - /home/amins/.local/bin:/home/amins/bin - user-defined directories should be empty to begin with and must not take precedence over system-default paths in PATH and LD_LIBRARY_PATH. Once we install anaconda3 and other tools, we will modify ~/.bash_profile and loading of bash login env such that user-defined paths override system-default paths. exit #from sumner ## login again ssh login.sumner.jax.org Store default hpc configuration Useful to fall back to HPC defaults if something goes awry! mkdir -p ~/bkup/confs/hpc_default_env/ cp .bashrc bkup/confs/hpc_default_env/ cp .bash_profile bkup/confs/hpc_default_env/ ## export global env env | tee -a \" ${ HOME } \" /bkup/confs/hpc_default_env/hpc_sumner_env_ \" $( date +%d%b%y_%H%M%S_%Z ) \" .log Configure defaults \u00b6 Following configuration can vary across HPC env. Specifically, I prefer to keep minimal modules (tools that HPC staff installs as default) that are critical for login to HPC and compiling certain packages, e.g., CUDA libraries for Winter GPU-based HPC (detailed later in the setup). dot module only appends . to PATH variable (see module show dot ), so that you do not need to prefix ./ to run an executable file under present/current working directory. Since I do not need dot module, I will override default module loading by doing module unload dot in my bash configuration (later). For now, I do not need system gcc and will rely on conda-installed gcc and other devtools x86_64-conda_cos6-linux-gnu-* . More on that later but let's unload dot and gcc first. module unload dot module unload gcc module list Currently Loaded Modules: 1) shared 2) DefaultModules 3) default-environment 4) slurm/18.08.8 For now, you may add following cmd to your ~/.bash_profile to unload dot and gcc at each login to HPC. Eventually it will go to ~/.profile.d/ setup detailed below. module unload dot module unload gcc Make sure that gcc is unloaded While settings in ~/.bash_profile should be respected during login to HPC, sometimes starting a pseudo-terminal, e.g., screen or tmux session may not source ~/.bash_profile due to directives related to interactive versus non-interactive session . Then, you may notice that gcc module is not unloaded and still present under module list output. If so, manually unload gcc by module unload gcc after each time you enter into screen or tmux session or interactive HPC session (detailed below). If you do not add unload command to ~/.bash_profile and rather rely on manually running unload command prior to anaconda installation, you do need ensure that these modules are unloaded in your current terminal, especially when starting a pseudo-terminal like screen, tmux, or slurm interactive job. In summary, make sure to do module unload gcc before running setup further. exit #from sumner ## login again ssh login.sumner.jax.org Install conda \u00b6 Download and install anaconda3. I am using a variant of anaconda3 called, Mambaforge . It is based on Miniforge, a minimal conda installer (similar to minioconda) with an added support for conda-forge as a default channel and use of Mamba instead of default conda command to manage packages. cd \" $HOME \" && \\ mkdir -p Downloads/conda && \\ cd Downloads/conda && \\ wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh && \\ md5sum Mambaforge-Linux-x86_64.sh > Mambaforge-Linux-x86_64.sh.md5 md5: ab95d7b4fb52c299e92b04d7dc89fa95 Mambaforge-Linux-x86_64.sh Prefer running setup on a dedicated interactive node instead of login node. Some of compute/memory-intensive conda install/update steps may get killed on a login node. ## start screen session prior to running interactive session ## doing so will keep remote interactive session alive if your connection ## from local computer to HPC is lost. screen ## run interactive job ## options may vary across HPC srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash ## unload gcc if loaded module unload gcc You may not notice a change in login env except that your login prompt may change from a login node: user@sumner-log1 to one of compute nodes: user@sumner50 . By default, conda will setup ~/anaconda3 (or ~/mambaforge if using mambaforge) under home directory. Since conda env can grow over time and home directories are typically capped at 50 GB or so (at least with our HPC env), we will setup conda env on tier 1 space at /projects/verhaak-lab/amins/hpcenv/mambaforge cd \" $HOME \" && \\ bash \" ${ HOME } \" /Downloads/conda/Mambaforge-Linux-x86_64.sh Accept to license agreement and then set installation path to tier 1 space, e.g., /projects/verhaak-lab/amins/hpcenv/mambaforge in my case. Note that this will vary based on your username and available location on your HPC where you can store large amount of data. Conda env and related setup can grow over time and may exceed typical 50 GB quota for a user home directory. So, prefer installing conda and related env at location where you can store more data. For JAX, it's called tier 1 space under /projects/<lab_name>/<user_name>/ path. Installer will start installing conda env and towards the end, it will prompt you for initializing conda env. Say yes! If you say no, you can follow instructions that installer outputs to ensure that you have a working conda env each time you login to HPC. To do so, conda needs to write a few lines of code to ~/.bashrc file, so that HPC login env will always start with a valid (by modifying PATH and a several other env variables) conda env. Do you wish the installer to initialize Mambaforge by running conda init? [yes|no] [no] >>> no You have chosen to not have conda modify your shell scripts at all. To activate conda's base environment in your current shell session: eval \"$(/projects/verhaak-lab/amins/hpcenv/mambaforge/bin/conda shell.YOUR_SHELL_NAME hook)\" ## where YOUR_SHELL_NAME is bash or zsh or other shells. To install conda's shell functions for easier access, first activate, then: conda init If you'd prefer that conda's base environment not be activated on startup, set the auto_activate_base parameter to false: conda config --set auto_activate_base false Thank you for installing Mambaforge! Since I typed no above, I need to manually activate conda by following steps. cd \" ${ HOME } \" ## replace shell.bash with shell.zsh or other shells you may be using by now. ## know which shell you are using in HPC echo \" $( basename ${ SHELL } ) \" ## activate conda base env in the current terminal eval \" $( /projects/verhaak-lab/amins/hpcenv/mambaforge/bin/conda shell.bash hook ) \" Once conda base env has been activated, you will notice your login prompt changing from user@sumner50 to (base) [userid@sumner50] . You can also check which conda env you are in by running echo $CONDA_DEFAULT_ENV . Since we have not installed additional conda env yet, we only have base env to begin with. You can also run echo $CONDA_PREFIX to confirm that conda has been installed on non-default, tier 1 path and not under ~/mambaforge . Now, let conda edit ~/.bashrc file so conda can load base env each time we login to HPC. conda init ## if using mamba, also run mamba init ## to enable mamba activate/deactivate env mamba init ## Check what code has been added to ~/.bashrc cat ~/.bashrc You will notice that conda has now added initialization code to ~/.bashrc Our minimal conda installation is now complete. Logout from interactive session, and then logout from HPC. # exit from interactive session exit ## exit from HPC exit In Part 2 , we will login and start an interactive session again to customize conda and HPC env. There were no default dot directories on the day one. \u21a9","title":"Part 1"},{"location":"hpc/cpu/sumner_1/#set-up-for-hpc-sumner","text":"There are at least two scenarios with an existing HPC env: We start setting up HPC env from scratch, i.e., right after login to HPC login node for the first time or We already have an existing custom setup, e.g., using linuxbrew or even conda, and we like to start from fresh setup. This is true (as in my case) when you may have setup conda env in your home directory and conda env grew in size over time that you are now approaching disk quota of 50 GB for home directory. So, now we like to move it to tier 1 space ( /projects/ ) which has disk quota in TBs in not GBs. Following will guide setting up HPC env for both scenarios. Change user paths per your username and HPC paths Most of setup below have commands and locations (paths) tied to my username, amins and our HPC cluster at The Jackson Laboratory (JAX) , namely Sumner and Winter HPC, one each of CPU and GPU-based computing. Please ensure that you edit paths such that it reflects your username and paths that are available for HPC at your institute.","title":"Set up for HPC Sumner"},{"location":"hpc/cpu/sumner_1/#option-1-start-from-scratch","text":"ssh userid@login.sumner.jax.org ## Know OS and kernel version cat /etc/redhat-release uname -a Running CentOS Linux release 7.7.1908 (Core) Linux sumner-log1 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux First, login to sumner with a clean env, i.e., as it ships with default profile from HPC team, and nothing added in following files. Default bash configuration for sumner looks similar to following files. See example files in the source at confs/hpc/initials/ . ~/.bashrc ~/.bash_profile ~/.bash_aliases # if it exists ~/.profile # if it exists If you had custom bash configs (linuxbrew, previous conda, etc.), disable those by commenting out from above files. If you'd linuxbrew installed, make sure to disable it unless you are confident that conda and brew can work in harmony! Same goes for ~/.local/ directory which should not exist at the fresh startup. If it does, you may have installed some tools using python, perl, or other non-root based setup scripts. For clean setup, ~/.local directory needs to be removed from user environment, i.e., either rename it to say, ~/.local_deprecated or archive it somewhere! If you have made significant changes to HPC env, it is better to follow Option 2.","title":"Option 1: Start from scratch"},{"location":"hpc/cpu/sumner_1/#option-2-override-existing-setup","text":"Here, we essentially revert from our existing custom setup to fresh HPC env that we had on the day one of login to HPC, and then we start setting up a fresh HPC env that includes conda env and other space-occupying (julia, custon apps, etc.) softwares to resolve disk quota issue. In order to reset to the fresh HPC env that we had on day one of login, we need to reset the entire bash login env to that of day one. Most of custom HPC env is managed by series of dot files (.bashrc, .Renviron, .Rprofile, etc.) and directories (.local, .config, etc.) in your home directory. So, in order to reset to day one env, I will archive these dot files and directories away from home directory (say ~/legacy_setup/ ), and replace with dot files from HPC default env that we had on the day one of login 1 . Please note that if you are using non-bash shell, e.g., zsh or other, you do need to make sure to reset login env to the HPC default bash env. \u2620\ufe0f Be careful moving dotfiles \u2620\ufe0f Moving some of dotfiles is tricky as some of those files are needed for login to sumner, e.g., files within ~/.ssh/ directory, If you are doing this, make sure NOT to logout of sumner and at the end of executing this code block on sumner, make sure that you can login from another terminal to sumner. Following script should be run manually unless you know for sure that it will exit without any error and will move all except essential dotfiles and dot directories to an archived directory. Also, make sure to check exit code by running echo $? immediately after running critical commands to make sure you had no error running those commands. ssh userid@login.sumner.jax.org cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" ## make an empty archived directory mkdir -p \" ${ HOME } \" /legacy_env ## list files and directories that we will archive ls -alh \" ${ HOME } \" /. [ ^. ] * | tee -a \" ${ HOME } \" /legacy_env/list_dotfiles_dirs_ \" $( date +%d%b%y_%H%M%S_%Z ) \" .txt echo $? ## list HPC env that we will archive env | tee -a \" ${ HOME } \" /legacy_env/hpc_sumner_env_ \" $( date +%d%b%y_%H%M%S_%Z ) \" .log echo $? ## Moving all dot files and dot directories ## Make sure to check exit code mv \" ${ HOME } \" /. [ ^. ] * legacy_env/ echo $? Copy essential files and directories back to home directory Do not forget to copy back following files to \"${HOME}\" else you may get locked out of sumner. You may not have all of following files/directories but at least run each command once to ensure that essential files/directories, e.g., ~/.ssh/ are in the home directory. Ideally, confirm with your HPC staff on which files and directories are essential as HPC env may vary across institutes. cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" ## sumner ssh dir rsync -avhP legacy_env/.ssh ./ ## sumner login tokens, if any cp legacy_env/.vas_* ./ cp legacy_env/.ksh* ./ cp legacy_env/.k5login ./ rsync -avhP legacy_env/.pki ./ ## optional files, if any ## singularity may take a larger space rsync -avhP legacy_env/.singularity ./ rsync -avhP legacy_env/.terminfo ./ rsync -avhP legacy_env/.subversion ./ cp legacy_env/.emacs ./ cp legacy_env/.viminfo ./ cp legacy_env/.screenrc ./ Make following empty dirs. These are unix specific configuration directories where some of softwares we install at later will keep their respective configurations. Note that we already backed up previous configurations in these directories under ~/legacy_env cd \" ${ HOME } \" && \\ echo \"You are in home directory at $( pwd ) \" mkdir -p \" ${ HOME } \" /.cache mkdir -p \" ${ HOME } \" /.config mkdir -p \" ${ HOME } \" /.local ## list dotfiles and dot dirs after reset ls -alh \" ${ HOME } \" /. [ ^. ] * Notice that we now have a fewer (and essential) dot files and dot directories right under home directory. Also, notice that we are missing essential dotfiles for loading bash login env, e.g., ~/.bashrc and ~/.bash_profile. These two files should originally be present when we logged to HPC on day one. Let's copy those original files back to home directory. I am using following default bash dotfiles but it may vary across different HPC env. Contact your HPC staff for more. You can copy following dotfile and use text editor like nano or vi to paste contents to respective dotfiles. ~/.bash_profile ~/.bashrc # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ] ; then . ~/.bashrc fi # User specific environment and startup programs PATH = $PATH : $HOME /.local/bin: $HOME /bin export PATH # .bashrc # Source global definitions if [ -f /etc/bashrc ] ; then . /etc/bashrc fi # Uncomment the following line if you don't like systemctl's auto-paging feature: # export SYSTEMD_PAGER= # User specific aliases and functions # This may vary across different HPC module load gcc Now, you should ensure that you can login to HPC from a separate terminal. Do not logout from an existing terminal yet! ssh userid@login.sumner.jax.org env If above command succeeds and env looks similar (PATH in particular) to outputs of default env variables (set by HPC staff) below, you're good! You can then exit old sumner session and install anaconda3 from a new (with a fresh or reset env) terminal session. bash commands expected output ## paths where all executables can be found echo $PATH ## paths where shared libraries are available to run programs echo $LD_LIBRARY_PATH ## Used by gcc before compiling program ## Read https://stackoverflow.com/a/4250666/1243763 echo $LIBRARY_PATH ## default loaded modules module list /cm/local/apps/gcc/8.2.0/bin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.:/home/amins/.local/bin:/home/amins/bin /cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64:/cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 /cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 Currently Loaded Modules: 1) shared 3) dot 5) slurm/18.08.8 2) DefaultModules 4) default-environment 6) gcc/8.2.0 Know what changes module load command can do When you load a module, it configures one or more of PATH, LD_LIBRARY_PATH, and other env variables. Command: module show <module name> can show you list of changes that a module makes during loading, e.g., module show gcc/8.2.0 In this case, loading gcc module will change PATH and LD_LIBRARY_PATH variables by perpending following respective paths. module unload gcc should remove these paths. -------------------------------------------------------------------------------------------- /cm/local/modulefiles/gcc/8.2.0: -------------------------------------------------------------------------------------------- whatis(\"adds GNU Cross Compilers to your environment variables \") prepend_path(\"PATH\",\"/cm/local/apps/gcc/8.2.0/bin\") prepend_path(\"LD_LIBRARY_PATH\",\"/cm/local/apps/gcc/8.2.0/lib:/cm/local/apps/gcc/8.2.0/lib64\") help([[ Adds GNU Cross Compilers to your environment variables, ]]) Make sure to logout and login to sumner again for a clean env to take an effect. Once you login, your env should look something similar to above. Note that PATH and LD_LIBRARY_PATH variables should default to Cent OS 7 standard paths with no user-defined paths except /home/amins/.local/bin:/home/amins/bin if those directories are present. Careful with user-defined paths For error-free setup, these - /home/amins/.local/bin:/home/amins/bin - user-defined directories should be empty to begin with and must not take precedence over system-default paths in PATH and LD_LIBRARY_PATH. Once we install anaconda3 and other tools, we will modify ~/.bash_profile and loading of bash login env such that user-defined paths override system-default paths. exit #from sumner ## login again ssh login.sumner.jax.org Store default hpc configuration Useful to fall back to HPC defaults if something goes awry! mkdir -p ~/bkup/confs/hpc_default_env/ cp .bashrc bkup/confs/hpc_default_env/ cp .bash_profile bkup/confs/hpc_default_env/ ## export global env env | tee -a \" ${ HOME } \" /bkup/confs/hpc_default_env/hpc_sumner_env_ \" $( date +%d%b%y_%H%M%S_%Z ) \" .log","title":"Option 2: Override existing setup"},{"location":"hpc/cpu/sumner_1/#configure-defaults","text":"Following configuration can vary across HPC env. Specifically, I prefer to keep minimal modules (tools that HPC staff installs as default) that are critical for login to HPC and compiling certain packages, e.g., CUDA libraries for Winter GPU-based HPC (detailed later in the setup). dot module only appends . to PATH variable (see module show dot ), so that you do not need to prefix ./ to run an executable file under present/current working directory. Since I do not need dot module, I will override default module loading by doing module unload dot in my bash configuration (later). For now, I do not need system gcc and will rely on conda-installed gcc and other devtools x86_64-conda_cos6-linux-gnu-* . More on that later but let's unload dot and gcc first. module unload dot module unload gcc module list Currently Loaded Modules: 1) shared 2) DefaultModules 3) default-environment 4) slurm/18.08.8 For now, you may add following cmd to your ~/.bash_profile to unload dot and gcc at each login to HPC. Eventually it will go to ~/.profile.d/ setup detailed below. module unload dot module unload gcc Make sure that gcc is unloaded While settings in ~/.bash_profile should be respected during login to HPC, sometimes starting a pseudo-terminal, e.g., screen or tmux session may not source ~/.bash_profile due to directives related to interactive versus non-interactive session . Then, you may notice that gcc module is not unloaded and still present under module list output. If so, manually unload gcc by module unload gcc after each time you enter into screen or tmux session or interactive HPC session (detailed below). If you do not add unload command to ~/.bash_profile and rather rely on manually running unload command prior to anaconda installation, you do need ensure that these modules are unloaded in your current terminal, especially when starting a pseudo-terminal like screen, tmux, or slurm interactive job. In summary, make sure to do module unload gcc before running setup further. exit #from sumner ## login again ssh login.sumner.jax.org","title":"Configure defaults"},{"location":"hpc/cpu/sumner_1/#install-conda","text":"Download and install anaconda3. I am using a variant of anaconda3 called, Mambaforge . It is based on Miniforge, a minimal conda installer (similar to minioconda) with an added support for conda-forge as a default channel and use of Mamba instead of default conda command to manage packages. cd \" $HOME \" && \\ mkdir -p Downloads/conda && \\ cd Downloads/conda && \\ wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh && \\ md5sum Mambaforge-Linux-x86_64.sh > Mambaforge-Linux-x86_64.sh.md5 md5: ab95d7b4fb52c299e92b04d7dc89fa95 Mambaforge-Linux-x86_64.sh Prefer running setup on a dedicated interactive node instead of login node. Some of compute/memory-intensive conda install/update steps may get killed on a login node. ## start screen session prior to running interactive session ## doing so will keep remote interactive session alive if your connection ## from local computer to HPC is lost. screen ## run interactive job ## options may vary across HPC srun -p compute -q batch -N 1 -n 3 --mem 10G -t 08 :00:00 --pty bash ## unload gcc if loaded module unload gcc You may not notice a change in login env except that your login prompt may change from a login node: user@sumner-log1 to one of compute nodes: user@sumner50 . By default, conda will setup ~/anaconda3 (or ~/mambaforge if using mambaforge) under home directory. Since conda env can grow over time and home directories are typically capped at 50 GB or so (at least with our HPC env), we will setup conda env on tier 1 space at /projects/verhaak-lab/amins/hpcenv/mambaforge cd \" $HOME \" && \\ bash \" ${ HOME } \" /Downloads/conda/Mambaforge-Linux-x86_64.sh Accept to license agreement and then set installation path to tier 1 space, e.g., /projects/verhaak-lab/amins/hpcenv/mambaforge in my case. Note that this will vary based on your username and available location on your HPC where you can store large amount of data. Conda env and related setup can grow over time and may exceed typical 50 GB quota for a user home directory. So, prefer installing conda and related env at location where you can store more data. For JAX, it's called tier 1 space under /projects/<lab_name>/<user_name>/ path. Installer will start installing conda env and towards the end, it will prompt you for initializing conda env. Say yes! If you say no, you can follow instructions that installer outputs to ensure that you have a working conda env each time you login to HPC. To do so, conda needs to write a few lines of code to ~/.bashrc file, so that HPC login env will always start with a valid (by modifying PATH and a several other env variables) conda env. Do you wish the installer to initialize Mambaforge by running conda init? [yes|no] [no] >>> no You have chosen to not have conda modify your shell scripts at all. To activate conda's base environment in your current shell session: eval \"$(/projects/verhaak-lab/amins/hpcenv/mambaforge/bin/conda shell.YOUR_SHELL_NAME hook)\" ## where YOUR_SHELL_NAME is bash or zsh or other shells. To install conda's shell functions for easier access, first activate, then: conda init If you'd prefer that conda's base environment not be activated on startup, set the auto_activate_base parameter to false: conda config --set auto_activate_base false Thank you for installing Mambaforge! Since I typed no above, I need to manually activate conda by following steps. cd \" ${ HOME } \" ## replace shell.bash with shell.zsh or other shells you may be using by now. ## know which shell you are using in HPC echo \" $( basename ${ SHELL } ) \" ## activate conda base env in the current terminal eval \" $( /projects/verhaak-lab/amins/hpcenv/mambaforge/bin/conda shell.bash hook ) \" Once conda base env has been activated, you will notice your login prompt changing from user@sumner50 to (base) [userid@sumner50] . You can also check which conda env you are in by running echo $CONDA_DEFAULT_ENV . Since we have not installed additional conda env yet, we only have base env to begin with. You can also run echo $CONDA_PREFIX to confirm that conda has been installed on non-default, tier 1 path and not under ~/mambaforge . Now, let conda edit ~/.bashrc file so conda can load base env each time we login to HPC. conda init ## if using mamba, also run mamba init ## to enable mamba activate/deactivate env mamba init ## Check what code has been added to ~/.bashrc cat ~/.bashrc You will notice that conda has now added initialization code to ~/.bashrc Our minimal conda installation is now complete. Logout from interactive session, and then logout from HPC. # exit from interactive session exit ## exit from HPC exit In Part 2 , we will login and start an interactive session again to customize conda and HPC env. There were no default dot directories on the day one. \u21a9","title":"Install conda"},{"location":"hpc/cpu/sumner_2/","tags":["hpc","setup","conda","kernels","jupyter","programming"],"text":"Following up from Part 1: Initial HPC setup , we now start installing essential softwares or (in conda dictionary) packages, e.g., R, Jupyter, etc. ## login back to HPC ssh userid@login.sumner.jax.org ## start screen screen ## start interactive session srun -p compute -q batch -N 1 -n 4 --mem 10G -t 08 :00:00 --pty bash ## unload gcc if loaded module unload gcc \u2620\ufe0f Keep a note of walltime \u2620\ufe0f Make sure you do not run over alloted walltime while configuring conda env, especially when you are in the middle of installing via mamba install or mamba update command, and walltime dies out! (speaking from an experience) and that may break an ongoing conda setup. conda is good in a way that it locks process files for package(s) it is trying to install or update. If you are lucky, you should be able to save your ongoing setup else start again! Just read errors you may encounter during such issue and it should resolve such issue. Configuring conda env \u00b6 Typically, we use conda command to manage all of conda env, e.g., conda install package , conda update package , conda list , etc. However, we will now use mamba instead of conda . Mamba is a faster drop-in replacement for conda to manage packages. Use mamba instead of conda Moving forward, do not forget to use mamba instead of conda for all of available commands. See mamba --help to list available commands. At present, following commands from conda are supported by mamba : install, create, list, search, run, info and clean. For rest of commands, e.g, config, activate, deactivate, etc., use conda command. Turns out mamba activate <env_name> or mamba deactivate also works following one-time command: mamba init which will ensure sourcing mamba shell variable at the bash startup by writing a few lines towards the end of ~/.bashrc file. Please note to execute mamba init after conda init as in Part 1: Initialize conda . This will ensure at the bash startup sequence to load conda setup prior to mamba setup. If you end up running mamba init now (I ended up running it late in the setup), prefer activating or deactivating conda env using mamba and not conda command. It is important to install packages only from a single channel and not do mix-and-match install. Read more at conda-forge page on channel_priority: strict which is enabled as default for conda v4.6 or higher. We are using conda v4.10.3 and anaconda v2021-11. We can check that using conda --version and conda list anaconda respectively. Add Bioconda and conda-forge channels to get updated and compbio related packages. Do not change the order of following commands and a command with --add channels conda-forge must be the last one else other channels may take a priority over default conda-forge channel. conda config --add channels bioconda conda config --add channels conda-forge Above command will generate ~/.condarc file and sets priority for channels, i.e., when same package is available from more than one channels, we prioritize installation per ordered channel list in ~/.condarc file as below. This file should be present after above commands and no need to edit unless changing priority of channels. precedence of condarc file Since we are using Mambaforge and not anaconda3, it already ships with conda-forge as a default channel, as specified in cat \"${CONDA_PREFIX}\"/.condarc . Note that ~/.condarc will take precedence over default \"${CONDA_PREFIX}\"/.condarc file, so ensure that conda-forge is the most preferred channel (first channel) in both files. Besides channel priority, I have added a few other custom settings to my ~/.condarc file. For more on these settings, please refer to condarc documentation before applying to your own HPC env. ~/.condarc is a yml format file, so take care of preceding spaces (and not tabs) before and after - while editing this file. channels : - conda-forge - bioconda auto_update_conda : False always_yes : False add_pip_as_python_dependency : True ssl_verify : True allow_softlinks : True use_only_tar_bz2 : False anaconda_upload : False repodata_threads : 4 verify_threads : 4 execute_threads : 4 Do not exceed requested resources for an interactive job If you increase number of threads for conda operations, make sure that you request required threads for an interactive HPC job else you may consume more resources than requested threads, and HPC workload manager may kill your interactive job - conda env can potentially break if interrupted during install or update command. You can verify custom set ~/.condarc configurations using conda config --get command. If some of key:value pairs are showing warnings saying unknown key , you should remove those entries from ~/.condarc . If channel priority has been set as above, you will notice --add channels 'conda-forge' # highest priority from the output of conda config --get command. Update conda package \u00b6 Since we used Mambaforge, anaconda3 variant from a third-pary conda-forge open-source community, let's make sure that the base conda package is the most up-to-date or not. conda --version conda 4.10.3 To check if this is the current version, we will update conda package. mamba update conda You may notice that conda update is available else no further action needed. Notice source of updated conda package from conda-forge/linux-64 . That is because we set conda-forge with the highest channel priority in ~/.condarc . - conda 4.10.3 py39hf3d152e_2 installed + conda 4.11.0 py39hf3d152e_0 conda-forge/linux-64 17 MB Confirm that conda package is now updated with an updated version, if any. conda --version conda 4.11.0 JupyterLab \u00b6 For most packages, e.g., R, snakemake workflow, etc., we will use dedicated conda env and avoid installing into base env. That is to keep base env clean and without much of dependencies. Unlike base env, additional env can be recreated without a risk of breaking conda setup. However, we will require a few packages, e.g., JupyterLab and Notebook, that typically ships with regular (and not miniconda or mambaforge) anaconda3 installation. Install JupyterLab in its own conda env If you prefer, you can skip installing JupyterLab in base env and instead use its own dedicated env. This is perhaps a preferred way to keep base env minimal and also allows you to update JupyterLab from time to time without worrying about breaking base env. However, when you start jupyterlab session, you need to switch (activate) to the respective conda env from base or other envs. To install jupyterlab in its dedicated env, do following: mamba create -c conda-forge -n jlab jupyterlab nodejs jupyterthemes jupytext dos2unix jupyter_http_over_ws jupyterlab-link-share mamba activate jlab ## check installed extensions, if any jupyter lab extension list jupyter server extension list Read install guide for extensions, if any, e.g. some extensions like jupyter_http_over_ws are not enabled by default for good (saftey) reasons. JupyterLab is similar to RStudio IDE and provides richer interface to several programming languages, including python, R, julia, and many more. To install jupyterlab, please read installation guide . ## core package mamba install -c conda-forge jupyterlab Even though conda-forge is set as the highest priority channel in ~/.condarc , I am explicitly specifying to use the same channel while running install or update command. This will install jupyterlab and series of its dependencies. You can check version of related packages using mamba list | grep -E \"jupyter\" although versions may differ as they get updated over time. # packages in environment at /projects/verhaak-lab/amins/hpcenv/mambaforge: # # Name Version Build Channel jupyter_client 7.1.0 pyhd8ed1ab_0 conda-forge jupyter_core 4.9.1 py39hf3d152e_1 conda-forge jupyter_server 1.12.1 pyhd8ed1ab_0 conda-forge jupyterlab 3.2.4 pyhd8ed1ab_0 conda-forge jupyterlab_pygments 0.1.2 pyh9f0ad1d_0 conda-forge jupyterlab_server 2.8.2 pyhd8ed1ab_0 conda-forge Optional: I have also installed following set of packages in the base env for my own needs or convenience for not switching to other conda environments. If you do so, try to limit installing packages in base env to bare minimum, and avoid installing packages that require multiple dependencies, e.g., R, tensorflow, node, julia, GO library, etc. mamba install -c conda-forge git rsync vim globus-cli tmux screen We will setup rest of jupyterlab settings and initialize it later after we install R in a separate conda env. Installing R \u00b6 We will install R and other routinely used tools in a separate conda env for reasons explained above. You should read official documentation on installing R to familiarize with steps that I am going to follow below. Create a new env \u00b6 I am going to name a new env as yoda . Ideally, naming should be such that you should not have difficulty finding which env to switch to for the type of packages and analysis you may end up running at the later date, e.g., in my case, while far from ideal, I have used nomenclature based on the Jedi members for conda env: env id intended use yoda hosts all packages, including R and others that I use on daily basis luke serves as an env for background processes using databases leia a standalone env for running snakemake workflows obiwan fallback to yoda when I require to use another mature version of R or other packages windu conda env using legacy Python 2 over Python 3 anakin dev or beta env for testing: Optimized for CPU-based HPC rey Similar to yoda but optimized for GPU-based HPC ben dev or beta env for testing: Optimized for GPU-based HPC grogu toy env for everything else: for experimental purpose rey and ben env are optimized for GPU-based, Winter HPC at JAX and should not be used while working in the CPU-based, Sumner HPC at JAX. However, all of CPU-based envs, e.g., yoda, luke, leia , etc. will work on both, Sumner and Winter HPCs. Let's create the first env, yoda and install base R package and a several essential R packages for routine analysis. mamba create -c conda-forge -n yoda r-base r-essentials Note that most up-to-date R version may be available in the conda-forge or sometimes in the other conda channels, like r or bioconda . However, it is preferable to install R from the first priority channel , i.e., conda-forge in our case. Activate a new env. Note that we use conda instead of mamba command as the latter (at least for now) only accepts following sub commands: install, create, list, search, run, info and clean. conda activate yoda You will notice bash prompt changing from (base) [userid@sumner50] to (yoda) [userid@sumner50] . Pin R and conda auto-updates \u00b6 Before moving further, let's pin R version to 4.1.1 (at the time of this writing) and also disallow conda auto-updates. That way, we have lesser chances of breaking conda env when we do mamba install <pkg> in future, and carefully install/update packages without breaking existing setup. For more on pinning packages, read official documentation . Technical note Typically, I avoid installing or updating package if mamba install throws a message or warning about removing or downgrading existing packages . In such cases, I fall back to compiling package using available devtools in conda . Also, I load compiled package using Modulefile when needed, and not integrate it in my default bash environment as this may give errors while running some random program due to conflicts in shared library versions. ## we already set auto update to False above ## under ~/.condarc settings # conda config --set auto_update_conda False # Find package version using mamba list | grep -Ei 'r-base' We can notice that R version is 4.1.1 (or higher). You can also check R version by R --version . Remember this version and add it to following newly created file: nano \" ${ CONDA_PREFIX } \" /conda-meta/pinned Note that echo ${CONDA_PREFIX} points to conda-meta/ directory under yoda and not the base env because we are within yoda env. In other words, pinned packages env specific and you can update R package in other environment(s), if present. Add following as a new line entry: r-base ==4.1.1 Check a valid line break Since we are creating a new file and only adding a single line of text, when we save this text file, we should confirm that it is the end of the line . This is usually recognized by pressing the Enter . Unix systems recognizes such line break using an invisible $ sign which you can confirm by running cat -e \"${CONDA_PREFIX}\"/conda-meta/pinned r-base ==4.1.1$ $ With each line break, you will notice $ sign, e.g., two lines in my case. You may remove a second line by editing file again but make sure to run cat -e \"${CONDA_PREFIX}\"/conda-meta/pinned to check a valid line break. You may pin only part of the major and minor version , i.e., to allow updates from 4.1.1 to 4.1.2 or 4.1.3 but not from 4.1.1 to 4.2.*. However, I rather freeze the specific version and update to builds at the later date, if need arises. To do so, remove the pinned entry from \"${CONDA_PREFIX}\"/conda-meta/pinned and do mamba update r-base=4.1.2 or other version. Since R developer team updates major.minor version of R every quarter or so, I try to keep those R versions in a separate env rather updating as certain R packages may throw an error with such major updates. Install R libraries \u00b6 You may now install additional R libraries (or packages - it's alias!) or any other packages, e.g., git, bedtools, samtools, etc. in the new env using mamba install command. Just make sure that you are in the new env by conda activate yoda . You can check which env you are in by echo \"$(basename ${CONDA_PREFIX})\" Some of packages, e.g., git, rsync, etc. are already installed in the base env. However, they may not be recognized in the new env: yoda . Ideally, you should install the same package in the new env. Conda will usually link package files (which takes much of space) from the central package directory, so installing the same package in different env should not take significant additional space. To search for packages, prefer using anaconda website and look for packages that are under conda-forge/ or bioconda channels, i.e., the first and second preference, respectively in our ~/.condarc file. Avoid installing packages from non-standard channels For a stable and error-free conda env, avoid installing conda packages from non-standard channels, i.e., a channel other than conda-forge and bioconda or ones specified in ~/.condarc file. Installing packages from non-standard channels will unnecessarily increase complexity of package dependencies in conda env and will increase likelihood of slowing or breaking down one or more conda env. Note that we have yet to create additional conda env besides a default base env. Install R packages: You can tie up all of your packages in a single command or break it down to smaller chunks. The former appraoch may take longer and difficult to debug if package installation fails due to conflicting dependencies with one or more packages. mamba install -c conda-forge r-tidyverse r-tidymodels r-devtools r-biocmanager mamba install -c conda-forge gnupg git rsync vim openjdk r-rjava mamba install -c conda-forge bedtools pybedtools bedops mamba install -c conda-forge matplotlib scikit-learn Reticulate and rpy2 packages will allow us to use R and python interchangeably in the same R script or python notebook, respectively! Read details about reticulate on RStudio website and rpy2 here . reticulate R package \u00b6 mamba install -c conda-forge r-reticulate rpy2 python package \u00b6 mamba install -c conda-forge rpy2 If above command works without forcing you to downgrade python, R, or other major packages, good for you! If not, it is due to strict requirements of rpy2 package which can conflict with other core packages in yoda env. So, mamba install -c conda-forge rpy2 may not work. \u2620\ufe0f Beware of using pip install in conda env \u2620\ufe0f You should be comfortable compiling and installing packages using pip install and knowing how to manually install python package requirements. If not, it is safer to skip installing rpy2 (or any other) package by steps detailed below. Wait until conda-forge developers make rpy2 package available. conda-forge developer community is pretty good and active, so patience should pay off then risking to break an otherwise functioning yoda env! From rpy2 setup.py and requirements.txt file, list number of packages required as dependencies for rpy2. If we do pip install rpy2 , pip will automatically install these requirements. However, I prefer to let conda manage all package versions because pip may not necessarily evaluate if certain package versions are compatible with other conda-installed packages . That's why I prefer installing requirements by myself using mamba install and then do pip install , so only minimal set of requirements will be installed by pip command. That way, I can minimize chances of breaking conda env due to conflicting package dependencies. ## required packages by rpy2 ## see how many of these are already installed conda list | grep -E \"cffi|pytest|pandas|numpy|jinja2|pytz|tzlocal\" Install missing packages using mamba . ## ok to write packages which are already installed ## conda will take care of version conflict, if any. mamba install -c conda-forge cffi pytest pandas numpy jinja2 pytz tzlocal Now try (fingers crossed!) installing rpy2 using pip. Make sure that rpy2 pip website is showing same version (3.4.5 in my case) as on rpy2 github website else you need to download source file from pip website, and check respective setup.py and requirements.txt file to ensure dependencies are identical and satisfied or installed via mamba install command. ## in yoda env mkdir - ~/logs && \\ pip install rpy2 |& tee -a logs/pip_install_rpy2.log Success installing rpy2 Great! pip install did not end up installing any dependencies (as we already installed those using mamba install ), and rpy2 is now successfully installed. Collecting rpy2 Downloading rpy2-3.4.5.tar.gz (194 kB) Preparing metadata (setup.py): started Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: cffi>=1.10.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (1.15.0) Requirement already satisfied: jinja2 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (3.0.3) Requirement already satisfied: pytz in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (2021.3) Requirement already satisfied: tzlocal in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (4.1) Requirement already satisfied: pycparser in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from cffi>=1.10.0->rpy2) (2.21) Requirement already satisfied: MarkupSafe>=2.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from jinja2->rpy2) (2.0.1) Requirement already satisfied: pytz-deprecation-shim in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from tzlocal->rpy2) (0.1.0.post0) Requirement already satisfied: tzdata in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal->rpy2) (2021.5) Building wheels for collected packages: rpy2 Building wheel for rpy2 (setup.py): started Building wheel for rpy2 (setup.py): finished with status 'done' Created wheel for rpy2: filename=rpy2-3.4.5-cp310-cp310-linux_x86_64.whl size=300939 sha256=136144e165e2b0f156fdb7f525547ae073dda06346a893de398aa6836e625af1 Stored in directory: /home/amins/.cache/pip/wheels/ba/d8/8b/68fc240578a71188d0ca04b6fe8a58053fbcbcfbe2a3cbad12 Successfully built rpy2 Installing collected packages: rpy2 Successfully installed rpy2-3.4.5 Since we just installed JAVA (openjdk) and rJava R library, for sanity check, run R CMD javareconf (to print and update java configuration for R) and echo $JAVA_HOME (to confirm that bash variable is correctly set). If you also have installed git, you can copy boilerplate ~/.gitconfig , and make changes under line starting with name, email, and excludesfile. Make sure to read about gitconfig too. Best practices using mamba install or mamba update Keep a habit of checking following when using mamba install command: Prefer conda-forge channel, followed by bioconda , and avoid installing packages from other channels. You can do so in dev or beta env, e.g., env grogu in my case to see how it pans out. Before hitting Y/Yes to install packages, ensure that installing or updating packages does not force downgrading of one or more major packages, like python, R, and any other packages that you deem it as major package in your routine analysis and downgrading it may break reproducibility of your analysis. You can always use dev env, like grogu to play packages showing such downgrade warnings. While doing mamba install -c conda-forge samtools bcftools htslib , I noticed openssl dependency being downgraded from v3.0.0 to v1.1.1. I find it a major downgrade to one of the core package and so I skipped installing samtools and related packages using mamba . I would rather install these packages by compiling from their respective source tar balls. You can read installation details for samtools family packages at author's website . Similarly for mamba install -c conda-forge bedtools pybedtools bedops , I got following error as yoda env is using python 3.10 and it is pinned by default. So, it can not be downgraded! Similar to samtools, I will compile pybedtools or use a separate env, like grogu for such packages with unique requirements! For bedops , it was asking me to install older version of samtools which I was not ok with. I will rather compile those tools from the current version. So, finally I ended up installing only bedtools with clean requirements! mamba install -c conda-forge bedtools . package pybedtools-0.8.2-py39h39abbe0_1 requires python >=3.9,<3.10.0a0, but none of the providers can be installed Setup Rprofile and Renviron \u00b6 Rprofile and Renviron files provide additional configuration option for R, similar to ~/.condarc file to manage several conda-related configurations. You can read more about these files at R developer website and RStudio website . For detailed notes, Jennifer Bryan and Jim Hester has written an excellent resources, titled What they forgot to teach you about R with a chapter on R Startup . Setup R library directory path for R 3.6 Before setting up R startup, I will make a dedicated package directory that will store R libraries or packages. By default, R will store all libraries at conda env specific path, i.e., for my case, it is at \"${CONDA_PREFIX}\"/lib/R/library/ . You can check this path using Rscript -e '.libPaths()' . This default path is ideally intended for R libraries managed via mamba install or mamba update command. However, I also compile R libraries using install.packages() R command when I find conflicting dependencies in installing R libraries using mamba install command. In such cases, I prefer to use a separate R library directory than a default R library path. I am making an empty library directory for installing libraries or R packages that I may compile using R install.packages() command. R usually defaults to making such user-package directory in the user's home path but I will use tier 1 space again to avoid filling up my home directory with a limited quota. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/4.1 Create ~/.Renviron file nano ~/.Renviron Add following to ~/.Renviron . Notice the order in which R will store newly compiled packages. It will use an empty directory we just created to store new packages. If for some reasons, this directory is not accessible (file permission errors), R will fallback to the second path, and so on. The second path is an expanded path of a default R package directory, i.e., output of echo _\"${CONDA_PREFIX}\"/lib/R/library/_ command. R_LIBS = \"/projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/4.1:/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/library\" You can confirm precedence of R library paths in R using .libPaths() R command or from bash using Rscript -e '.libPaths()' . Create ~/.Rprofile file nano ~/.Rprofile Add following contents. Please read more about configuring R startup from links shared earlier in this section. Comment out lines using # if you do not require one or more of following options. ## set user specific env variables, e.g., GITHUB_PAT here Sys.setenv ( \"GITHUB_PAT\" = \"my_github_secret_token\" ) ## Default source to download packages local ({ r <- getOption ( \"repos\" ) r [ \"CRAN\" ] <- \"https://cran.rstudio.com\" options ( repos = r ) }) Protect secret tokens, passwords, etc. If your ~/.Rprofile file contains any secret tokens, it's a best practice to make it read/write only by file owner (you) using chmod 600 ~/.Rprofile . Same goes for similar files in your home directory, e.g., .gitconfig , .netrc , etc. Equivalent command for directory, e.g., for .ssh/ is chmod 700 ~/.ssh . Read more about linux file permissions . Compiling R libraries \u00b6 Since we already have setup yoda env for R, you can now install additional R libraries using mamba install r-PackageName ( see above ) as long as it is available in anaconda repository , preferably in conda-forge or bioconda sources AND it does not significantly downgrades essential packages like python, R, and other core libraries, e.g., zlib, openssl, etc. The latter is subjective and depends on what you will consider as an essential. In any case where I have doubts of breaking conda env, I fall back to compiling R library using native R command: install.packages() . However, I am going to wait using this command until I have finished my HPC setup, specifically, bash startup using ~/.profile.d configuration. We are not there yet but not far from it too! Install essentials \u00b6 This can again vary per user's need and optional. If you find errors compiling packages, you may end up installing respective dev libraries, e.g., libiconv, zlib, etc. if they are not already installed in the current ( yoda in this case) conda env. mamba install -c conda-forge wget curl rsync libiconv parallel ipyparallel Setup JupyterLab \u00b6 We have installed JupyterLab earlier but did not finish complete setup. Let's do that! In this section, I will setup jupyter notebook and kernels to launch python, R, and bash based notebooks. I will also harden notebook server with several security settings. Remember that we will be using JupyterLab installation from the conda base env even though JupyterLab could have been installed in yoda and other conda env. Managing jupyterlab server from base env is convenient and allows us to interact with all other env. Also, if we end up resetting or deleting other env, jupyter configuration in base env will remain intact. First, we install language-specific kernels. By default, jupyterlab ships with python kernel, named ipykernel which is backend when we interact with jupyter python notebook. Jupyter can allow us to use other language-specific kernels to interact with R, Julia, bash, and many more languages . Accordingly, I will install kernels for R and bash in specific envs, e.g., yoda and then switch back to base env to configure jupyterlab, such that we can connect to kernels in yoda env from conda base env. Make sure you are in yoda env. We will switch to base env after we configure all kernels in yoda env. conda activate yoda Install kernels mamba install -c conda-forge ipykernel r-irkernel bash_kernel Note that I have also installed python kernel, ipykernel in yoda env as we may not have jupyterlab installed in yoda as we only need to install jupyterlab in the base env. Also, some of kernels may already have been downloaded as part of dependency for other packages we installed earlier. You can check conda list | grep kernel output to confirm which kernels are already installed. Now we can link each of these kernels to jupyterlab in base env. python kernel \u00b6 python -m ipykernel install --user --name yoda_py310 --display-name \"yoda_py310\" # confirm that installation exited without any error echo $? # this should return 0 for successful installation --name and --display-name will show up as kernel file location at ~/.local/share/jupyter/kernels/ and icon name in the jupyterlab launcher page, respectively. You can name as you like but without any spaces or special characters. I am following naming format that uses env followed by language and its major and minor version. R kernel \u00b6 Start R session R Install kernel while in yoda env Read available options for IRkernel installation. Also, consider installing jupytertext-text-shortcuts but not now and we can install this along with other extensions towards the end of configuring jupyterlab. library ( IRkernel ) installspec ( name = \"yoda_r41\" , displayname = \"yoda_r41\" , user = TRUE ) ## quit R session q ( save = \"no\" ) Similar to python kernel, r kernel should now be at ~/.local/share/jupyter/kernels/ . bash kernel \u00b6 We will now install bash kernel in yoda env. Unlike other (R and python) kernels, bash kernel do not need to be installed in all of conda env because we can always switch between conda env specific bash env using mamba activate anakin or mamba activate rey , etc. Here, I will install bash kernel in yoda and not base env as I intend to use yoda as my primary go-to env when I login to HPC. Keep use of base env to bare minimum Please remember that we keep use of base to bare minimum for maintaining core of codna packages, and should avoid installing (and populating dependencies) packages in base env. You can always delete secondary conda env and restart but you cannot do so with base env! ## in yoda env mamba list | grep -E \"bash_kernel\" ## if this does not show bash_kernel installed, redo install # mamba install -c conda-forge ipykernel r-irkernel bash_kernel ## and the install kernel into jupyter env python -m bash_kernel.install Unlike python and R kernels, I could not find overriding default name and display_name for bash kernel. So, I will rename bash kernel manually else if I end up installing similar kernel from other env, it will override kernel with the same default name: bash . Typically, you do not need to install bash kernels in all conda env as jupyterlab ships with a powerful terminal that allows switching from one to other env using same conda activate command. So, I hardly use bash kernel. ## go to kernel base dir cd ~/.local/share/jupyter/kernels/ ## rename bash dir to yoda_bash mv bash yoda_bash ## edit bash kernel.json to rename display name cd yoda_bash nano kernel.json Rename \"display_name\": \"Bash\" to \"display_name\": \"yoda_bash\" kernel loading \u00b6 Following completion of the entire setup, we are going to run JupyterLab from the base env. However, on daily basis, we like to access Python and R from yoda and not base 1 . Default kernel setup above should let jupyterlab handle conda env specific python but not so for other kernels. However, I have noticed issues running Python and R from a non base conda env as sometimes packages requiring shared libraries may throw an error as such shared libraries are either missing in base env or have a different version than one in the current env, i.e., yoda env where package was originally installed or compiled. I mitigate such issues by loading a valid bash env prior to initializing kernel , e.g., I will wrap a default jupyter kernel settings into a bash script (wrapper) and will activate a valid conda env, e.g., yoda in this case prior to initializing yoda specific Python or R kernel. That way, kernel will consistently inherit a valid login (bash) env for the respective conda env. yoda python \u00b6 Create a new kernel wrapper matching name of kernel we like to edit, e.g., /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310 mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/kernels touch /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310 # make file executable chmod 700 /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310 Add following to wrap_yoda_py310 file. Change user paths where applicable. #!/bin/bash ## Load env before loading jupyter kernel ## @sbamin ## https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425 #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 # I am using conda instead of mamba to activate env # as somehow I notices warnings/errors sourcing # mamba.sh in sub-shells. CONDA_BASE=$(conda info --base) && \\ source \"${CONDA_BASE}\"/etc/profile.d/conda.sh && \\ conda activate yoda #### END CONDA SETUP #### # this is the critical part, and should be at the end of your script: exec /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/bin/python -m ipykernel_launcher \"$@\" ## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/<kernel_name>/kernel.json #_end_ Now, adjust kernel settings. ## go to kernel base dir cd ~/.local/share/jupyter/kernels/ ## there should be yoda_py310 directory or ## one matching --name yoda_py310 argument ## we used above when installing python kernel cd yoda_py310 ## edit kernel.json nano kernel.json Replace contents of kernel.json with following: { \"argv\" : [ \"/projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310\" , \"-f\" , \"{connection_file}\" ], \"display_name\" : \"yoda_py310\" , \"language\" : \"python\" , \"metadata\" : { \"debugger\" : true } } yoda R \u00b6 Now, we can reconfigure R kernel for yoda same as above but with a few changes in the wrapper script. Create a new kernel wrapper for R, e.g., /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41 mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/kernels touch /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41 # make file executable chmod 700 /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41 Add following to wrap_yoda_r41 file. Change user paths where applicable. #!/bin/bash ## Load env before loading jupyter kernel @sbamin https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425 #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 CONDA_BASE=$(conda info --base) && \\ source \"${CONDA_BASE}\"/etc/profile.d/conda.sh && \\ conda activate yoda #### END CONDA SETUP #### ## this is the critical part, and should be at the end of your script: ## path to R and arguments come from original kernel.json under ## ~/.local/share/jupyter/kernels/yoda_r41/ directory. ## In some cases, path to R may differ and may originate from ## .../envs/yoda/lib64/R/bin/R instead of .../envs/rey/lib64/R/bin/R ## If so, adjust path to R here accordingly. exec /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/bin/R --slave -e \"IRkernel::main()\" --args \"$@\" ## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/<kernel_name>/kernel.json #_end_ Now, adjust kernel settings. ## go to kernel base dir cd ~/.local/share/jupyter/kernels/ ## there should be yoda_py310 directory or ## one matching --name yoda_py310 argument ## we used above when installing python kernel cd yoda_r41 ## edit kernel.json nano kernel.json Replace contents of kernel.json with following: { \"argv\" : [ \"/projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41\" , \"{connection_file}\" ], \"display_name\" : \"yoda_r41\" , \"language\" : \"R\" } Done! Next time you run jupyter, you should have a new julia kernel in JupyterLab. Configure JupyterLab \u00b6 Once we have installed env specific kernels as in yoda (and other envs, if a ny), now it's a time configure JupyterLab in the base env. ## deactivate yoda env conda deactivate You should now be in conda base env If you were jumping across more than one conda envs, then each instance of conda deactivate command will bring you back to previously active env. So, make sure to return to base env which you can confirm using echo $CONDA_PREFIX output. That should point to base path of conda (mambaforge in my case) installation: /projects/verhaak-lab/amins/hpcenv/mambaforge/ . Also, notice change in bash prompt to (base) userid@sumner50 . Once in the base env, generate skeleton for default jupyter configuration. ## return to home dir cd \" ${ HOME } \" && \\ jupyter server --generate-config Writing default config to: /home/userid/.jupyter/jupyter_notebook_config.py Secure Jupyter Server It is critical that you harden security of jupyterlab server. Default configuration is not good enough (in my view) for launching notebook server over HPC, especially without SSL (or https ) support. Setting up individual security steps is beyond scope of this documentation. However, I strongly recommend reading official documentation on running a public Jupyter Server and security in the jupyter server . Checkout this example guide on creating self-signed SSL certificates in case you do not have SSL certificates from Research IT department. self-signed SSL certificates \u00b6 Ideally you should have SSL certificates signed by a verified certificate authority (CA) else most of modern web browsers will issue a warning or an error regarding SSL secutity. CA-signed SSLs are typically paid unless issued via let's encrypt or your DNS providers. Securing a website (Jupyter Env in our case) is beyond scope of this guide but I will suggest to inquire to your research IT department to see if they can help secure your Jupyter env. It's better to have CA-signed SSL over self-signed SSL, and always better to have SSL (https) over insecure (http) connection to your Jupyter env. This is true even when you are working within a secure firewall of your work network. Here, I am creating a self-signed SSL which most of you can generate and at least have self-signed SSL. See this post for details . ## switch to base conda env mamba activate base ## create dir to save SSL certificates mkdir ~/ssl_cert && \\ chmod 700 ~/ssl_cert && \\ cd ~/ssl_cert ## generate a private key openssl genrsa -out hpc_cert.key 2048 ## secure your private key chmod 600 hpc_cert.key ## create a signed certificate using a key we generated above openssl req -new -key hpc_cert.key -out hpc_cert.csr ## finally self-sign this certicate openssl x509 -req -days 365 -in hpc_cert.csr -signkey hpc_cert.key -out hpc_cert.pem While creating a signed certificate, hpc_cert.csr , you will be prompted to enter issuer's country, city, etc., including Common Name (CN). You should put some info related to use of this certificate into respective fields, e.g., Common Name can be hpcjupyter . SSL certificate - unable to verify SSL connection Regardless of what you put in CN, say hpcjupyter.mywebsite.com , most of modern web browsers so either SSL warning or error of not verifying your self-signed SSL. This is because you have self-signed this SSL and not using an approved certificate authority (CA) provider for signing your SSL. Read this post for details . In essense, your self-signed SSL is acting as it is an authentic SSL for a CN you provided above, hpcjupyter.mywebsite.com . Since that's a clearly a security risk, most modern browsers will throw a warning before you are allowed to visit a site (Jupyter server in this case) or even may not allow at all to load that website! Hence, prefer asking your Research IT to make a signed SSL available for a specific intranet subdomain, e.g., jupyter.company.com for users to use. cookie file \u00b6 After creating SSL certificates, also create a cookie file for jupyter server. openssl rand -hex 32 > /home/foo/ssl_cert/jp_cookie chmod 600 /home/foo/ssl_cert/jp_cookie config file \u00b6 Example config for /home/userid/.jupyter/jupyter_server_config.py . Please do not copy and paste these options without knowing underlying details . Jupyter token - provide a strong token string While editing jupyter config file below, please read inline comments carefully, especially for c.ServerApp.token . Do not use the default token as that token is used for login to your running jupyter server . Provide a secure (a longer string, 32 characters or more) string generated using uuidgen and after removing dashes. Do not worry of remembering this token. Jupyter does allow you to have an alternate way of login using a custom, user-generated password (see further below). ## leave commented out portion of default config as it is. ## then you can add your custom config ## Do not copy these configurations without knowing what they do! #### NOTEBOOK CONFIGS #### ## SSL settings ## read documentation for details c . ServerApp . certfile = u '/home/foo/ssl_cert/hpc_cert.csr' c . ServerApp . keyfile = u '/home/foo/ssl_cert/hpc_cert.key' c . JupyterHub . cookie_secret_file = '/home/foo/ssl_cert/jp_cookie' c . ServerApp . open_browser = False ## token used to programmatically login to jupyter, ## e.g., via VS Code Jupyter extension. ## This is essentially a password to login to jupyter ## Provide an alphanumeric secret string - longer the better. ## you can create one using uuidgen command: Remove dashes. c . ServerApp . token = 'PLEASE_REPLACE_THIS_TOKEN_c8syb2lnd89g2fosyyfskdfy02h' c . ServerApp . allow_password_change = False ## should be set to False ## unsafe to set True from https security point of view c . ServerApp . disable_check_xsrf = False ## use one of available options: See documentation c . Application . log_level = 'INFO' ## use login shell c . ServerApp . terminado_settings = { 'shell_command' :[ 'bash' , '-l' ]} ## END ## jupyter password \u00b6 Once you customize config file above, make sure to generate a secret and strong password using jupyter server password command. Your password then will be written in an encrypted format in /home/userid/.jupyter/jupyter_server_config.json file. Make both files read/write-only by you. ## for directory, we use permission 700 chmod 700 ~/.jupyter # For files, we use permission 600 chmod 600 ~/.jupyter/jupyter_server_config.py chmod 600 ~/.jupyter/jupyter_server_config.json Customizing user interface \u00b6 If you need to add custom themes, fonts, shortcuts, etc. for Jupyter, you may follow this section, else safe to skip to Start Jupyter section. Before installing themes or customizing jupyterlab further, I will install node js package to base env. Ideally, base env should not be cluttered with packages except bare mininmum that comes with original conda installation (mambaforge in my case). However, node js is required to setup and manage jupyterlab extensions and how jupyterhub can interact with a few kernels, e.g., jupyterlab-sql extension to interact with sql databases that I will end up installing in the future. mamba install -c conda-forge nodejs npm --version using v8.1.2 Themes \u00b6 Themes provide custom user interface and is optional for setup. See example themes at dunovank/jupyter-themes mamba install -c conda-forge jupyterthemes Note: This may downgrade node js. Since I do not use node js in base and it is installed in base to manage jupyter extensions, I was ok downgrading it as it changed only a build and not major or minor version . setup theme, see details here jt -t solarizedl -T -N -f firacode -fs 12 -tf ptserif -tfs 11 -nf ptsans -nfs 12 -dfs 11 -ofs 10 -cellw 90 % -lineh 170 keyboard shortcuts \u00b6 If you are familiar with RStudio shortcuts for R pipe %>% and assignment <- operator, you can enable those in JupyterLab too 2 by first starting a jupyterlab session. You can then go to Advanced Settings Editor either by pressing Cmd + , on a mac or go to Settings from a top menubar, and then clicking Keyboard Shortcuts option. There, under User Preferences pane, you can paste following to enable keyboard shortcuts, i.e., Alt + - for <- and Shift + Cmd + M for %>% operator. { \"shortcuts\": [ { \"command\": \"apputils:run-first-enabled\", \"selector\": \"body\", \"keys\": [\"Alt -\"], \"args\": { \"commands\": [ \"console:replace-selection\", \"fileeditor:replace-selection\", \"notebook:replace-selection\", ], \"args\": {\"text\": \" <- \"} } }, { \"command\": \"apputils:run-first-enabled\", \"selector\": \"body\", \"keys\": [\"Accel Shift M\"], \"args\": { \"commands\": [ \"console:replace-selection\", \"fileeditor:replace-selection\", \"notebook:replace-selection\", ], \"args\": {\"text\": \" %>% \"} } } ] } gpg signatures \u00b6 Import gpg keys, if any for code signing . More at https://unix.stackexchange.com/a/392355/28675 Earlier I installed required gpg packages, gpg and python-gnupg but they ended up conflicting with gpg-agent that is running by the system gpg at /usr/bin/gpg . So, I have to remove both conda packages in order to use system gpg at /usr/bin/ . mamba remove -c conda-forge gnupg python-gnupg Removing packages using mamba remove This also removed several other packages which were required by gpg packages but not by other packages still present in conda env. However, over time, you may end up compiling softwares outside of conda env but still using certain dependencies installed via conda. If so, be careful running mamba remove command as it can not check dependencies for softwares installed outside conda env , and removing packages like below may break your compiled tools. - gnupg 2.3.3 h7853c96_0 installed - libassuan 2.5.5 h9c3ff4c_0 installed - libgcrypt 1.9.4 h7f98852_0 installed - libgpg-error 1.42 h9c3ff4c_0 installed - libksba 1.3.5 hf484d3e_1000 installed - npth 1.6 hf484d3e_1000 installed - ntbtls 0.1.2 hdbcaa40_1000 installed - python-gnupg 0.4.8 pyhd8ed1ab_0 installed Careful with gpg command For code signing, you do not need private keys and public keys works ok. Make sure to check gpg documentation before running these commands. Incorrect use may expose your private keys (worst if you push incorrect keys to a public gpg server!) and defeats the purpose of encryption. ## list public keys, if any ## This will setup ~/.gnupg dir if running command for the first time ## Do chmod 700 ~/.gnupg in case dir perm are not correctly set gpg --list-keys gpg --allow-secret-key-import --import private_public.key ## list public keys gpg --list-keys ## set trust level ## set a valid trust level after reading documentation gpg --edit-key { KEY } trust quit ## list secret keys gpg --list-secret-keys rmate \u00b6 I user rmate command to open remote files on HPC in the text editor like Atom or SublimeText on my macbook. Prefer installing standalone binary over ruby-based ( gem install rmate ) command. If you prefer ruby based installation, better to add ruby installation in a separate conda env, e.g., in luke or other backend env. # in base env ## download standalone binary and save as rmate in ~/bin/ mkdir -p ~/bin curl -Lo ~/bin/rmate https://raw.githubusercontent.com/textmate/rmate/master/bin/rmate chmod 700 ~/bin/rmate Read usage instructions for more on using rmate command. Backup conda env \u00b6 Let's backup conda setup we have done so far. I will backup configurations for each of conda env we created above. I have created a small wrapper, conda_bkup.sh - using conda env export and conda list commands - to backup conda env. Base or root env ~/conda_env_bkup/sumner/base/ / conda_bkup.sh dev env ~/conda_env_bkup/sumner/yoda/ / conda activate yoda && \\ conda_bkup.sh ## return to base env conda deactivate Start JupyterLab \u00b6 Optional: Install several tools in base env. # I use dos2unix often to fix line endings for # files created from windows (dos2unix) or mac (mac2unix) ## A few other packages for jupyterlab extensions mamba install -c conda-forge jupyter_http_over_ws jupyterlab-link-share dos2unix ## check for successful install echo $? # Run only if you have installed jupyter_http_over_ws AND # you are familiar with managing jupyter server backend. # jupyter server extension enable --py jupyter_http_over_ws Test jupyterlab run. \u2620\ufe0f Use SSL and password protection \u2620\ufe0f Avoid running notebook server without SSL and proper password and token configuration as detailed above ) else you may encounter a significant data security risk. mkdir -p ~/tmp/jupyter/sumner ## capture LAN IP for a login or compute node ## https://stackoverflow.com/a/3232433 REMOTEIP = \" $( hostname -I | head -n1 | xargs ) \" ## test run from a login or compute node ## SSL related settings will be inherited from jupyter config file that ## we already have created as above. jupyter lab --no-browser --ip = \" ${ REMOTEIP } \" |& tee -a ~/tmp/jupyter/sumner/runtime.log Once a jupyter session begins and assuming you are on a secure local area network, you can open URL: https://<REMOTEIP>:<PORT>/lab to launch jupyter lab. Here, <PORT> is randomly assigned when you start a server and URL will be displayed on the terminal or in a log file at ~/tmp/jupyter/sumner/runtime.log . Run jupyterlab from a compute and not login node Avoid running JupyterLab server on a login node. It will most likely be killed by HPC admins. For longer running and compute-intensive jupyterlab sessions, it is preferable to run jupyterlab from a compute and not a login node. This requires series of secure port forwarding which is beyond the scope of current documentation. However, your HPC may already have support for running JupyterLab on a compute node, e.g, similar to this one at Univ. of Bern or Princeton Univ. . Talk to your HPC staff for policies on running JupyterLab server. Before continuing setup (not over yet!), let's logout and login first from interactive job and exit HPC. exit # from interactive session exit # from sumner ssh sumner In Part 3 , I will finalize setting up Sumner (or CPU-based) HPC and also install a dedicated conda env for Winter (GPU-based) HPC. If you like to stop here, you may except I prefer that you follow bash startup section in Part 3, so that conda and program-specific (R and python) environment variables are consistently loaded across HPC bash user env. Notice that there is no R in the base env. So, hitting R will not start R session unless you do mamba activate yoda ! \u21a9 Based on a reply from @krassowski at JupyterLab forums \u21a9","title":"Part 2"},{"location":"hpc/cpu/sumner_2/#configuring-conda-env","text":"Typically, we use conda command to manage all of conda env, e.g., conda install package , conda update package , conda list , etc. However, we will now use mamba instead of conda . Mamba is a faster drop-in replacement for conda to manage packages. Use mamba instead of conda Moving forward, do not forget to use mamba instead of conda for all of available commands. See mamba --help to list available commands. At present, following commands from conda are supported by mamba : install, create, list, search, run, info and clean. For rest of commands, e.g, config, activate, deactivate, etc., use conda command. Turns out mamba activate <env_name> or mamba deactivate also works following one-time command: mamba init which will ensure sourcing mamba shell variable at the bash startup by writing a few lines towards the end of ~/.bashrc file. Please note to execute mamba init after conda init as in Part 1: Initialize conda . This will ensure at the bash startup sequence to load conda setup prior to mamba setup. If you end up running mamba init now (I ended up running it late in the setup), prefer activating or deactivating conda env using mamba and not conda command. It is important to install packages only from a single channel and not do mix-and-match install. Read more at conda-forge page on channel_priority: strict which is enabled as default for conda v4.6 or higher. We are using conda v4.10.3 and anaconda v2021-11. We can check that using conda --version and conda list anaconda respectively. Add Bioconda and conda-forge channels to get updated and compbio related packages. Do not change the order of following commands and a command with --add channels conda-forge must be the last one else other channels may take a priority over default conda-forge channel. conda config --add channels bioconda conda config --add channels conda-forge Above command will generate ~/.condarc file and sets priority for channels, i.e., when same package is available from more than one channels, we prioritize installation per ordered channel list in ~/.condarc file as below. This file should be present after above commands and no need to edit unless changing priority of channels. precedence of condarc file Since we are using Mambaforge and not anaconda3, it already ships with conda-forge as a default channel, as specified in cat \"${CONDA_PREFIX}\"/.condarc . Note that ~/.condarc will take precedence over default \"${CONDA_PREFIX}\"/.condarc file, so ensure that conda-forge is the most preferred channel (first channel) in both files. Besides channel priority, I have added a few other custom settings to my ~/.condarc file. For more on these settings, please refer to condarc documentation before applying to your own HPC env. ~/.condarc is a yml format file, so take care of preceding spaces (and not tabs) before and after - while editing this file. channels : - conda-forge - bioconda auto_update_conda : False always_yes : False add_pip_as_python_dependency : True ssl_verify : True allow_softlinks : True use_only_tar_bz2 : False anaconda_upload : False repodata_threads : 4 verify_threads : 4 execute_threads : 4 Do not exceed requested resources for an interactive job If you increase number of threads for conda operations, make sure that you request required threads for an interactive HPC job else you may consume more resources than requested threads, and HPC workload manager may kill your interactive job - conda env can potentially break if interrupted during install or update command. You can verify custom set ~/.condarc configurations using conda config --get command. If some of key:value pairs are showing warnings saying unknown key , you should remove those entries from ~/.condarc . If channel priority has been set as above, you will notice --add channels 'conda-forge' # highest priority from the output of conda config --get command.","title":"Configuring conda env"},{"location":"hpc/cpu/sumner_2/#update-conda-package","text":"Since we used Mambaforge, anaconda3 variant from a third-pary conda-forge open-source community, let's make sure that the base conda package is the most up-to-date or not. conda --version conda 4.10.3 To check if this is the current version, we will update conda package. mamba update conda You may notice that conda update is available else no further action needed. Notice source of updated conda package from conda-forge/linux-64 . That is because we set conda-forge with the highest channel priority in ~/.condarc . - conda 4.10.3 py39hf3d152e_2 installed + conda 4.11.0 py39hf3d152e_0 conda-forge/linux-64 17 MB Confirm that conda package is now updated with an updated version, if any. conda --version conda 4.11.0","title":"Update conda package"},{"location":"hpc/cpu/sumner_2/#jupyterlab","text":"For most packages, e.g., R, snakemake workflow, etc., we will use dedicated conda env and avoid installing into base env. That is to keep base env clean and without much of dependencies. Unlike base env, additional env can be recreated without a risk of breaking conda setup. However, we will require a few packages, e.g., JupyterLab and Notebook, that typically ships with regular (and not miniconda or mambaforge) anaconda3 installation. Install JupyterLab in its own conda env If you prefer, you can skip installing JupyterLab in base env and instead use its own dedicated env. This is perhaps a preferred way to keep base env minimal and also allows you to update JupyterLab from time to time without worrying about breaking base env. However, when you start jupyterlab session, you need to switch (activate) to the respective conda env from base or other envs. To install jupyterlab in its dedicated env, do following: mamba create -c conda-forge -n jlab jupyterlab nodejs jupyterthemes jupytext dos2unix jupyter_http_over_ws jupyterlab-link-share mamba activate jlab ## check installed extensions, if any jupyter lab extension list jupyter server extension list Read install guide for extensions, if any, e.g. some extensions like jupyter_http_over_ws are not enabled by default for good (saftey) reasons. JupyterLab is similar to RStudio IDE and provides richer interface to several programming languages, including python, R, julia, and many more. To install jupyterlab, please read installation guide . ## core package mamba install -c conda-forge jupyterlab Even though conda-forge is set as the highest priority channel in ~/.condarc , I am explicitly specifying to use the same channel while running install or update command. This will install jupyterlab and series of its dependencies. You can check version of related packages using mamba list | grep -E \"jupyter\" although versions may differ as they get updated over time. # packages in environment at /projects/verhaak-lab/amins/hpcenv/mambaforge: # # Name Version Build Channel jupyter_client 7.1.0 pyhd8ed1ab_0 conda-forge jupyter_core 4.9.1 py39hf3d152e_1 conda-forge jupyter_server 1.12.1 pyhd8ed1ab_0 conda-forge jupyterlab 3.2.4 pyhd8ed1ab_0 conda-forge jupyterlab_pygments 0.1.2 pyh9f0ad1d_0 conda-forge jupyterlab_server 2.8.2 pyhd8ed1ab_0 conda-forge Optional: I have also installed following set of packages in the base env for my own needs or convenience for not switching to other conda environments. If you do so, try to limit installing packages in base env to bare minimum, and avoid installing packages that require multiple dependencies, e.g., R, tensorflow, node, julia, GO library, etc. mamba install -c conda-forge git rsync vim globus-cli tmux screen We will setup rest of jupyterlab settings and initialize it later after we install R in a separate conda env.","title":"JupyterLab"},{"location":"hpc/cpu/sumner_2/#installing-r","text":"We will install R and other routinely used tools in a separate conda env for reasons explained above. You should read official documentation on installing R to familiarize with steps that I am going to follow below.","title":"Installing R"},{"location":"hpc/cpu/sumner_2/#create-a-new-env","text":"I am going to name a new env as yoda . Ideally, naming should be such that you should not have difficulty finding which env to switch to for the type of packages and analysis you may end up running at the later date, e.g., in my case, while far from ideal, I have used nomenclature based on the Jedi members for conda env: env id intended use yoda hosts all packages, including R and others that I use on daily basis luke serves as an env for background processes using databases leia a standalone env for running snakemake workflows obiwan fallback to yoda when I require to use another mature version of R or other packages windu conda env using legacy Python 2 over Python 3 anakin dev or beta env for testing: Optimized for CPU-based HPC rey Similar to yoda but optimized for GPU-based HPC ben dev or beta env for testing: Optimized for GPU-based HPC grogu toy env for everything else: for experimental purpose rey and ben env are optimized for GPU-based, Winter HPC at JAX and should not be used while working in the CPU-based, Sumner HPC at JAX. However, all of CPU-based envs, e.g., yoda, luke, leia , etc. will work on both, Sumner and Winter HPCs. Let's create the first env, yoda and install base R package and a several essential R packages for routine analysis. mamba create -c conda-forge -n yoda r-base r-essentials Note that most up-to-date R version may be available in the conda-forge or sometimes in the other conda channels, like r or bioconda . However, it is preferable to install R from the first priority channel , i.e., conda-forge in our case. Activate a new env. Note that we use conda instead of mamba command as the latter (at least for now) only accepts following sub commands: install, create, list, search, run, info and clean. conda activate yoda You will notice bash prompt changing from (base) [userid@sumner50] to (yoda) [userid@sumner50] .","title":"Create a new env"},{"location":"hpc/cpu/sumner_2/#pin-r-and-conda-auto-updates","text":"Before moving further, let's pin R version to 4.1.1 (at the time of this writing) and also disallow conda auto-updates. That way, we have lesser chances of breaking conda env when we do mamba install <pkg> in future, and carefully install/update packages without breaking existing setup. For more on pinning packages, read official documentation . Technical note Typically, I avoid installing or updating package if mamba install throws a message or warning about removing or downgrading existing packages . In such cases, I fall back to compiling package using available devtools in conda . Also, I load compiled package using Modulefile when needed, and not integrate it in my default bash environment as this may give errors while running some random program due to conflicts in shared library versions. ## we already set auto update to False above ## under ~/.condarc settings # conda config --set auto_update_conda False # Find package version using mamba list | grep -Ei 'r-base' We can notice that R version is 4.1.1 (or higher). You can also check R version by R --version . Remember this version and add it to following newly created file: nano \" ${ CONDA_PREFIX } \" /conda-meta/pinned Note that echo ${CONDA_PREFIX} points to conda-meta/ directory under yoda and not the base env because we are within yoda env. In other words, pinned packages env specific and you can update R package in other environment(s), if present. Add following as a new line entry: r-base ==4.1.1 Check a valid line break Since we are creating a new file and only adding a single line of text, when we save this text file, we should confirm that it is the end of the line . This is usually recognized by pressing the Enter . Unix systems recognizes such line break using an invisible $ sign which you can confirm by running cat -e \"${CONDA_PREFIX}\"/conda-meta/pinned r-base ==4.1.1$ $ With each line break, you will notice $ sign, e.g., two lines in my case. You may remove a second line by editing file again but make sure to run cat -e \"${CONDA_PREFIX}\"/conda-meta/pinned to check a valid line break. You may pin only part of the major and minor version , i.e., to allow updates from 4.1.1 to 4.1.2 or 4.1.3 but not from 4.1.1 to 4.2.*. However, I rather freeze the specific version and update to builds at the later date, if need arises. To do so, remove the pinned entry from \"${CONDA_PREFIX}\"/conda-meta/pinned and do mamba update r-base=4.1.2 or other version. Since R developer team updates major.minor version of R every quarter or so, I try to keep those R versions in a separate env rather updating as certain R packages may throw an error with such major updates.","title":"Pin R and conda auto-updates"},{"location":"hpc/cpu/sumner_2/#install-r-libraries","text":"You may now install additional R libraries (or packages - it's alias!) or any other packages, e.g., git, bedtools, samtools, etc. in the new env using mamba install command. Just make sure that you are in the new env by conda activate yoda . You can check which env you are in by echo \"$(basename ${CONDA_PREFIX})\" Some of packages, e.g., git, rsync, etc. are already installed in the base env. However, they may not be recognized in the new env: yoda . Ideally, you should install the same package in the new env. Conda will usually link package files (which takes much of space) from the central package directory, so installing the same package in different env should not take significant additional space. To search for packages, prefer using anaconda website and look for packages that are under conda-forge/ or bioconda channels, i.e., the first and second preference, respectively in our ~/.condarc file. Avoid installing packages from non-standard channels For a stable and error-free conda env, avoid installing conda packages from non-standard channels, i.e., a channel other than conda-forge and bioconda or ones specified in ~/.condarc file. Installing packages from non-standard channels will unnecessarily increase complexity of package dependencies in conda env and will increase likelihood of slowing or breaking down one or more conda env. Note that we have yet to create additional conda env besides a default base env. Install R packages: You can tie up all of your packages in a single command or break it down to smaller chunks. The former appraoch may take longer and difficult to debug if package installation fails due to conflicting dependencies with one or more packages. mamba install -c conda-forge r-tidyverse r-tidymodels r-devtools r-biocmanager mamba install -c conda-forge gnupg git rsync vim openjdk r-rjava mamba install -c conda-forge bedtools pybedtools bedops mamba install -c conda-forge matplotlib scikit-learn Reticulate and rpy2 packages will allow us to use R and python interchangeably in the same R script or python notebook, respectively! Read details about reticulate on RStudio website and rpy2 here .","title":"Install R libraries"},{"location":"hpc/cpu/sumner_2/#reticulate-r-package","text":"mamba install -c conda-forge r-reticulate","title":"reticulate R package"},{"location":"hpc/cpu/sumner_2/#rpy2-python-package","text":"mamba install -c conda-forge rpy2 If above command works without forcing you to downgrade python, R, or other major packages, good for you! If not, it is due to strict requirements of rpy2 package which can conflict with other core packages in yoda env. So, mamba install -c conda-forge rpy2 may not work. \u2620\ufe0f Beware of using pip install in conda env \u2620\ufe0f You should be comfortable compiling and installing packages using pip install and knowing how to manually install python package requirements. If not, it is safer to skip installing rpy2 (or any other) package by steps detailed below. Wait until conda-forge developers make rpy2 package available. conda-forge developer community is pretty good and active, so patience should pay off then risking to break an otherwise functioning yoda env! From rpy2 setup.py and requirements.txt file, list number of packages required as dependencies for rpy2. If we do pip install rpy2 , pip will automatically install these requirements. However, I prefer to let conda manage all package versions because pip may not necessarily evaluate if certain package versions are compatible with other conda-installed packages . That's why I prefer installing requirements by myself using mamba install and then do pip install , so only minimal set of requirements will be installed by pip command. That way, I can minimize chances of breaking conda env due to conflicting package dependencies. ## required packages by rpy2 ## see how many of these are already installed conda list | grep -E \"cffi|pytest|pandas|numpy|jinja2|pytz|tzlocal\" Install missing packages using mamba . ## ok to write packages which are already installed ## conda will take care of version conflict, if any. mamba install -c conda-forge cffi pytest pandas numpy jinja2 pytz tzlocal Now try (fingers crossed!) installing rpy2 using pip. Make sure that rpy2 pip website is showing same version (3.4.5 in my case) as on rpy2 github website else you need to download source file from pip website, and check respective setup.py and requirements.txt file to ensure dependencies are identical and satisfied or installed via mamba install command. ## in yoda env mkdir - ~/logs && \\ pip install rpy2 |& tee -a logs/pip_install_rpy2.log Success installing rpy2 Great! pip install did not end up installing any dependencies (as we already installed those using mamba install ), and rpy2 is now successfully installed. Collecting rpy2 Downloading rpy2-3.4.5.tar.gz (194 kB) Preparing metadata (setup.py): started Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: cffi>=1.10.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (1.15.0) Requirement already satisfied: jinja2 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (3.0.3) Requirement already satisfied: pytz in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (2021.3) Requirement already satisfied: tzlocal in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from rpy2) (4.1) Requirement already satisfied: pycparser in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from cffi>=1.10.0->rpy2) (2.21) Requirement already satisfied: MarkupSafe>=2.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from jinja2->rpy2) (2.0.1) Requirement already satisfied: pytz-deprecation-shim in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from tzlocal->rpy2) (0.1.0.post0) Requirement already satisfied: tzdata in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal->rpy2) (2021.5) Building wheels for collected packages: rpy2 Building wheel for rpy2 (setup.py): started Building wheel for rpy2 (setup.py): finished with status 'done' Created wheel for rpy2: filename=rpy2-3.4.5-cp310-cp310-linux_x86_64.whl size=300939 sha256=136144e165e2b0f156fdb7f525547ae073dda06346a893de398aa6836e625af1 Stored in directory: /home/amins/.cache/pip/wheels/ba/d8/8b/68fc240578a71188d0ca04b6fe8a58053fbcbcfbe2a3cbad12 Successfully built rpy2 Installing collected packages: rpy2 Successfully installed rpy2-3.4.5 Since we just installed JAVA (openjdk) and rJava R library, for sanity check, run R CMD javareconf (to print and update java configuration for R) and echo $JAVA_HOME (to confirm that bash variable is correctly set). If you also have installed git, you can copy boilerplate ~/.gitconfig , and make changes under line starting with name, email, and excludesfile. Make sure to read about gitconfig too. Best practices using mamba install or mamba update Keep a habit of checking following when using mamba install command: Prefer conda-forge channel, followed by bioconda , and avoid installing packages from other channels. You can do so in dev or beta env, e.g., env grogu in my case to see how it pans out. Before hitting Y/Yes to install packages, ensure that installing or updating packages does not force downgrading of one or more major packages, like python, R, and any other packages that you deem it as major package in your routine analysis and downgrading it may break reproducibility of your analysis. You can always use dev env, like grogu to play packages showing such downgrade warnings. While doing mamba install -c conda-forge samtools bcftools htslib , I noticed openssl dependency being downgraded from v3.0.0 to v1.1.1. I find it a major downgrade to one of the core package and so I skipped installing samtools and related packages using mamba . I would rather install these packages by compiling from their respective source tar balls. You can read installation details for samtools family packages at author's website . Similarly for mamba install -c conda-forge bedtools pybedtools bedops , I got following error as yoda env is using python 3.10 and it is pinned by default. So, it can not be downgraded! Similar to samtools, I will compile pybedtools or use a separate env, like grogu for such packages with unique requirements! For bedops , it was asking me to install older version of samtools which I was not ok with. I will rather compile those tools from the current version. So, finally I ended up installing only bedtools with clean requirements! mamba install -c conda-forge bedtools . package pybedtools-0.8.2-py39h39abbe0_1 requires python >=3.9,<3.10.0a0, but none of the providers can be installed","title":"rpy2 python package"},{"location":"hpc/cpu/sumner_2/#setup-rprofile-and-renviron","text":"Rprofile and Renviron files provide additional configuration option for R, similar to ~/.condarc file to manage several conda-related configurations. You can read more about these files at R developer website and RStudio website . For detailed notes, Jennifer Bryan and Jim Hester has written an excellent resources, titled What they forgot to teach you about R with a chapter on R Startup . Setup R library directory path for R 3.6 Before setting up R startup, I will make a dedicated package directory that will store R libraries or packages. By default, R will store all libraries at conda env specific path, i.e., for my case, it is at \"${CONDA_PREFIX}\"/lib/R/library/ . You can check this path using Rscript -e '.libPaths()' . This default path is ideally intended for R libraries managed via mamba install or mamba update command. However, I also compile R libraries using install.packages() R command when I find conflicting dependencies in installing R libraries using mamba install command. In such cases, I prefer to use a separate R library directory than a default R library path. I am making an empty library directory for installing libraries or R packages that I may compile using R install.packages() command. R usually defaults to making such user-package directory in the user's home path but I will use tier 1 space again to avoid filling up my home directory with a limited quota. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/4.1 Create ~/.Renviron file nano ~/.Renviron Add following to ~/.Renviron . Notice the order in which R will store newly compiled packages. It will use an empty directory we just created to store new packages. If for some reasons, this directory is not accessible (file permission errors), R will fallback to the second path, and so on. The second path is an expanded path of a default R package directory, i.e., output of echo _\"${CONDA_PREFIX}\"/lib/R/library/_ command. R_LIBS = \"/projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/4.1:/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/library\" You can confirm precedence of R library paths in R using .libPaths() R command or from bash using Rscript -e '.libPaths()' . Create ~/.Rprofile file nano ~/.Rprofile Add following contents. Please read more about configuring R startup from links shared earlier in this section. Comment out lines using # if you do not require one or more of following options. ## set user specific env variables, e.g., GITHUB_PAT here Sys.setenv ( \"GITHUB_PAT\" = \"my_github_secret_token\" ) ## Default source to download packages local ({ r <- getOption ( \"repos\" ) r [ \"CRAN\" ] <- \"https://cran.rstudio.com\" options ( repos = r ) }) Protect secret tokens, passwords, etc. If your ~/.Rprofile file contains any secret tokens, it's a best practice to make it read/write only by file owner (you) using chmod 600 ~/.Rprofile . Same goes for similar files in your home directory, e.g., .gitconfig , .netrc , etc. Equivalent command for directory, e.g., for .ssh/ is chmod 700 ~/.ssh . Read more about linux file permissions .","title":"Setup Rprofile and Renviron"},{"location":"hpc/cpu/sumner_2/#compiling-r-libraries","text":"Since we already have setup yoda env for R, you can now install additional R libraries using mamba install r-PackageName ( see above ) as long as it is available in anaconda repository , preferably in conda-forge or bioconda sources AND it does not significantly downgrades essential packages like python, R, and other core libraries, e.g., zlib, openssl, etc. The latter is subjective and depends on what you will consider as an essential. In any case where I have doubts of breaking conda env, I fall back to compiling R library using native R command: install.packages() . However, I am going to wait using this command until I have finished my HPC setup, specifically, bash startup using ~/.profile.d configuration. We are not there yet but not far from it too!","title":"Compiling R libraries"},{"location":"hpc/cpu/sumner_2/#install-essentials","text":"This can again vary per user's need and optional. If you find errors compiling packages, you may end up installing respective dev libraries, e.g., libiconv, zlib, etc. if they are not already installed in the current ( yoda in this case) conda env. mamba install -c conda-forge wget curl rsync libiconv parallel ipyparallel","title":"Install essentials"},{"location":"hpc/cpu/sumner_2/#setup-jupyterlab","text":"We have installed JupyterLab earlier but did not finish complete setup. Let's do that! In this section, I will setup jupyter notebook and kernels to launch python, R, and bash based notebooks. I will also harden notebook server with several security settings. Remember that we will be using JupyterLab installation from the conda base env even though JupyterLab could have been installed in yoda and other conda env. Managing jupyterlab server from base env is convenient and allows us to interact with all other env. Also, if we end up resetting or deleting other env, jupyter configuration in base env will remain intact. First, we install language-specific kernels. By default, jupyterlab ships with python kernel, named ipykernel which is backend when we interact with jupyter python notebook. Jupyter can allow us to use other language-specific kernels to interact with R, Julia, bash, and many more languages . Accordingly, I will install kernels for R and bash in specific envs, e.g., yoda and then switch back to base env to configure jupyterlab, such that we can connect to kernels in yoda env from conda base env. Make sure you are in yoda env. We will switch to base env after we configure all kernels in yoda env. conda activate yoda Install kernels mamba install -c conda-forge ipykernel r-irkernel bash_kernel Note that I have also installed python kernel, ipykernel in yoda env as we may not have jupyterlab installed in yoda as we only need to install jupyterlab in the base env. Also, some of kernels may already have been downloaded as part of dependency for other packages we installed earlier. You can check conda list | grep kernel output to confirm which kernels are already installed. Now we can link each of these kernels to jupyterlab in base env.","title":"Setup JupyterLab"},{"location":"hpc/cpu/sumner_2/#python-kernel","text":"python -m ipykernel install --user --name yoda_py310 --display-name \"yoda_py310\" # confirm that installation exited without any error echo $? # this should return 0 for successful installation --name and --display-name will show up as kernel file location at ~/.local/share/jupyter/kernels/ and icon name in the jupyterlab launcher page, respectively. You can name as you like but without any spaces or special characters. I am following naming format that uses env followed by language and its major and minor version.","title":"python kernel"},{"location":"hpc/cpu/sumner_2/#r-kernel","text":"Start R session R Install kernel while in yoda env Read available options for IRkernel installation. Also, consider installing jupytertext-text-shortcuts but not now and we can install this along with other extensions towards the end of configuring jupyterlab. library ( IRkernel ) installspec ( name = \"yoda_r41\" , displayname = \"yoda_r41\" , user = TRUE ) ## quit R session q ( save = \"no\" ) Similar to python kernel, r kernel should now be at ~/.local/share/jupyter/kernels/ .","title":"R kernel"},{"location":"hpc/cpu/sumner_2/#bash-kernel","text":"We will now install bash kernel in yoda env. Unlike other (R and python) kernels, bash kernel do not need to be installed in all of conda env because we can always switch between conda env specific bash env using mamba activate anakin or mamba activate rey , etc. Here, I will install bash kernel in yoda and not base env as I intend to use yoda as my primary go-to env when I login to HPC. Keep use of base env to bare minimum Please remember that we keep use of base to bare minimum for maintaining core of codna packages, and should avoid installing (and populating dependencies) packages in base env. You can always delete secondary conda env and restart but you cannot do so with base env! ## in yoda env mamba list | grep -E \"bash_kernel\" ## if this does not show bash_kernel installed, redo install # mamba install -c conda-forge ipykernel r-irkernel bash_kernel ## and the install kernel into jupyter env python -m bash_kernel.install Unlike python and R kernels, I could not find overriding default name and display_name for bash kernel. So, I will rename bash kernel manually else if I end up installing similar kernel from other env, it will override kernel with the same default name: bash . Typically, you do not need to install bash kernels in all conda env as jupyterlab ships with a powerful terminal that allows switching from one to other env using same conda activate command. So, I hardly use bash kernel. ## go to kernel base dir cd ~/.local/share/jupyter/kernels/ ## rename bash dir to yoda_bash mv bash yoda_bash ## edit bash kernel.json to rename display name cd yoda_bash nano kernel.json Rename \"display_name\": \"Bash\" to \"display_name\": \"yoda_bash\"","title":"bash kernel"},{"location":"hpc/cpu/sumner_2/#kernel-loading","text":"Following completion of the entire setup, we are going to run JupyterLab from the base env. However, on daily basis, we like to access Python and R from yoda and not base 1 . Default kernel setup above should let jupyterlab handle conda env specific python but not so for other kernels. However, I have noticed issues running Python and R from a non base conda env as sometimes packages requiring shared libraries may throw an error as such shared libraries are either missing in base env or have a different version than one in the current env, i.e., yoda env where package was originally installed or compiled. I mitigate such issues by loading a valid bash env prior to initializing kernel , e.g., I will wrap a default jupyter kernel settings into a bash script (wrapper) and will activate a valid conda env, e.g., yoda in this case prior to initializing yoda specific Python or R kernel. That way, kernel will consistently inherit a valid login (bash) env for the respective conda env.","title":"kernel loading"},{"location":"hpc/cpu/sumner_2/#yoda-python","text":"Create a new kernel wrapper matching name of kernel we like to edit, e.g., /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310 mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/kernels touch /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310 # make file executable chmod 700 /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310 Add following to wrap_yoda_py310 file. Change user paths where applicable. #!/bin/bash ## Load env before loading jupyter kernel ## @sbamin ## https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425 #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 # I am using conda instead of mamba to activate env # as somehow I notices warnings/errors sourcing # mamba.sh in sub-shells. CONDA_BASE=$(conda info --base) && \\ source \"${CONDA_BASE}\"/etc/profile.d/conda.sh && \\ conda activate yoda #### END CONDA SETUP #### # this is the critical part, and should be at the end of your script: exec /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/bin/python -m ipykernel_launcher \"$@\" ## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/<kernel_name>/kernel.json #_end_ Now, adjust kernel settings. ## go to kernel base dir cd ~/.local/share/jupyter/kernels/ ## there should be yoda_py310 directory or ## one matching --name yoda_py310 argument ## we used above when installing python kernel cd yoda_py310 ## edit kernel.json nano kernel.json Replace contents of kernel.json with following: { \"argv\" : [ \"/projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_py310\" , \"-f\" , \"{connection_file}\" ], \"display_name\" : \"yoda_py310\" , \"language\" : \"python\" , \"metadata\" : { \"debugger\" : true } }","title":"yoda python"},{"location":"hpc/cpu/sumner_2/#yoda-r","text":"Now, we can reconfigure R kernel for yoda same as above but with a few changes in the wrapper script. Create a new kernel wrapper for R, e.g., /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41 mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/kernels touch /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41 # make file executable chmod 700 /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41 Add following to wrap_yoda_r41 file. Change user paths where applicable. #!/bin/bash ## Load env before loading jupyter kernel @sbamin https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425 #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 CONDA_BASE=$(conda info --base) && \\ source \"${CONDA_BASE}\"/etc/profile.d/conda.sh && \\ conda activate yoda #### END CONDA SETUP #### ## this is the critical part, and should be at the end of your script: ## path to R and arguments come from original kernel.json under ## ~/.local/share/jupyter/kernels/yoda_r41/ directory. ## In some cases, path to R may differ and may originate from ## .../envs/yoda/lib64/R/bin/R instead of .../envs/rey/lib64/R/bin/R ## If so, adjust path to R here accordingly. exec /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/bin/R --slave -e \"IRkernel::main()\" --args \"$@\" ## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/<kernel_name>/kernel.json #_end_ Now, adjust kernel settings. ## go to kernel base dir cd ~/.local/share/jupyter/kernels/ ## there should be yoda_py310 directory or ## one matching --name yoda_py310 argument ## we used above when installing python kernel cd yoda_r41 ## edit kernel.json nano kernel.json Replace contents of kernel.json with following: { \"argv\" : [ \"/projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_r41\" , \"{connection_file}\" ], \"display_name\" : \"yoda_r41\" , \"language\" : \"R\" } Done! Next time you run jupyter, you should have a new julia kernel in JupyterLab.","title":"yoda R"},{"location":"hpc/cpu/sumner_2/#configure-jupyterlab","text":"Once we have installed env specific kernels as in yoda (and other envs, if a ny), now it's a time configure JupyterLab in the base env. ## deactivate yoda env conda deactivate You should now be in conda base env If you were jumping across more than one conda envs, then each instance of conda deactivate command will bring you back to previously active env. So, make sure to return to base env which you can confirm using echo $CONDA_PREFIX output. That should point to base path of conda (mambaforge in my case) installation: /projects/verhaak-lab/amins/hpcenv/mambaforge/ . Also, notice change in bash prompt to (base) userid@sumner50 . Once in the base env, generate skeleton for default jupyter configuration. ## return to home dir cd \" ${ HOME } \" && \\ jupyter server --generate-config Writing default config to: /home/userid/.jupyter/jupyter_notebook_config.py Secure Jupyter Server It is critical that you harden security of jupyterlab server. Default configuration is not good enough (in my view) for launching notebook server over HPC, especially without SSL (or https ) support. Setting up individual security steps is beyond scope of this documentation. However, I strongly recommend reading official documentation on running a public Jupyter Server and security in the jupyter server . Checkout this example guide on creating self-signed SSL certificates in case you do not have SSL certificates from Research IT department.","title":"Configure JupyterLab"},{"location":"hpc/cpu/sumner_2/#self-signed-ssl-certificates","text":"Ideally you should have SSL certificates signed by a verified certificate authority (CA) else most of modern web browsers will issue a warning or an error regarding SSL secutity. CA-signed SSLs are typically paid unless issued via let's encrypt or your DNS providers. Securing a website (Jupyter Env in our case) is beyond scope of this guide but I will suggest to inquire to your research IT department to see if they can help secure your Jupyter env. It's better to have CA-signed SSL over self-signed SSL, and always better to have SSL (https) over insecure (http) connection to your Jupyter env. This is true even when you are working within a secure firewall of your work network. Here, I am creating a self-signed SSL which most of you can generate and at least have self-signed SSL. See this post for details . ## switch to base conda env mamba activate base ## create dir to save SSL certificates mkdir ~/ssl_cert && \\ chmod 700 ~/ssl_cert && \\ cd ~/ssl_cert ## generate a private key openssl genrsa -out hpc_cert.key 2048 ## secure your private key chmod 600 hpc_cert.key ## create a signed certificate using a key we generated above openssl req -new -key hpc_cert.key -out hpc_cert.csr ## finally self-sign this certicate openssl x509 -req -days 365 -in hpc_cert.csr -signkey hpc_cert.key -out hpc_cert.pem While creating a signed certificate, hpc_cert.csr , you will be prompted to enter issuer's country, city, etc., including Common Name (CN). You should put some info related to use of this certificate into respective fields, e.g., Common Name can be hpcjupyter . SSL certificate - unable to verify SSL connection Regardless of what you put in CN, say hpcjupyter.mywebsite.com , most of modern web browsers so either SSL warning or error of not verifying your self-signed SSL. This is because you have self-signed this SSL and not using an approved certificate authority (CA) provider for signing your SSL. Read this post for details . In essense, your self-signed SSL is acting as it is an authentic SSL for a CN you provided above, hpcjupyter.mywebsite.com . Since that's a clearly a security risk, most modern browsers will throw a warning before you are allowed to visit a site (Jupyter server in this case) or even may not allow at all to load that website! Hence, prefer asking your Research IT to make a signed SSL available for a specific intranet subdomain, e.g., jupyter.company.com for users to use.","title":"self-signed SSL certificates"},{"location":"hpc/cpu/sumner_2/#cookie-file","text":"After creating SSL certificates, also create a cookie file for jupyter server. openssl rand -hex 32 > /home/foo/ssl_cert/jp_cookie chmod 600 /home/foo/ssl_cert/jp_cookie","title":"cookie file"},{"location":"hpc/cpu/sumner_2/#config-file","text":"Example config for /home/userid/.jupyter/jupyter_server_config.py . Please do not copy and paste these options without knowing underlying details . Jupyter token - provide a strong token string While editing jupyter config file below, please read inline comments carefully, especially for c.ServerApp.token . Do not use the default token as that token is used for login to your running jupyter server . Provide a secure (a longer string, 32 characters or more) string generated using uuidgen and after removing dashes. Do not worry of remembering this token. Jupyter does allow you to have an alternate way of login using a custom, user-generated password (see further below). ## leave commented out portion of default config as it is. ## then you can add your custom config ## Do not copy these configurations without knowing what they do! #### NOTEBOOK CONFIGS #### ## SSL settings ## read documentation for details c . ServerApp . certfile = u '/home/foo/ssl_cert/hpc_cert.csr' c . ServerApp . keyfile = u '/home/foo/ssl_cert/hpc_cert.key' c . JupyterHub . cookie_secret_file = '/home/foo/ssl_cert/jp_cookie' c . ServerApp . open_browser = False ## token used to programmatically login to jupyter, ## e.g., via VS Code Jupyter extension. ## This is essentially a password to login to jupyter ## Provide an alphanumeric secret string - longer the better. ## you can create one using uuidgen command: Remove dashes. c . ServerApp . token = 'PLEASE_REPLACE_THIS_TOKEN_c8syb2lnd89g2fosyyfskdfy02h' c . ServerApp . allow_password_change = False ## should be set to False ## unsafe to set True from https security point of view c . ServerApp . disable_check_xsrf = False ## use one of available options: See documentation c . Application . log_level = 'INFO' ## use login shell c . ServerApp . terminado_settings = { 'shell_command' :[ 'bash' , '-l' ]} ## END ##","title":"config file"},{"location":"hpc/cpu/sumner_2/#jupyter-password","text":"Once you customize config file above, make sure to generate a secret and strong password using jupyter server password command. Your password then will be written in an encrypted format in /home/userid/.jupyter/jupyter_server_config.json file. Make both files read/write-only by you. ## for directory, we use permission 700 chmod 700 ~/.jupyter # For files, we use permission 600 chmod 600 ~/.jupyter/jupyter_server_config.py chmod 600 ~/.jupyter/jupyter_server_config.json","title":"jupyter password"},{"location":"hpc/cpu/sumner_2/#customizing-user-interface","text":"If you need to add custom themes, fonts, shortcuts, etc. for Jupyter, you may follow this section, else safe to skip to Start Jupyter section. Before installing themes or customizing jupyterlab further, I will install node js package to base env. Ideally, base env should not be cluttered with packages except bare mininmum that comes with original conda installation (mambaforge in my case). However, node js is required to setup and manage jupyterlab extensions and how jupyterhub can interact with a few kernels, e.g., jupyterlab-sql extension to interact with sql databases that I will end up installing in the future. mamba install -c conda-forge nodejs npm --version using v8.1.2","title":"Customizing user interface"},{"location":"hpc/cpu/sumner_2/#themes","text":"Themes provide custom user interface and is optional for setup. See example themes at dunovank/jupyter-themes mamba install -c conda-forge jupyterthemes Note: This may downgrade node js. Since I do not use node js in base and it is installed in base to manage jupyter extensions, I was ok downgrading it as it changed only a build and not major or minor version . setup theme, see details here jt -t solarizedl -T -N -f firacode -fs 12 -tf ptserif -tfs 11 -nf ptsans -nfs 12 -dfs 11 -ofs 10 -cellw 90 % -lineh 170","title":"Themes"},{"location":"hpc/cpu/sumner_2/#keyboard-shortcuts","text":"If you are familiar with RStudio shortcuts for R pipe %>% and assignment <- operator, you can enable those in JupyterLab too 2 by first starting a jupyterlab session. You can then go to Advanced Settings Editor either by pressing Cmd + , on a mac or go to Settings from a top menubar, and then clicking Keyboard Shortcuts option. There, under User Preferences pane, you can paste following to enable keyboard shortcuts, i.e., Alt + - for <- and Shift + Cmd + M for %>% operator. { \"shortcuts\": [ { \"command\": \"apputils:run-first-enabled\", \"selector\": \"body\", \"keys\": [\"Alt -\"], \"args\": { \"commands\": [ \"console:replace-selection\", \"fileeditor:replace-selection\", \"notebook:replace-selection\", ], \"args\": {\"text\": \" <- \"} } }, { \"command\": \"apputils:run-first-enabled\", \"selector\": \"body\", \"keys\": [\"Accel Shift M\"], \"args\": { \"commands\": [ \"console:replace-selection\", \"fileeditor:replace-selection\", \"notebook:replace-selection\", ], \"args\": {\"text\": \" %>% \"} } } ] }","title":"keyboard shortcuts"},{"location":"hpc/cpu/sumner_2/#gpg-signatures","text":"Import gpg keys, if any for code signing . More at https://unix.stackexchange.com/a/392355/28675 Earlier I installed required gpg packages, gpg and python-gnupg but they ended up conflicting with gpg-agent that is running by the system gpg at /usr/bin/gpg . So, I have to remove both conda packages in order to use system gpg at /usr/bin/ . mamba remove -c conda-forge gnupg python-gnupg Removing packages using mamba remove This also removed several other packages which were required by gpg packages but not by other packages still present in conda env. However, over time, you may end up compiling softwares outside of conda env but still using certain dependencies installed via conda. If so, be careful running mamba remove command as it can not check dependencies for softwares installed outside conda env , and removing packages like below may break your compiled tools. - gnupg 2.3.3 h7853c96_0 installed - libassuan 2.5.5 h9c3ff4c_0 installed - libgcrypt 1.9.4 h7f98852_0 installed - libgpg-error 1.42 h9c3ff4c_0 installed - libksba 1.3.5 hf484d3e_1000 installed - npth 1.6 hf484d3e_1000 installed - ntbtls 0.1.2 hdbcaa40_1000 installed - python-gnupg 0.4.8 pyhd8ed1ab_0 installed Careful with gpg command For code signing, you do not need private keys and public keys works ok. Make sure to check gpg documentation before running these commands. Incorrect use may expose your private keys (worst if you push incorrect keys to a public gpg server!) and defeats the purpose of encryption. ## list public keys, if any ## This will setup ~/.gnupg dir if running command for the first time ## Do chmod 700 ~/.gnupg in case dir perm are not correctly set gpg --list-keys gpg --allow-secret-key-import --import private_public.key ## list public keys gpg --list-keys ## set trust level ## set a valid trust level after reading documentation gpg --edit-key { KEY } trust quit ## list secret keys gpg --list-secret-keys","title":"gpg signatures"},{"location":"hpc/cpu/sumner_2/#rmate","text":"I user rmate command to open remote files on HPC in the text editor like Atom or SublimeText on my macbook. Prefer installing standalone binary over ruby-based ( gem install rmate ) command. If you prefer ruby based installation, better to add ruby installation in a separate conda env, e.g., in luke or other backend env. # in base env ## download standalone binary and save as rmate in ~/bin/ mkdir -p ~/bin curl -Lo ~/bin/rmate https://raw.githubusercontent.com/textmate/rmate/master/bin/rmate chmod 700 ~/bin/rmate Read usage instructions for more on using rmate command.","title":"rmate"},{"location":"hpc/cpu/sumner_2/#backup-conda-env","text":"Let's backup conda setup we have done so far. I will backup configurations for each of conda env we created above. I have created a small wrapper, conda_bkup.sh - using conda env export and conda list commands - to backup conda env. Base or root env ~/conda_env_bkup/sumner/base/ / conda_bkup.sh dev env ~/conda_env_bkup/sumner/yoda/ / conda activate yoda && \\ conda_bkup.sh ## return to base env conda deactivate","title":"Backup conda env"},{"location":"hpc/cpu/sumner_2/#start-jupyterlab","text":"Optional: Install several tools in base env. # I use dos2unix often to fix line endings for # files created from windows (dos2unix) or mac (mac2unix) ## A few other packages for jupyterlab extensions mamba install -c conda-forge jupyter_http_over_ws jupyterlab-link-share dos2unix ## check for successful install echo $? # Run only if you have installed jupyter_http_over_ws AND # you are familiar with managing jupyter server backend. # jupyter server extension enable --py jupyter_http_over_ws Test jupyterlab run. \u2620\ufe0f Use SSL and password protection \u2620\ufe0f Avoid running notebook server without SSL and proper password and token configuration as detailed above ) else you may encounter a significant data security risk. mkdir -p ~/tmp/jupyter/sumner ## capture LAN IP for a login or compute node ## https://stackoverflow.com/a/3232433 REMOTEIP = \" $( hostname -I | head -n1 | xargs ) \" ## test run from a login or compute node ## SSL related settings will be inherited from jupyter config file that ## we already have created as above. jupyter lab --no-browser --ip = \" ${ REMOTEIP } \" |& tee -a ~/tmp/jupyter/sumner/runtime.log Once a jupyter session begins and assuming you are on a secure local area network, you can open URL: https://<REMOTEIP>:<PORT>/lab to launch jupyter lab. Here, <PORT> is randomly assigned when you start a server and URL will be displayed on the terminal or in a log file at ~/tmp/jupyter/sumner/runtime.log . Run jupyterlab from a compute and not login node Avoid running JupyterLab server on a login node. It will most likely be killed by HPC admins. For longer running and compute-intensive jupyterlab sessions, it is preferable to run jupyterlab from a compute and not a login node. This requires series of secure port forwarding which is beyond the scope of current documentation. However, your HPC may already have support for running JupyterLab on a compute node, e.g, similar to this one at Univ. of Bern or Princeton Univ. . Talk to your HPC staff for policies on running JupyterLab server. Before continuing setup (not over yet!), let's logout and login first from interactive job and exit HPC. exit # from interactive session exit # from sumner ssh sumner In Part 3 , I will finalize setting up Sumner (or CPU-based) HPC and also install a dedicated conda env for Winter (GPU-based) HPC. If you like to stop here, you may except I prefer that you follow bash startup section in Part 3, so that conda and program-specific (R and python) environment variables are consistently loaded across HPC bash user env. Notice that there is no R in the base env. So, hitting R will not start R session unless you do mamba activate yoda ! \u21a9 Based on a reply from @krassowski at JupyterLab forums \u21a9","title":"Start JupyterLab"},{"location":"hpc/cpu/sumner_3/","tags":["hpc","setup","conda","startup","database","jupyter","programming"],"text":"Continuing the setup from the Part 2 , now we will finalize setup for Sumner or CPU-optimized HPC with following key configurations: Install additional tools and conda envs for Sumner (CPU-based) HPC: Julia in yoda env a new env, luke for installing sql related backend tools, and a new env, leia for running snakemake workflows. Configure Modules to load tools that either I use infrequently or require multiple dependencies that may break my stable env, yoda Finalize bash startup env using ~/.profile.d/ configuration. Let's start with a fresh terminal session: exit # from prior interactive session, if any exit # from sumner ssh sumner screen ## start interactive session srun -p compute -q batch -N 1 -n 4 --mem 10G -t 08 :00:00 --pty bash ## activate env conda activate yoda ## unload gcc if loaded module unload gcc Confirm that echo $PATH output should now have paths related to conda env prefixed but nothing else related to modules, LD_LIBRARY_PATH, etc. Also, make sure that module: gcc is unloaded and should not show up in module list output. We do not want system gcc (or any other devtools), and instead rely on conda-installed devtools when compiling R libraries or any other softwares, e.g., samtools, bcftools, etc. ## example PATH variable /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/bin:/projects/verhaak-lab/amins/hpcenv/mambaforge/condabin:/cm/shared/apps/slurm/18.08.8/sbin:/cm/shared/apps/slurm/18.08.8/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/amins/.local/bin:/home/amins/bin ## example LD_LIBRARY_PATH variable /cm/shared/apps/slurm/18.08.8/lib64/slurm:/cm/shared/apps/slurm/18.08.8/lib64 Duplicate entries in PATH and LD_LIBRARY_PATH If you are using screen , you may notice duplicate entries for one or more paths, typically for those which are loaded by the system defaults, e.g., modules and /usr/local/... . We will take care of this later when setting up our bash startup using ~/.profile.d/ configurations. Julia \u00b6 Since I use Julia alongside R and Python, I will install it under a primary env, yoda . I will prefer using a long-term support (LTS) version over the latest version. You can install julia via conda-forge channel. However, I'd trouble running conda installed julia in jupyter notebooks with kernel unable to start and connect to console 1 . Hence, I ended up installing pre-compiled version from julia downloads page and then adjusting PATH variable to setup julia command using ~/.bashrc or preferably ~/.profile.d/ configuration (explained later). Removing conda installed version of julia If you have previously installed julia in conda env, yoda and now like to remove it, you can run mamba remove -n yoda julia to remove julia and all of dependencies which are NOT shared by other conda installed packages. Before removing dependencies, it's good to check if any of removed packages (shared libraries in particular) are requirements or not for other packages by listing package dependencies . After ensuring no other conda packages require julia-related dependencies, I used mamba remove -n yoda julia libunwind to remove conda-installed julia and its dependencies. Create an empty path to store compiled packages. Later in the setup (Modules section), I will end up loading most of these packages as module load package as I use them less often. For julia, I will rather use bash startup to assign it to PATH variable and load it as a routine package. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia Install LTS version of julia: Go to julia downloads page and download 64-bit LTS version (and not one with musl which is statically linked libraries). cd /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia wget https://julialang-s3.julialang.org/bin/linux/x64/1.6/julia-1.6.4-linux-x86_64.tar.gz tar xvzf julia-1.6.4-linux-x86_64.tar.gz # This is to standardize package naming for module definitions, if any. mv julia-1.6.4 1 .6.4 Reconfigure julia base directory \u00b6 By default, julia will store packages (which takes much of space) into ~/.julia . Since my home directory is capped at 50 GB, I will reconfigure julia to use non-home path (on tier 1 space) for storing packages. Before reconfiguring, please read Environment Variable section in julia documentation and following post on stackoverflow . Why not to simply move ~/.julia/ to a new place? Perhaps the easiest solution would be to move ~/.julia/ to a new path and symlink it from there. While it looks easy, I usually avoid such hack as several softwares, including conda and some of commands of python and R rely on hardlinks over symnlinks and can throw errors. Besides, it is always good to understand how softwares set default configurations. Since pre-compiled julia is not in bash PATH, for now, we will just run following command to manually make julia available in PATH for the current terminal session . Once setup is complete, we will set julia path in PATH permanently using bash startup, so julia can load anytime we login to HPC. export PATH = \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/bin: ${ PATH } \" echo \" $PATH \" Start julia prompt. # show location of julia command -v julia # start julia prompt julia segmentation fault running julia If you have libunwind library for some other conda package(s) and if it is a version higher than 1.5.0, running julia command may throw an error saying segmentation fault. More at conda-forge/julia-feedstock#135 . In that case, you may need to tweak PATH and LD_LIBRARY_PATH such that conda installed libunwind does not take precedence when running julia. This is usually achieved using module load julia prior to running julia and thus, tweaking required PATH and LD_LIBRARY_PATH. Alternately, you may downgrade conda installed libunwind only if conda does not throw a warning. mamba install -c conda-forge libunwind=1.5.0 Once inside julia terminal, notice existing paths from an output of julia command: DEPOT_PATH 3 - element Vector { String } : \"/home/amins/.julia\" \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia\" \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\" Quit julia by Ctrl + D We will now update PATH variable and also set a new bash env variable, JULIA_DEPOT_PATH in ~/.bashrc / Once we finalize bash startup , we will move most of custom configurations from ~/.bashrc to a dedicated ~/.profile.d/ directory. Since we have not installed any julia packages, only packages that are shipped with julia are at /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia/ , specifically under base/ subdirectory. We like to keep this path unaltered and hence, we will keep it at the last in JULIA_DEPOT_PATH to assign the lowest priority to install new packages. First, make an empty package directory similar to R package directory we created earlier. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6 Now copy all of ~/.julia/ contents to this new path. Since we have not installed any new packages, we will be copying only a skeleton of directory and files to a new package path. rsync -avhP ~/.julia/ /projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/ Do not delete ~/.julia/ directory Do not delete ~/.julia/ after copying it to a new path. Julia will still use this path to store user-defined configurations, primarily under ~/.julia/config/startup.jl file. Please read Environment Variable in julia documentation if you have not read that yet! Add following line to ~/.bashrc , preferably above the # >>> conda initialize >>> line because we like to take conda setup precedence over rest of configurations during bash startup. #### custom configs #### export PATH = \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/bin: ${ PATH } \" export JULIA_DEPOT_PATH = \"/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\" # >>> conda initialize >>> Notice that we have now purposefully kept ~/.julia/ path out of default paths to store packages. Of course, julia will still use ~/.julia/ for storing user configurations among other things but those should not take up much space as installing packages do. If otherwise, we will know later and come up with a better solution! Before we start julia again to check new DEPOT_PATH , ensure that an updated bash startup has been loaded from ~/.bashrc file. Ideally, you should exit from a current session and login again same as we did in the beginning of this page. Alternately, you may run above export JULIA_... command in the current terminal to update julia env variable. Why not we do source ~/.bash_profile or source ~/.bashrc ? You may do source ~/.bash_profile but do note that this may have unwanted effects on PATH and LD_LIBRARY_PATH variables depending on how ~/.bashrc is loading bash startup env, including from /etc/bashrc . In a nutshell, better to run a single command as follows or the best is to log out and login again to have updated bash startup to take an effect. Update current terminal env with an updated depot path variable. Make sure to run subsequent julia command in the same terminal (and not in any other terminal sessions in screen you may have already opened) else julia may fall back to old depot path. export JULIA_DEPOT_PATH = \"/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\" Start julia and type DEPOT_PATH inside julia prompt. 3 - element Vector { String } : \"/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6\" \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia\" \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\" You should now see an updated DEPOT_PATH with our custom path taking the highest priority. Install julia kernel \u00b6 Similar to python and r kernel, we will install julia kernel to interact with julia from JupyterLab console. Running Jupyter notebook from julia prompt IJulia kernel package in julia also allows you to run jupyter notebook or JupyterLab from julia prompt. It does so by installing a separate conda env inside julia package directory. Since we already have installed conda env outside julia package paths, I prefer not to install conda env via julia . So, if you ever come across Running IJulia guide , please tread carefully installing two different conda env on your system! Unless you know what you are doing, avoid running commands like notebook() or jupyterlab() inside julia prompt. Run julia command under yoda env Since we have installed julia as a standalone package and path to julia is fixed in PATH variable, we can run julia independent of which conda env we are in. However, I am using yoda env as my routine env where R and other tools are installed. So, as the best practice, I will run julia after activating yoda env , including running julia as a kernel. mamba activate yoda # enter julia prompt julia Install IJulia kernel using Pkg Pkg . add ( \"IJulia\" ) Notice where julia is now installing new packages to! julia> Pkg.add(\"IJulia\") Installing known registries into `/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6` ...Resolving and installing dependency packages... Building Conda \u2500\u2192 `/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/6cdc8832ba11c7695f494c9d9a1c31e90959ce0f/build.log` Building IJulia \u2192 `/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/d8b9c31196e1dd92181cd0f5760ca2d2ffb4ac0f/build.log` Precompiling project... 11 dependencies successfully precompiled in 8 seconds (4 already precompiled) Exit julia prompt by Ctrl + D For consistency with naming kernels and ensuring that we load a valid yoda env prior to running kernel, let's adjust kernel settings. For rationale, see kernel loading section in Part 2 . Create a new kernel wrapper, /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16 mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/kernels touch /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16 # make file executable chmod 700 /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16 Add following to wrap_yoda_jl16 #!/bin/bash ## Load env before loading jupyter kernel @sbamin https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425 #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 # I am using conda instead of mamba to activate env # as somehow I notices warnings/errors sourcing # mamba.sh in sub-shells. CONDA_BASE=$(conda info --base) && \\ source \"${CONDA_BASE}\"/etc/profile.d/conda.sh && \\ conda activate yoda && \\ echo \"Env is $(basename ${CONDA_PREFIX})\" #### END CONDA SETUP #### # this is the critical part, and should be at the end of your script: exec /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/bin/julia -i --color=yes --project=@. /projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/packages/IJulia/e8kqU/src/kernel.jl \"$@\" ## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/<kernel_name>/kernel.json #_end_ Now, adjust kernel settings. ## go to kernel base dir cd ~/.local/share/jupyter/kernels/ ## rename julia kernel dir to yoda_jl16 mv julia-1.6 yoda_jl16 ## edit kernel.json to rename display name to yoda_jl16 cd yoda_jl16 nano kernel.json Replace contents of kernel.json with following: { \"argv\" : [ \"/projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16\" , \"{connection_file}\" ], \"display_name\" : \"yoda_jl16\" , \"language\" : \"julia\" , \"env\" : {}, \"interrupt_mode\" : \"signal\" } Done! Next time you run jupyter, you should have a new julia kernel in JupyterLab. Database \u00b6 This is an optional setup. If you are using database like postgresql, you may end up needing similar setup to install required postgresql drivers in conda env. Database driver(s) vary based on type of database, e.g., postgresql, mongodb, etc., and supported programming language. postgresql driver \u00b6 Create a new env, luke to host database related drivers among other backend tools. mamba create -c conda-forge -n luke psqlodbc # activate _luke_ env mamba activate luke Add database connection entries to ~/.odbc.ini and make it chmod 600 ~/.odbc.ini as it contains login credentials in plain characters! We use postgresql database in our lab and it is hosted internally on a dedicated linux node. I can programmatically (via R, python, jupyterlab kernel) connect to this database by declaring following connection definition in ~/.odbc.ini file. Notice path to database driver, psqlodbcw.so that we have just installed, and will be used to connect to a remote database. [db1] Driver = /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/luke/lib/psqlodbcw.so Database = db1 Servername = db1.example.com UserName = user1 Password = password1 Port = <db1 port> sslmode = require [db2] Driver = /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/luke/lib/psqlodbcw.so Database = db2 Servername = db2.example.com UserName = user2 Password = password2 Port = <db1 port> If database connection requires SSL (preferable), then you will need to put required SSL configuration files into ~/.postgresql or similar database-specific directory. jupyterlab-sql (deprecated) \u00b6 I installed jupyterlab-sql in base env thinking it is supported for jupyterlab v3 or higher. However, jupyterlab-sql seems to be outdated and may not function in jupyterlab v3+. You can ignore following setup for jupyterlab-sql (and jump to postgresql-kernel ) unless source wbesite confirms that it is supported in an updated jupyrterlab env. Since I already installed jupyterlab-sql, I am going to remove it from base env. Remove deprecated package. pip uninstall jupyterlab-sql pip uninstall jupyterlab-sql Found existing installation: jupyterlab-sql 0.3.3 Uninstalling jupyterlab-sql-0.3.3: Would remove: /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/python3.9/site-packages/jupyterlab_sql-0.3.3.dist-info/* /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/python3.9/site-packages/jupyterlab_sql/* Proceed (Y/n)? Y Remove a related extension. jupyter server extension disable jupyterlab_sql jupyter lab extension disable jupyterlab-sql Rebuild existing jupyter extensions. jupyter lab build # check enabled extensions jupyter lab extension list jupyter server extension list echo $? Error loading server extension If you still notice a following warning (though echo $? should be 0 for an error-free configuration of jupyterlab extensions) related to jupyterlab_sql, it should not impact running jupyterlab. This seems like an open issue with jupyterlab extension configuration. jupyter/notebook#2584 Error loading server extension jupyterlab_sql X is jupyterlab_sql importable? Before installing third-party packages, check project page for compatibility Turns out that jupyterlab-sql is no longer actively being maintained and may not be compatible with an updated jupyterlab v2+. pbugnion/jupyterlab-sql#147 Before installing third-party packages, prefer packages that are being actively maintained by looking into last commit date , release history, if any , and list of open issues . jupyterlab-sql is a GUI extension in the base env because we run jupyter from base . From jupyterlab-sql page, check setup.py requirements. ## return to base env from _yoda_ mamba deactivate ## confirm that login prompt is showing (base) [userid@sumner] ## else run mamba deactivate or mamba activate base ## check if requirements are already installed or not mamba list | grep -E \"sqlalchemy|jsonschema\" ## install required dependencies mamba install -c conda-forge sqlalchemy jsonschema pip install jupyterlab-sql Successfully built jupyterlab-sql Installing collected packages: jupyterlab-sql Successfully installed jupyterlab-sql-0.3.3 Build required jupyterlab extension for SQL jupyter server extension enable jupyterlab_sql --py --sys-prefix ## Rebuild all of jupyterlab extensions ## this may take a while (~5 minutes) jupyter lab build # check enabled extensions jupyter lab extension list jupyter server extension list Read on how-to use SQL GUI postgresql kernel \u00b6 Similar to other kernels, let's install postgresql kernel, so that jupyter notebooks can interact with postgres database. If using non-postgres system, follow driver installation based on sqlalchemy-based compatible drivers . Install postgres driver. This is not available in conda-forge channel. So, installing using pip install after ensuring that all of dependencies for the drivers are satisfied in base env and if not, prefer installing dependencies first by mamba install and then do pip install . pip install py-postgresql Successfully built py-postgresql Installing collected packages: py-postgresql Successfully installed py-postgresql-1.2.2 Install ipython-sql kernel and psycopg2 - a popular postgresql driver for python. ## install core package, sqlalchemy and related dependencies, if any. mamba install -c conda-forge sqlalchemy jsonschema mamba install -c conda-forge ipython-sql psycopg2 Since I will be using yoda env for most times, I have also installed core sql drivers into yoda env without any conflicts with an existing setup. mamba activate yoda mamba install -c conda-forge sqlalchemy ipython-sql psycopg2 mamba deactivate If applicable, restart jupyterlab server to enable SQL integration in jupyterlab env. Here is a good tutorial on using jupyter magic commands with SQL . Python 2 \u00b6 Python comes in two major flavors: Python 2 and Python 3. Since January 1, 2020, the official python developer community have stopped supporting further development and bug fixes for Python 2 . So, it's ideal to use Pythonh 3 over Python 2 unless for a few (or more!) softwares in computational biology that depend on Python 2. Since conda env is bound to Python (and the major version), let's create a separate conda env, windu that will host Python 2. Optional: Along with python 2, I will also install a software, PhyloWGS which requires python 2 as a core dependency. ## python 2.7 was the last major release. mamba create -n windu python = 2 .7 phylowgs Let's check briefly if it works ok! mamba activate windu python --version ## if you have also installed phylowgs # evolve.py --help Python 2.7.15 Deactivate and return to base env. mamba deactivate Ruby \u00b6 Tools installed under dev or beta env, anakin mamba create -c conda-forge -n anakin httpie mamba activate anakin # install ruby and github cli, https://github.com/cli/cl mamba install -c conda-forge ruby gh # additional gist utility, https://github.com/defunkt/gist gem install gist Installing gem for the first time will prepend gem bin path, /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/anakin/share/rubygems/bin to $PATH variable. Use mamba deactivate prior to switching to a new conda env When switching to an another conda env, always prefer using mamba deactivate first and then run mamba activate <other_env> instead of directly running mamba activate <other_env> . By doing deactivation first, conda will reset conda-related paths in $PATH , i.e., to remove paths from a deactivated env and fall back to the conda env (or default base env) that was active prior to a deactivated env. However, if you run mamba activate <other_env> without prior mamba deactivate , some of non-standard paths, i.e., paths other than .../envs/ /bin/ , e.g., ruby gem path above, may remain in the $PATH and even takes the precedence over paths from a switched (and now active) conda env. Such Such invalid ordering of paths in $PATH variable may create issues when you either compile softwares by inadvertently using devtools from a wrong conda env or run softwares with a dynamically linked shared libraries from a wrong conda env. Perl \u00b6 I rarely use perl language except when it is part of a software, e.g., vcf2maf . If you have configured linux env using setup detailed above, including in previous two parts, you should already have a working perl setup under both, base and yoda env with an identical version ( perl --version 5.32.1). setup PERL5LIB \u00b6 Optional: Similar to setting up version-specific custom R and julia package path, let's do the same for perl too using a bash env variable, PERL5LIB . Prefer setting up PERL5LIB at the runtime Please know that it is a better to set PERL5LIB at the run time, i.e., using module load <some program> (see Modules ) of a specific package over hardcoding it in the bash startup (as I am doing below!). Hardcoding PERL5LIB with the same perl packages but built using two different perl versions may fail to run your program. Read this post at IBM website Since we have an identical perl version in base and yoda env, we will only create a common path for both env. If you have a different perl version across conda env, you need to source custom bash startup to update PERL5LIB at the time you activate or deactivate conda env. See notes under Tips on compiling packages . Also, read more on PERL5LIB path at stackoverflow and following guide on setting up custom PERL5LIB path . Setting up PERL5LIB path HPC job schedulers, like slurm and moab may use their respective perl libraries. Typically, installing a different version of perl in your (user) conda env should not conflict with running slurm or moab commands as the latter is built using system-installed perl (at /usr/bin/ ). If it does, you may need to debug specific errors and resolve issue(s) mostly related to PERL5LIB path. First, check default perl library paths by checking tail portion of perl -V output under @INC: , e.g., for yoda env, it shows following paths: @INC: /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/site_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/5.32/vendor_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/vendor_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/5.32/core_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/core_perl Create an empty path to store user-installed perl packages. We will only need perl standard library path and one for site_perl . Paths for vendor and core library are designed for system-only perl packages. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32 mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32 Update PERL5LIB to respective major.minor perl version in case you update perl program. For now, add following to ~/.bashrc above # >>> conda initialize >>> line. Again, we will move most of custom bash startup settings to a dedicated ~/.profile.d/ path. export PERL5LIB=\"/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl\" Unlike setting up other library paths, e.g., LD_LIBRARY_PATH, LIBPATH, etc. where we need to append existing system paths (else bash env may fail to find those libraries), in PERL5LIB, we only need to specify user-level library paths and perl will pick up system path based at the run-time. Logout and login again to HPC and activate yoda env. perl -V should now show a new %ENV variable and perl library paths should now show updated paths with precedence for user-level paths over system paths. %ENV: PERL5LIB=\"/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl\" @INC: /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32 /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32 /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/5.32/site_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/site_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/5.32/vendor_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/vendor_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/5.32/core_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/core_perl Snakemake \u00b6 I use snakemake for running computational workflows. Snakemake is a well maintained python package with frequent releases for new features and bugfixes. Hence, I prefer to install it in a dedicated conda env, leia . This will allow me to update snakemake periodically without worrying about conflicting dependencies for other packages installed in yoda or other envs. Also, I can execute complex workflows using snakemake and can activate other conda envs, like yoda (CPU-otpimized) or rey (GPU-optimized) using one or more of snakemake-based rules . To install snakemake, please read installation guide # for clarity (and sanity): Better to name this env as snakemake unless # being loyal to Star Wars saga! mamba create -c conda-forge -c bioconda -n leia snakemake To update snakemake to its latest release: mamba activate snakemake mamba update -c conda-forge -c bioconda snakemake Additional Setup \u00b6 Following setup is optional. Additional packages in yoda env. ## in yoda env # depmap was not available in conda-forge # biconda version had no conflict with existing setup mamba install -c bioconda bioconductor-depmap mamba install -c conda-forge r-odbc r-dbi mamba install -c conda-forge r-ggthemes r-cowplot r-ggstatsplot r-hrbrthemes ## OpenCL support for CPU mamba install -c conda-forge pocl JupyText \u00b6 Optional: Jupytext allows running jupyter notebooks as text or markdown files similar to running scripts for R, Python, and Julia. Install jupytext in base env if if is not installed before. Note compatibility for related jupyterlab extension at mwouts/jupytext Current version of jupytext (1.13.3) is only compatible with JupyterLab 3+ (I have v3.2.4 and so all good!) mamba deactivate mamba activate base mamba install -c conda-forge jupytext After installing or updating a new jupyterlab extension, good to update and list all extensions. ## list enabled extension jupyter lab extension list jupyter server extension list ## update all extensions jupyter lab extension update --all jupyter server extension update --all jupyter lab extension list jupyter server extension list I have also installed jupyter_contrib_nbextensions extension earlier in Part 2 which allows additional configuration for jupyter notebook. This is a beta extension and an optional setup. Modules \u00b6 Similar to managing conda env using mamba activate or mamba deactivate , we can also load/unload a specific package via module load or module unload commands. Modules allows us to manage compiled packages or softwares which otherwise are either not available via conda-forge or bioconda channel or installing those with conda is creating dependencies conflict with core conda packages like R and python. Besides overcoming conflicting dependencies, Modules is a better way to organize the same software with multiple versions in case we need to occasionally use an older version for some legacy (and complex) workflow while prefer using an updated version otherwise. Here, I need to assume you have a working knowledge of using Modules. If not, no worries and here are a few tutorials on using modules in HPC env. Also, talk to your Research IT staff as module configuration may vary across HPC envs. Resources on Modules HPC at NIH Sherlock at Stanford Official documentation also provides an excellent overview of working with Modules. Your HPC staff may already have installed a few modules. To view those, run module list . Besides these default modules, you may also want to load packages that you have compiled by yourself. While buding packages, you may find tips on compiling packages useful. Once packages are built, create an empty directory to package-specific directory and respective Modulefiles - one file each for a version, e.g., mkdir -p /projects/verhaak-lab/amins/hpcenv/modules cd /projects/verhaak-lab/amins/hpcenv/modules ## subdir that will host compiled packages in version specific manner mkdir apps ## subdir that will host Modulefiles mkdir def Now, if I have compiled multiple versions of a package, e.g., samtools v1.11, v1, 14, etc. or a pre-built packages, e.g., GATK v4.1.9.0 and GATK 4.2, I can do that as follows: cd /projects/verhaak-lab/amins/hpcenv/modules ## bash syntax, {1.11,1.14} will create multiple directories ## for each entry separated by comma mdkir -p apps/samtools/ { 1 .11,1,14 } mkdir -p def/samtools/ { 1 .11,1.14 } In /def/ directory, you will need to put Modulefile as detailed in official documentation . For advanced configuration, refer to following documentation if your HPC is using Lua module system . Following are example Modulefile for samtools v1.14 Example Modulefile Tcl format Lua format #%Module1.0 ## Example format in Tcl langugae ## samtools ## Author: Samir Amin ## Read about Modulefile manpage ## https://modules.readthedocs.io/en/latest/modulefile.html ## Substitute version number and app name below # for Tcl script use only set VERSION 1.11 set MODULEDIR / projects / verhaak-lab / amins / hpcenv / modules / apps set NAME samtools ## create a new modulefile for a different version, e.g., 1.14 set INSTALL_DIR $ { MODULEDIR } / $ { NAME } / $ { VERSION } proc ModulesHelp { } { global version puts stderr \"\\nLoads ${NAME}\\n\" } module-whatis \"${NAME}\" ## check available commands in documentation append - path PATH $ { INSTALL_DIR } / bin if { [ module-info mode load ] } { puts stderr \"\\nLoaded ${NAME} ${VERSION} from ${INSTALL_DIR}\\n\" } if { [ module-info mode remove ] } { puts stderr \"\\nUnloaded ${NAME} ${VERSION}\\n\" } ## END ## --[[ ## Example format in Lua langugae ## samtools ## Author: Samir Amin ## Read about Lmod ## https://lmod.readthedocs.io/en/latest/015_writing_modules.html ## https://lmod.readthedocs.io/en/latest/050_lua_modulefiles.html ## https://lmod.readthedocs.io/en/latest/020_advanced.html --]] --################################ INTERNAL VARS ################################# --Module Name and Version are parsed by Lmod from dir/version string in module path --Make sure to have exact version numbering when naming respective -- app directroy and Modulefile local pkgName = myModuleName () local version = myModuleVersion () local pkgNameVer = myModuleFullName () local approot = \"/projects/verhaak-lab/amins/hpcenv/modules/apps\" local appbase = \"samtools\" local pkgdir = pathJoin ( approot , appbase , version ) local pkgbin = pathJoin ( pkgdir , \"bin\" ) --################################# MODULE INFO ################################## whatis ( \"Name: \" .. pkgName ) whatis ( \"Version: \" .. version ) --################################## ENV SETUP ################################### --## check available commands in documentation prepend_path ( \"PATH\" , pkgbin ) --################################# MODULE LOAD ################################## help ( \"Loads \" .. pkgNameVer .. ' \\n Check env change, if any by \\n module show ' .. pkgNameVer ) if ( mode () == \"load\" ) then LmodMessage ( \"## INFO ## \\n Loading \" .. pkgName .. version .. \"from \" .. pkgdir ) end if ( mode () == \"unload\" ) then LmodMessage ( \"## INFO ## \\n Unloading \" .. pkgName .. version .. \"from \" .. pkgdir ) end --## END ## Containers \u00b6 Containers are a greay way to enclose project-specifig set of packages and workflow(s) to promote portablity and reproducibility of the same workflow across different HPC environments. Here is a brief intro on two of most popular container systems: Docker and Singularity . HPC at NIH provides a few examples on building singularity based containers, including packages that require GPU-based configuration, e.g., Keras/Tensorflow and Theano. Your HPC may already have a container system like Singularity . If so, you can take full advantage of container system. Optional: In my bash startup, I have tweaked singularity env variables to store cache data on tier 1 space over default ~/.singularity/ path. ### singularity ### ## add manpath for singularity to an existing manpath MANPATH = \" ${ SUM7ENV } /local/share/man:/cm/local/apps/singularity/current/share/man ${ MANPATH :+: $MANPATH } \" ## set cache dir to non-home path SINGULARITY_CACHEDIR = \"/projects/verhaak-lab/amins/containers/cache/singularity\" ## path were built SIF images are manually stored SINGULARITY_SIF = \"/projects/verhaak-lab/amins/containers/sifbin\" bash special syntax: ${VAR:+:$VAR} I have expanded bash env variable like, MANPATH using syntax: ${MANPATH:+:$MANPATH} . It is a special bash syntax which will expand to existing MANPATH only if MANPATH contained any value else output of MANPATH will be an empty string without space or : , thereby perseving MANPATH structure. Tips on compiling packages \u00b6 Store compiled packages under a base directory like /projects/verhaak-lab/amins/hpcenv/opt/ or */projects/verhaak-lab/amins/hpcenv/opt/apps/ . For each of compiled packages, keep a directory structure that maintains package version, e.g., .../apps/samtools/v10.2 , .../apps/samtools/v11.3 , etc. When possible, install or compile packages using a clean terminal session, i.e., ssh to HPC, start screen, and then interactive session to do rest of installation steps. Avoid installing packages via pip install as unlike mamba install or update command, pip install does not strictly check for package dependencies of conda-installed packaegs. So, at least try installing most of package dependencies using mamba install first before running pip install . Avoid installing packages via Jupyter notebook as unlike terminal session, debugging for a failed installation is difficult with notebook and env session may vary per initialization sequence of jupyter kernel, i.e., whether or not appropriate conda env was loaded prior to starting kernel. While installing packages, prefer using mamba install/update first. If this forces you to downgrade core packages like R and python or several core shared libraries, e.g., libz, openssl, etc., you may fall back to compiling via language-specific functions, e.g., install.packages() in R, pip install for python, etc. While compiling packages, ensure that precedence of paths in PATH, LD_LIBRARY_PATH, LIBRARY_PATH , and related devtools paths are aligned to conda env AND for the respective programming env profile, e.g., if you are compiling a package with intention to use it from yoda env (say a R package), your terminal bash session should have precedence for /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/ path in PATH and if applicable, in LD_LIBRARY_PATH and LIBRARY_PATH too. The identical precedence of paths should also be present in ~/.Renviron file. Know that ~/.Renviron will be read by any of R session running from yoda or other conda env (say you have R 4.1 in yoda but R 5.1 in anakin ). Of course, Renviron varies for R 4.1 and R 5.1, and hence, you should have a dedicated Renviron file for each of conda env. You can do that by either creating a Renviron.site file in the respective env under /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/etc path or loading/unloading a custom Renviron file each time you activate/deactivate conda env using configuration files similar to ~/profile.d/ files (see bash startup below), e.g., activate.d/load_R4.1env.sh and deactivate/unload_R4.1env.sh under /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/etc/conda/ . For julia, similar startup env files should be ~/.julia/config/startup.jl and $JULIA_BINDIR / $SYSCONFDIR /julia/startup.jl . bash startup \u00b6 As I wrap up setup for our CPU-based Sumner HPC, I will finally configure bash startup - a sequence of files containing bash code that should be loaded to setup a consistent bash login environment on HPC login (interactive) and compute nodes (non-interactive). Here the consistent environment equals to order of paths in PATH and LD_LIBRARY_PATH variables especially when switching conda env and loading of custom configurations for R, Julia, and other packages, etc. that I configured earlier. First, understand what an interactive and a non-interactive bash session is. For most times, we deal with interactive session when we ssh to HPC. You are in the interactive session if env variable, PS1 (called bash custom prompt ) is set and echo $PS1 is showing some output. For more on bash startup, please read official documentation , including notes on interactive shell , especially behavior of an interactive shell . Over years of fiddling with bash startup, my startup setup has become overly complex than needed and may not suit well for your need. At minimum, I prefer to have following in bash startup sequence: Keep minimal configuration (bash code) in the user ~/.bash_profile and rather keep all of custom configuration commands under ~/.profile.d/ directory. Under user ~/.profile.d/ , organize a few bash script based on order you like your bash startup to setup, e.g., ~/.profile.d/A01.sh should load commonly used bash env variables related, e.g., EDITOR to define default text editor, TZ to set default timezone, etc. Then, ~/.profile.d/A02.sh should have additional configurations for other packages, e.g., R, Julia, etc. which otherwise we populated in ~/.bash_profile above. Keep minimal configuration in the user ~/.bashrc file as this may not be loaded with non-interactive, non-login session, e.g., submitting a job to compute nodes without passing bash env of a terminal session or certain commands in R and python using system command to execute bash command(s). Ideally, you like to keep aesthetic configurations in ~/.bashrc file like setting up terminal fonts and colors, managing bash history, etc. You may also optionally create ~/.bash_aliases to store bash short codes that you may use while doing command-line interactive work. Contrary to a default setup of conda instructions in ~/.bashrc , avoid such major env configuration in ~/.bashrc as it may not be loaded in non-interactive sessions, i.e., mamba activate or mamba deactivate may not work within a running shell script or snakemake workflows before running mamba init code. Conda setup may change in future (an active issue conda/conda#8072 ) but until then, I will move conda initialization code from ~/.bashrc to ~/.bash_profile , specifically after loading ~/.bashrc but before loading ~/.profile.d/void block (see below). bash startup sequence also includes system default configurations, e.g., setting up default modules to run a workload manager like slurm and a singularity container. These instructions are typically located under /etc/profile file and /etc/profile.d/ directory, both of these will be sourced in the very beginning. These configurations are essential for working on HPC, including submitting jobs to compute nodes. However, sometimes default modules may alter PATH and LD_LIBRARY_PATH variables such that it may conflict with your custom env setup. If so, you can reconfigure respective variables using bash command(s) in ~/.profile.d/void/VA01.sh and similar shell files. These files under void/ will be loaded after bash has initialized system default configurations, i.e., sourced /etc/profile file and shell files under /etc/profile.d/ directory, and therefore, any of bash commands within ~/.profile.d/void/VA01.sh file will override the respective configuration set earlier. I emphasized reconfigure and not resetting earlier because you should be very careful of resetting PATH and LD_LIBRARY_PATH. You may get locked out of HPC login node and may need a help of sysadmin to let you in again! PS: I do reset PATH at the very end of ~/.bash_profile once bash startup sequence has traversed across all of files detailed above and shown in flow diagram below. By doing so, I can get a consistent bash login environment. However, this is an overkill and you can sure get a stable bash env without such reset. Order of precedence matters Be aware of how bash startup sequence will load these files. Check a flow diagram under My bash startup sequence below. For example, if I set PATH variable as export PATH=/a/b/c:/p/q/r in ~/.profile.d/A01.sh and then I add export PATH=/p/q/r:/a/b/c in ~/.profile.d/A02.sh , bash startup sequqence will overwrite previous PATH with an updated PATH from A02.sh file. My bash startup sequence: flow diagram Following diagram represents sequence of files (containing bash code) that gets sourced each time I login to HPC. If plot is not visible below, you can view plot by pasting following code to mermaid live editor . graph TB A[System <br>/etc/profile] --> B[User<br>.bash_profile] B --> C{User .profile.d/ directory}; C -->|Yes| D[source .profile.d/A01.sh]; D -.-> E[source .profile.d/A02.sh]; E -.-> F[source .profile.d/Z99.sh]; F --> G; C --> |No| G{User .bashrc}; G --> |Interactive shell<br>PS1 var is set| H[source /etc/bashrc] H --> I[User .bash_aliases] I --> J[bash terminal config, <br> e.g., colors, history, etc.] J --> K[Initialize<br>Conda Environment] G --> |Non-interactive shell<br>PS1 var is unset| K L[User .profile.d/void] --> M[source <br> .profile.d/void/VA01.sh]; M -.-> N[source <br> .profile.d/void/VZ99.sh]; K --> L; N --> O[Set PATH] O --> P[Set PS1] Example bash startup files Fix link - You can download my bash startup files . It will not work by cloning into your linux env. However, each file has inline comments that should help customizing your bash startup. And that's all! My setup for our CPU-based Sumner HPC or for that matter, a generic linux-based machine is now complete. Since our HPC env at JAX shares the common home directory and base linux image (CentOS 7) between CPU (Sumner) and GPU (Winter) based HPC, the above setup will work off-the-shelf on the Winter HPC too except for tasks which require GPU-based computing, e.g., using GPU-based tensorflow and pytorch libraries. For the latter, I will setup a dedicated GPU-based conda env, rey (and ben and gorgu !) and tweak bash startup such that I get GPU-based bash env only when I login to Winter and to Sumner HPC. For GPU-based setup, go to Part 4 . Related discussions on julia forums and troubleshooting guide . \u21a9","title":"Part 3"},{"location":"hpc/cpu/sumner_3/#julia","text":"Since I use Julia alongside R and Python, I will install it under a primary env, yoda . I will prefer using a long-term support (LTS) version over the latest version. You can install julia via conda-forge channel. However, I'd trouble running conda installed julia in jupyter notebooks with kernel unable to start and connect to console 1 . Hence, I ended up installing pre-compiled version from julia downloads page and then adjusting PATH variable to setup julia command using ~/.bashrc or preferably ~/.profile.d/ configuration (explained later). Removing conda installed version of julia If you have previously installed julia in conda env, yoda and now like to remove it, you can run mamba remove -n yoda julia to remove julia and all of dependencies which are NOT shared by other conda installed packages. Before removing dependencies, it's good to check if any of removed packages (shared libraries in particular) are requirements or not for other packages by listing package dependencies . After ensuring no other conda packages require julia-related dependencies, I used mamba remove -n yoda julia libunwind to remove conda-installed julia and its dependencies. Create an empty path to store compiled packages. Later in the setup (Modules section), I will end up loading most of these packages as module load package as I use them less often. For julia, I will rather use bash startup to assign it to PATH variable and load it as a routine package. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia Install LTS version of julia: Go to julia downloads page and download 64-bit LTS version (and not one with musl which is statically linked libraries). cd /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia wget https://julialang-s3.julialang.org/bin/linux/x64/1.6/julia-1.6.4-linux-x86_64.tar.gz tar xvzf julia-1.6.4-linux-x86_64.tar.gz # This is to standardize package naming for module definitions, if any. mv julia-1.6.4 1 .6.4","title":"Julia"},{"location":"hpc/cpu/sumner_3/#reconfigure-julia-base-directory","text":"By default, julia will store packages (which takes much of space) into ~/.julia . Since my home directory is capped at 50 GB, I will reconfigure julia to use non-home path (on tier 1 space) for storing packages. Before reconfiguring, please read Environment Variable section in julia documentation and following post on stackoverflow . Why not to simply move ~/.julia/ to a new place? Perhaps the easiest solution would be to move ~/.julia/ to a new path and symlink it from there. While it looks easy, I usually avoid such hack as several softwares, including conda and some of commands of python and R rely on hardlinks over symnlinks and can throw errors. Besides, it is always good to understand how softwares set default configurations. Since pre-compiled julia is not in bash PATH, for now, we will just run following command to manually make julia available in PATH for the current terminal session . Once setup is complete, we will set julia path in PATH permanently using bash startup, so julia can load anytime we login to HPC. export PATH = \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/bin: ${ PATH } \" echo \" $PATH \" Start julia prompt. # show location of julia command -v julia # start julia prompt julia segmentation fault running julia If you have libunwind library for some other conda package(s) and if it is a version higher than 1.5.0, running julia command may throw an error saying segmentation fault. More at conda-forge/julia-feedstock#135 . In that case, you may need to tweak PATH and LD_LIBRARY_PATH such that conda installed libunwind does not take precedence when running julia. This is usually achieved using module load julia prior to running julia and thus, tweaking required PATH and LD_LIBRARY_PATH. Alternately, you may downgrade conda installed libunwind only if conda does not throw a warning. mamba install -c conda-forge libunwind=1.5.0 Once inside julia terminal, notice existing paths from an output of julia command: DEPOT_PATH 3 - element Vector { String } : \"/home/amins/.julia\" \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia\" \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\" Quit julia by Ctrl + D We will now update PATH variable and also set a new bash env variable, JULIA_DEPOT_PATH in ~/.bashrc / Once we finalize bash startup , we will move most of custom configurations from ~/.bashrc to a dedicated ~/.profile.d/ directory. Since we have not installed any julia packages, only packages that are shipped with julia are at /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia/ , specifically under base/ subdirectory. We like to keep this path unaltered and hence, we will keep it at the last in JULIA_DEPOT_PATH to assign the lowest priority to install new packages. First, make an empty package directory similar to R package directory we created earlier. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6 Now copy all of ~/.julia/ contents to this new path. Since we have not installed any new packages, we will be copying only a skeleton of directory and files to a new package path. rsync -avhP ~/.julia/ /projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/ Do not delete ~/.julia/ directory Do not delete ~/.julia/ after copying it to a new path. Julia will still use this path to store user-defined configurations, primarily under ~/.julia/config/startup.jl file. Please read Environment Variable in julia documentation if you have not read that yet! Add following line to ~/.bashrc , preferably above the # >>> conda initialize >>> line because we like to take conda setup precedence over rest of configurations during bash startup. #### custom configs #### export PATH = \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/bin: ${ PATH } \" export JULIA_DEPOT_PATH = \"/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\" # >>> conda initialize >>> Notice that we have now purposefully kept ~/.julia/ path out of default paths to store packages. Of course, julia will still use ~/.julia/ for storing user configurations among other things but those should not take up much space as installing packages do. If otherwise, we will know later and come up with a better solution! Before we start julia again to check new DEPOT_PATH , ensure that an updated bash startup has been loaded from ~/.bashrc file. Ideally, you should exit from a current session and login again same as we did in the beginning of this page. Alternately, you may run above export JULIA_... command in the current terminal to update julia env variable. Why not we do source ~/.bash_profile or source ~/.bashrc ? You may do source ~/.bash_profile but do note that this may have unwanted effects on PATH and LD_LIBRARY_PATH variables depending on how ~/.bashrc is loading bash startup env, including from /etc/bashrc . In a nutshell, better to run a single command as follows or the best is to log out and login again to have updated bash startup to take an effect. Update current terminal env with an updated depot path variable. Make sure to run subsequent julia command in the same terminal (and not in any other terminal sessions in screen you may have already opened) else julia may fall back to old depot path. export JULIA_DEPOT_PATH = \"/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia:/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\" Start julia and type DEPOT_PATH inside julia prompt. 3 - element Vector { String } : \"/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6\" \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/local/share/julia\" \"/projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/share/julia\" You should now see an updated DEPOT_PATH with our custom path taking the highest priority.","title":"Reconfigure julia base directory"},{"location":"hpc/cpu/sumner_3/#install-julia-kernel","text":"Similar to python and r kernel, we will install julia kernel to interact with julia from JupyterLab console. Running Jupyter notebook from julia prompt IJulia kernel package in julia also allows you to run jupyter notebook or JupyterLab from julia prompt. It does so by installing a separate conda env inside julia package directory. Since we already have installed conda env outside julia package paths, I prefer not to install conda env via julia . So, if you ever come across Running IJulia guide , please tread carefully installing two different conda env on your system! Unless you know what you are doing, avoid running commands like notebook() or jupyterlab() inside julia prompt. Run julia command under yoda env Since we have installed julia as a standalone package and path to julia is fixed in PATH variable, we can run julia independent of which conda env we are in. However, I am using yoda env as my routine env where R and other tools are installed. So, as the best practice, I will run julia after activating yoda env , including running julia as a kernel. mamba activate yoda # enter julia prompt julia Install IJulia kernel using Pkg Pkg . add ( \"IJulia\" ) Notice where julia is now installing new packages to! julia> Pkg.add(\"IJulia\") Installing known registries into `/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6` ...Resolving and installing dependency packages... Building Conda \u2500\u2192 `/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/6cdc8832ba11c7695f494c9d9a1c31e90959ce0f/build.log` Building IJulia \u2192 `/projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/d8b9c31196e1dd92181cd0f5760ca2d2ffb4ac0f/build.log` Precompiling project... 11 dependencies successfully precompiled in 8 seconds (4 already precompiled) Exit julia prompt by Ctrl + D For consistency with naming kernels and ensuring that we load a valid yoda env prior to running kernel, let's adjust kernel settings. For rationale, see kernel loading section in Part 2 . Create a new kernel wrapper, /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16 mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/kernels touch /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16 # make file executable chmod 700 /projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16 Add following to wrap_yoda_jl16 #!/bin/bash ## Load env before loading jupyter kernel @sbamin https://github.com/jupyterhub/jupyterhub/issues/847#issuecomment-260152425 #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 # I am using conda instead of mamba to activate env # as somehow I notices warnings/errors sourcing # mamba.sh in sub-shells. CONDA_BASE=$(conda info --base) && \\ source \"${CONDA_BASE}\"/etc/profile.d/conda.sh && \\ conda activate yoda && \\ echo \"Env is $(basename ${CONDA_PREFIX})\" #### END CONDA SETUP #### # this is the critical part, and should be at the end of your script: exec /projects/verhaak-lab/amins/hpcenv/opt/modules/apps/julia/1.6.4/bin/julia -i --color=yes --project=@. /projects/verhaak-lab/amins/hpcenv/opt/julia/pkgs/1.6/packages/IJulia/e8kqU/src/kernel.jl \"$@\" ## Make sure to update corresponding kernel.json under ~/.local/share/jupyter/kernels/<kernel_name>/kernel.json #_end_ Now, adjust kernel settings. ## go to kernel base dir cd ~/.local/share/jupyter/kernels/ ## rename julia kernel dir to yoda_jl16 mv julia-1.6 yoda_jl16 ## edit kernel.json to rename display name to yoda_jl16 cd yoda_jl16 nano kernel.json Replace contents of kernel.json with following: { \"argv\" : [ \"/projects/verhaak-lab/amins/hpcenv/opt/kernels/wrap_yoda_jl16\" , \"{connection_file}\" ], \"display_name\" : \"yoda_jl16\" , \"language\" : \"julia\" , \"env\" : {}, \"interrupt_mode\" : \"signal\" } Done! Next time you run jupyter, you should have a new julia kernel in JupyterLab.","title":"Install julia kernel"},{"location":"hpc/cpu/sumner_3/#database","text":"This is an optional setup. If you are using database like postgresql, you may end up needing similar setup to install required postgresql drivers in conda env. Database driver(s) vary based on type of database, e.g., postgresql, mongodb, etc., and supported programming language.","title":"Database"},{"location":"hpc/cpu/sumner_3/#postgresql-driver","text":"Create a new env, luke to host database related drivers among other backend tools. mamba create -c conda-forge -n luke psqlodbc # activate _luke_ env mamba activate luke Add database connection entries to ~/.odbc.ini and make it chmod 600 ~/.odbc.ini as it contains login credentials in plain characters! We use postgresql database in our lab and it is hosted internally on a dedicated linux node. I can programmatically (via R, python, jupyterlab kernel) connect to this database by declaring following connection definition in ~/.odbc.ini file. Notice path to database driver, psqlodbcw.so that we have just installed, and will be used to connect to a remote database. [db1] Driver = /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/luke/lib/psqlodbcw.so Database = db1 Servername = db1.example.com UserName = user1 Password = password1 Port = <db1 port> sslmode = require [db2] Driver = /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/luke/lib/psqlodbcw.so Database = db2 Servername = db2.example.com UserName = user2 Password = password2 Port = <db1 port> If database connection requires SSL (preferable), then you will need to put required SSL configuration files into ~/.postgresql or similar database-specific directory.","title":"postgresql driver"},{"location":"hpc/cpu/sumner_3/#jupyterlab-sql-deprecated","text":"I installed jupyterlab-sql in base env thinking it is supported for jupyterlab v3 or higher. However, jupyterlab-sql seems to be outdated and may not function in jupyterlab v3+. You can ignore following setup for jupyterlab-sql (and jump to postgresql-kernel ) unless source wbesite confirms that it is supported in an updated jupyrterlab env. Since I already installed jupyterlab-sql, I am going to remove it from base env. Remove deprecated package. pip uninstall jupyterlab-sql pip uninstall jupyterlab-sql Found existing installation: jupyterlab-sql 0.3.3 Uninstalling jupyterlab-sql-0.3.3: Would remove: /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/python3.9/site-packages/jupyterlab_sql-0.3.3.dist-info/* /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/python3.9/site-packages/jupyterlab_sql/* Proceed (Y/n)? Y Remove a related extension. jupyter server extension disable jupyterlab_sql jupyter lab extension disable jupyterlab-sql Rebuild existing jupyter extensions. jupyter lab build # check enabled extensions jupyter lab extension list jupyter server extension list echo $? Error loading server extension If you still notice a following warning (though echo $? should be 0 for an error-free configuration of jupyterlab extensions) related to jupyterlab_sql, it should not impact running jupyterlab. This seems like an open issue with jupyterlab extension configuration. jupyter/notebook#2584 Error loading server extension jupyterlab_sql X is jupyterlab_sql importable? Before installing third-party packages, check project page for compatibility Turns out that jupyterlab-sql is no longer actively being maintained and may not be compatible with an updated jupyterlab v2+. pbugnion/jupyterlab-sql#147 Before installing third-party packages, prefer packages that are being actively maintained by looking into last commit date , release history, if any , and list of open issues . jupyterlab-sql is a GUI extension in the base env because we run jupyter from base . From jupyterlab-sql page, check setup.py requirements. ## return to base env from _yoda_ mamba deactivate ## confirm that login prompt is showing (base) [userid@sumner] ## else run mamba deactivate or mamba activate base ## check if requirements are already installed or not mamba list | grep -E \"sqlalchemy|jsonschema\" ## install required dependencies mamba install -c conda-forge sqlalchemy jsonschema pip install jupyterlab-sql Successfully built jupyterlab-sql Installing collected packages: jupyterlab-sql Successfully installed jupyterlab-sql-0.3.3 Build required jupyterlab extension for SQL jupyter server extension enable jupyterlab_sql --py --sys-prefix ## Rebuild all of jupyterlab extensions ## this may take a while (~5 minutes) jupyter lab build # check enabled extensions jupyter lab extension list jupyter server extension list Read on how-to use SQL GUI","title":"jupyterlab-sql (deprecated)"},{"location":"hpc/cpu/sumner_3/#postgresql-kernel","text":"Similar to other kernels, let's install postgresql kernel, so that jupyter notebooks can interact with postgres database. If using non-postgres system, follow driver installation based on sqlalchemy-based compatible drivers . Install postgres driver. This is not available in conda-forge channel. So, installing using pip install after ensuring that all of dependencies for the drivers are satisfied in base env and if not, prefer installing dependencies first by mamba install and then do pip install . pip install py-postgresql Successfully built py-postgresql Installing collected packages: py-postgresql Successfully installed py-postgresql-1.2.2 Install ipython-sql kernel and psycopg2 - a popular postgresql driver for python. ## install core package, sqlalchemy and related dependencies, if any. mamba install -c conda-forge sqlalchemy jsonschema mamba install -c conda-forge ipython-sql psycopg2 Since I will be using yoda env for most times, I have also installed core sql drivers into yoda env without any conflicts with an existing setup. mamba activate yoda mamba install -c conda-forge sqlalchemy ipython-sql psycopg2 mamba deactivate If applicable, restart jupyterlab server to enable SQL integration in jupyterlab env. Here is a good tutorial on using jupyter magic commands with SQL .","title":"postgresql kernel"},{"location":"hpc/cpu/sumner_3/#python-2","text":"Python comes in two major flavors: Python 2 and Python 3. Since January 1, 2020, the official python developer community have stopped supporting further development and bug fixes for Python 2 . So, it's ideal to use Pythonh 3 over Python 2 unless for a few (or more!) softwares in computational biology that depend on Python 2. Since conda env is bound to Python (and the major version), let's create a separate conda env, windu that will host Python 2. Optional: Along with python 2, I will also install a software, PhyloWGS which requires python 2 as a core dependency. ## python 2.7 was the last major release. mamba create -n windu python = 2 .7 phylowgs Let's check briefly if it works ok! mamba activate windu python --version ## if you have also installed phylowgs # evolve.py --help Python 2.7.15 Deactivate and return to base env. mamba deactivate","title":"Python 2"},{"location":"hpc/cpu/sumner_3/#ruby","text":"Tools installed under dev or beta env, anakin mamba create -c conda-forge -n anakin httpie mamba activate anakin # install ruby and github cli, https://github.com/cli/cl mamba install -c conda-forge ruby gh # additional gist utility, https://github.com/defunkt/gist gem install gist Installing gem for the first time will prepend gem bin path, /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/anakin/share/rubygems/bin to $PATH variable. Use mamba deactivate prior to switching to a new conda env When switching to an another conda env, always prefer using mamba deactivate first and then run mamba activate <other_env> instead of directly running mamba activate <other_env> . By doing deactivation first, conda will reset conda-related paths in $PATH , i.e., to remove paths from a deactivated env and fall back to the conda env (or default base env) that was active prior to a deactivated env. However, if you run mamba activate <other_env> without prior mamba deactivate , some of non-standard paths, i.e., paths other than .../envs/ /bin/ , e.g., ruby gem path above, may remain in the $PATH and even takes the precedence over paths from a switched (and now active) conda env. Such Such invalid ordering of paths in $PATH variable may create issues when you either compile softwares by inadvertently using devtools from a wrong conda env or run softwares with a dynamically linked shared libraries from a wrong conda env.","title":"Ruby"},{"location":"hpc/cpu/sumner_3/#perl","text":"I rarely use perl language except when it is part of a software, e.g., vcf2maf . If you have configured linux env using setup detailed above, including in previous two parts, you should already have a working perl setup under both, base and yoda env with an identical version ( perl --version 5.32.1).","title":"Perl"},{"location":"hpc/cpu/sumner_3/#setup-perl5lib","text":"Optional: Similar to setting up version-specific custom R and julia package path, let's do the same for perl too using a bash env variable, PERL5LIB . Prefer setting up PERL5LIB at the runtime Please know that it is a better to set PERL5LIB at the run time, i.e., using module load <some program> (see Modules ) of a specific package over hardcoding it in the bash startup (as I am doing below!). Hardcoding PERL5LIB with the same perl packages but built using two different perl versions may fail to run your program. Read this post at IBM website Since we have an identical perl version in base and yoda env, we will only create a common path for both env. If you have a different perl version across conda env, you need to source custom bash startup to update PERL5LIB at the time you activate or deactivate conda env. See notes under Tips on compiling packages . Also, read more on PERL5LIB path at stackoverflow and following guide on setting up custom PERL5LIB path . Setting up PERL5LIB path HPC job schedulers, like slurm and moab may use their respective perl libraries. Typically, installing a different version of perl in your (user) conda env should not conflict with running slurm or moab commands as the latter is built using system-installed perl (at /usr/bin/ ). If it does, you may need to debug specific errors and resolve issue(s) mostly related to PERL5LIB path. First, check default perl library paths by checking tail portion of perl -V output under @INC: , e.g., for yoda env, it shows following paths: @INC: /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/site_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/5.32/vendor_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/vendor_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/5.32/core_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/perl5/core_perl Create an empty path to store user-installed perl packages. We will only need perl standard library path and one for site_perl . Paths for vendor and core library are designed for system-only perl packages. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32 mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32 Update PERL5LIB to respective major.minor perl version in case you update perl program. For now, add following to ~/.bashrc above # >>> conda initialize >>> line. Again, we will move most of custom bash startup settings to a dedicated ~/.profile.d/ path. export PERL5LIB=\"/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl\" Unlike setting up other library paths, e.g., LD_LIBRARY_PATH, LIBPATH, etc. where we need to append existing system paths (else bash env may fail to find those libraries), in PERL5LIB, we only need to specify user-level library paths and perl will pick up system path based at the run-time. Logout and login again to HPC and activate yoda env. perl -V should now show a new %ENV variable and perl library paths should now show updated paths with precedence for user-level paths over system paths. %ENV: PERL5LIB=\"/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32:/projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl\" @INC: /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/5.32 /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl/5.32 /projects/verhaak-lab/amins/hpcenv/opt/perl/pkgs/perl5/site_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/5.32/site_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/site_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/5.32/vendor_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/vendor_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/5.32/core_perl /projects/verhaak-lab/amins/hpcenv/mambaforge/lib/perl5/core_perl","title":"setup PERL5LIB"},{"location":"hpc/cpu/sumner_3/#snakemake","text":"I use snakemake for running computational workflows. Snakemake is a well maintained python package with frequent releases for new features and bugfixes. Hence, I prefer to install it in a dedicated conda env, leia . This will allow me to update snakemake periodically without worrying about conflicting dependencies for other packages installed in yoda or other envs. Also, I can execute complex workflows using snakemake and can activate other conda envs, like yoda (CPU-otpimized) or rey (GPU-optimized) using one or more of snakemake-based rules . To install snakemake, please read installation guide # for clarity (and sanity): Better to name this env as snakemake unless # being loyal to Star Wars saga! mamba create -c conda-forge -c bioconda -n leia snakemake To update snakemake to its latest release: mamba activate snakemake mamba update -c conda-forge -c bioconda snakemake","title":"Snakemake"},{"location":"hpc/cpu/sumner_3/#additional-setup","text":"Following setup is optional. Additional packages in yoda env. ## in yoda env # depmap was not available in conda-forge # biconda version had no conflict with existing setup mamba install -c bioconda bioconductor-depmap mamba install -c conda-forge r-odbc r-dbi mamba install -c conda-forge r-ggthemes r-cowplot r-ggstatsplot r-hrbrthemes ## OpenCL support for CPU mamba install -c conda-forge pocl","title":"Additional Setup"},{"location":"hpc/cpu/sumner_3/#jupytext","text":"Optional: Jupytext allows running jupyter notebooks as text or markdown files similar to running scripts for R, Python, and Julia. Install jupytext in base env if if is not installed before. Note compatibility for related jupyterlab extension at mwouts/jupytext Current version of jupytext (1.13.3) is only compatible with JupyterLab 3+ (I have v3.2.4 and so all good!) mamba deactivate mamba activate base mamba install -c conda-forge jupytext After installing or updating a new jupyterlab extension, good to update and list all extensions. ## list enabled extension jupyter lab extension list jupyter server extension list ## update all extensions jupyter lab extension update --all jupyter server extension update --all jupyter lab extension list jupyter server extension list I have also installed jupyter_contrib_nbextensions extension earlier in Part 2 which allows additional configuration for jupyter notebook. This is a beta extension and an optional setup.","title":"JupyText"},{"location":"hpc/cpu/sumner_3/#modules","text":"Similar to managing conda env using mamba activate or mamba deactivate , we can also load/unload a specific package via module load or module unload commands. Modules allows us to manage compiled packages or softwares which otherwise are either not available via conda-forge or bioconda channel or installing those with conda is creating dependencies conflict with core conda packages like R and python. Besides overcoming conflicting dependencies, Modules is a better way to organize the same software with multiple versions in case we need to occasionally use an older version for some legacy (and complex) workflow while prefer using an updated version otherwise. Here, I need to assume you have a working knowledge of using Modules. If not, no worries and here are a few tutorials on using modules in HPC env. Also, talk to your Research IT staff as module configuration may vary across HPC envs. Resources on Modules HPC at NIH Sherlock at Stanford Official documentation also provides an excellent overview of working with Modules. Your HPC staff may already have installed a few modules. To view those, run module list . Besides these default modules, you may also want to load packages that you have compiled by yourself. While buding packages, you may find tips on compiling packages useful. Once packages are built, create an empty directory to package-specific directory and respective Modulefiles - one file each for a version, e.g., mkdir -p /projects/verhaak-lab/amins/hpcenv/modules cd /projects/verhaak-lab/amins/hpcenv/modules ## subdir that will host compiled packages in version specific manner mkdir apps ## subdir that will host Modulefiles mkdir def Now, if I have compiled multiple versions of a package, e.g., samtools v1.11, v1, 14, etc. or a pre-built packages, e.g., GATK v4.1.9.0 and GATK 4.2, I can do that as follows: cd /projects/verhaak-lab/amins/hpcenv/modules ## bash syntax, {1.11,1.14} will create multiple directories ## for each entry separated by comma mdkir -p apps/samtools/ { 1 .11,1,14 } mkdir -p def/samtools/ { 1 .11,1.14 } In /def/ directory, you will need to put Modulefile as detailed in official documentation . For advanced configuration, refer to following documentation if your HPC is using Lua module system . Following are example Modulefile for samtools v1.14 Example Modulefile Tcl format Lua format #%Module1.0 ## Example format in Tcl langugae ## samtools ## Author: Samir Amin ## Read about Modulefile manpage ## https://modules.readthedocs.io/en/latest/modulefile.html ## Substitute version number and app name below # for Tcl script use only set VERSION 1.11 set MODULEDIR / projects / verhaak-lab / amins / hpcenv / modules / apps set NAME samtools ## create a new modulefile for a different version, e.g., 1.14 set INSTALL_DIR $ { MODULEDIR } / $ { NAME } / $ { VERSION } proc ModulesHelp { } { global version puts stderr \"\\nLoads ${NAME}\\n\" } module-whatis \"${NAME}\" ## check available commands in documentation append - path PATH $ { INSTALL_DIR } / bin if { [ module-info mode load ] } { puts stderr \"\\nLoaded ${NAME} ${VERSION} from ${INSTALL_DIR}\\n\" } if { [ module-info mode remove ] } { puts stderr \"\\nUnloaded ${NAME} ${VERSION}\\n\" } ## END ## --[[ ## Example format in Lua langugae ## samtools ## Author: Samir Amin ## Read about Lmod ## https://lmod.readthedocs.io/en/latest/015_writing_modules.html ## https://lmod.readthedocs.io/en/latest/050_lua_modulefiles.html ## https://lmod.readthedocs.io/en/latest/020_advanced.html --]] --################################ INTERNAL VARS ################################# --Module Name and Version are parsed by Lmod from dir/version string in module path --Make sure to have exact version numbering when naming respective -- app directroy and Modulefile local pkgName = myModuleName () local version = myModuleVersion () local pkgNameVer = myModuleFullName () local approot = \"/projects/verhaak-lab/amins/hpcenv/modules/apps\" local appbase = \"samtools\" local pkgdir = pathJoin ( approot , appbase , version ) local pkgbin = pathJoin ( pkgdir , \"bin\" ) --################################# MODULE INFO ################################## whatis ( \"Name: \" .. pkgName ) whatis ( \"Version: \" .. version ) --################################## ENV SETUP ################################### --## check available commands in documentation prepend_path ( \"PATH\" , pkgbin ) --################################# MODULE LOAD ################################## help ( \"Loads \" .. pkgNameVer .. ' \\n Check env change, if any by \\n module show ' .. pkgNameVer ) if ( mode () == \"load\" ) then LmodMessage ( \"## INFO ## \\n Loading \" .. pkgName .. version .. \"from \" .. pkgdir ) end if ( mode () == \"unload\" ) then LmodMessage ( \"## INFO ## \\n Unloading \" .. pkgName .. version .. \"from \" .. pkgdir ) end --## END ##","title":"Modules"},{"location":"hpc/cpu/sumner_3/#containers","text":"Containers are a greay way to enclose project-specifig set of packages and workflow(s) to promote portablity and reproducibility of the same workflow across different HPC environments. Here is a brief intro on two of most popular container systems: Docker and Singularity . HPC at NIH provides a few examples on building singularity based containers, including packages that require GPU-based configuration, e.g., Keras/Tensorflow and Theano. Your HPC may already have a container system like Singularity . If so, you can take full advantage of container system. Optional: In my bash startup, I have tweaked singularity env variables to store cache data on tier 1 space over default ~/.singularity/ path. ### singularity ### ## add manpath for singularity to an existing manpath MANPATH = \" ${ SUM7ENV } /local/share/man:/cm/local/apps/singularity/current/share/man ${ MANPATH :+: $MANPATH } \" ## set cache dir to non-home path SINGULARITY_CACHEDIR = \"/projects/verhaak-lab/amins/containers/cache/singularity\" ## path were built SIF images are manually stored SINGULARITY_SIF = \"/projects/verhaak-lab/amins/containers/sifbin\" bash special syntax: ${VAR:+:$VAR} I have expanded bash env variable like, MANPATH using syntax: ${MANPATH:+:$MANPATH} . It is a special bash syntax which will expand to existing MANPATH only if MANPATH contained any value else output of MANPATH will be an empty string without space or : , thereby perseving MANPATH structure.","title":"Containers"},{"location":"hpc/cpu/sumner_3/#tips-on-compiling-packages","text":"Store compiled packages under a base directory like /projects/verhaak-lab/amins/hpcenv/opt/ or */projects/verhaak-lab/amins/hpcenv/opt/apps/ . For each of compiled packages, keep a directory structure that maintains package version, e.g., .../apps/samtools/v10.2 , .../apps/samtools/v11.3 , etc. When possible, install or compile packages using a clean terminal session, i.e., ssh to HPC, start screen, and then interactive session to do rest of installation steps. Avoid installing packages via pip install as unlike mamba install or update command, pip install does not strictly check for package dependencies of conda-installed packaegs. So, at least try installing most of package dependencies using mamba install first before running pip install . Avoid installing packages via Jupyter notebook as unlike terminal session, debugging for a failed installation is difficult with notebook and env session may vary per initialization sequence of jupyter kernel, i.e., whether or not appropriate conda env was loaded prior to starting kernel. While installing packages, prefer using mamba install/update first. If this forces you to downgrade core packages like R and python or several core shared libraries, e.g., libz, openssl, etc., you may fall back to compiling via language-specific functions, e.g., install.packages() in R, pip install for python, etc. While compiling packages, ensure that precedence of paths in PATH, LD_LIBRARY_PATH, LIBRARY_PATH , and related devtools paths are aligned to conda env AND for the respective programming env profile, e.g., if you are compiling a package with intention to use it from yoda env (say a R package), your terminal bash session should have precedence for /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/ path in PATH and if applicable, in LD_LIBRARY_PATH and LIBRARY_PATH too. The identical precedence of paths should also be present in ~/.Renviron file. Know that ~/.Renviron will be read by any of R session running from yoda or other conda env (say you have R 4.1 in yoda but R 5.1 in anakin ). Of course, Renviron varies for R 4.1 and R 5.1, and hence, you should have a dedicated Renviron file for each of conda env. You can do that by either creating a Renviron.site file in the respective env under /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/etc path or loading/unloading a custom Renviron file each time you activate/deactivate conda env using configuration files similar to ~/profile.d/ files (see bash startup below), e.g., activate.d/load_R4.1env.sh and deactivate/unload_R4.1env.sh under /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/etc/conda/ . For julia, similar startup env files should be ~/.julia/config/startup.jl and $JULIA_BINDIR / $SYSCONFDIR /julia/startup.jl .","title":"Tips on compiling packages"},{"location":"hpc/cpu/sumner_3/#bash-startup","text":"As I wrap up setup for our CPU-based Sumner HPC, I will finally configure bash startup - a sequence of files containing bash code that should be loaded to setup a consistent bash login environment on HPC login (interactive) and compute nodes (non-interactive). Here the consistent environment equals to order of paths in PATH and LD_LIBRARY_PATH variables especially when switching conda env and loading of custom configurations for R, Julia, and other packages, etc. that I configured earlier. First, understand what an interactive and a non-interactive bash session is. For most times, we deal with interactive session when we ssh to HPC. You are in the interactive session if env variable, PS1 (called bash custom prompt ) is set and echo $PS1 is showing some output. For more on bash startup, please read official documentation , including notes on interactive shell , especially behavior of an interactive shell . Over years of fiddling with bash startup, my startup setup has become overly complex than needed and may not suit well for your need. At minimum, I prefer to have following in bash startup sequence: Keep minimal configuration (bash code) in the user ~/.bash_profile and rather keep all of custom configuration commands under ~/.profile.d/ directory. Under user ~/.profile.d/ , organize a few bash script based on order you like your bash startup to setup, e.g., ~/.profile.d/A01.sh should load commonly used bash env variables related, e.g., EDITOR to define default text editor, TZ to set default timezone, etc. Then, ~/.profile.d/A02.sh should have additional configurations for other packages, e.g., R, Julia, etc. which otherwise we populated in ~/.bash_profile above. Keep minimal configuration in the user ~/.bashrc file as this may not be loaded with non-interactive, non-login session, e.g., submitting a job to compute nodes without passing bash env of a terminal session or certain commands in R and python using system command to execute bash command(s). Ideally, you like to keep aesthetic configurations in ~/.bashrc file like setting up terminal fonts and colors, managing bash history, etc. You may also optionally create ~/.bash_aliases to store bash short codes that you may use while doing command-line interactive work. Contrary to a default setup of conda instructions in ~/.bashrc , avoid such major env configuration in ~/.bashrc as it may not be loaded in non-interactive sessions, i.e., mamba activate or mamba deactivate may not work within a running shell script or snakemake workflows before running mamba init code. Conda setup may change in future (an active issue conda/conda#8072 ) but until then, I will move conda initialization code from ~/.bashrc to ~/.bash_profile , specifically after loading ~/.bashrc but before loading ~/.profile.d/void block (see below). bash startup sequence also includes system default configurations, e.g., setting up default modules to run a workload manager like slurm and a singularity container. These instructions are typically located under /etc/profile file and /etc/profile.d/ directory, both of these will be sourced in the very beginning. These configurations are essential for working on HPC, including submitting jobs to compute nodes. However, sometimes default modules may alter PATH and LD_LIBRARY_PATH variables such that it may conflict with your custom env setup. If so, you can reconfigure respective variables using bash command(s) in ~/.profile.d/void/VA01.sh and similar shell files. These files under void/ will be loaded after bash has initialized system default configurations, i.e., sourced /etc/profile file and shell files under /etc/profile.d/ directory, and therefore, any of bash commands within ~/.profile.d/void/VA01.sh file will override the respective configuration set earlier. I emphasized reconfigure and not resetting earlier because you should be very careful of resetting PATH and LD_LIBRARY_PATH. You may get locked out of HPC login node and may need a help of sysadmin to let you in again! PS: I do reset PATH at the very end of ~/.bash_profile once bash startup sequence has traversed across all of files detailed above and shown in flow diagram below. By doing so, I can get a consistent bash login environment. However, this is an overkill and you can sure get a stable bash env without such reset. Order of precedence matters Be aware of how bash startup sequence will load these files. Check a flow diagram under My bash startup sequence below. For example, if I set PATH variable as export PATH=/a/b/c:/p/q/r in ~/.profile.d/A01.sh and then I add export PATH=/p/q/r:/a/b/c in ~/.profile.d/A02.sh , bash startup sequqence will overwrite previous PATH with an updated PATH from A02.sh file. My bash startup sequence: flow diagram Following diagram represents sequence of files (containing bash code) that gets sourced each time I login to HPC. If plot is not visible below, you can view plot by pasting following code to mermaid live editor . graph TB A[System <br>/etc/profile] --> B[User<br>.bash_profile] B --> C{User .profile.d/ directory}; C -->|Yes| D[source .profile.d/A01.sh]; D -.-> E[source .profile.d/A02.sh]; E -.-> F[source .profile.d/Z99.sh]; F --> G; C --> |No| G{User .bashrc}; G --> |Interactive shell<br>PS1 var is set| H[source /etc/bashrc] H --> I[User .bash_aliases] I --> J[bash terminal config, <br> e.g., colors, history, etc.] J --> K[Initialize<br>Conda Environment] G --> |Non-interactive shell<br>PS1 var is unset| K L[User .profile.d/void] --> M[source <br> .profile.d/void/VA01.sh]; M -.-> N[source <br> .profile.d/void/VZ99.sh]; K --> L; N --> O[Set PATH] O --> P[Set PS1] Example bash startup files Fix link - You can download my bash startup files . It will not work by cloning into your linux env. However, each file has inline comments that should help customizing your bash startup. And that's all! My setup for our CPU-based Sumner HPC or for that matter, a generic linux-based machine is now complete. Since our HPC env at JAX shares the common home directory and base linux image (CentOS 7) between CPU (Sumner) and GPU (Winter) based HPC, the above setup will work off-the-shelf on the Winter HPC too except for tasks which require GPU-based computing, e.g., using GPU-based tensorflow and pytorch libraries. For the latter, I will setup a dedicated GPU-based conda env, rey (and ben and gorgu !) and tweak bash startup such that I get GPU-based bash env only when I login to Winter and to Sumner HPC. For GPU-based setup, go to Part 4 . Related discussions on julia forums and troubleshooting guide . \u21a9","title":"bash startup"},{"location":"hpc/gpu/winter_1/","tags":["hpc","setup","gpu","deep learning","imaging","programming"],"text":"Winter HPC at JAX is a GPU-based computing cluster and it is powered by NVIDIA\u00ae V100 series GPU cards. If you are working on GPU-based HPC or linux env, following page should guide you on setting up commonly used GPU libraries, e.g., Tensorflow 2 , Keras , and PyTorch . GPU setup involves several technical jargon related to hardware compliant libraries, e.g., CUDA toolkit if using NVIDIA marketed GPU cards. I will not go into details of each step here and instead link to installation guide for further details. Knowing such details should be useful while working with deep learning tools and debugging runtime errors. Before starting GPU setup, I expect that you have finished CPU setup, up until Part 3 , mainly installing yoda env and bash startup sequence . Login to GPU HPC \u00b6 First, let's move away from CPU HPC, aka Sumer HPC at JAX, and instead login to GPU HPC, i.e., Winter HPC at JAX. We have a common linux base operating system (OS), i.e., Cent OS 7 and a user home directory for both HPCs, hence I will have an identical bash env - via bash startup sequence - in Winter as of Sumner. home directory and operating system If you have a separate OS and a home directory for CPU and GPU HPCs, you need to start from a scratch in setting up GPU HPC, i.e., initial setup is identical to CPU setup , preferably all three parts or at least installing yoda env and bash startup sequence. If you have an identical home directory but different OS, e.g., Cent OS 6 on CPU HPC and Cent OS 7 on GPU HPC, that's a bad system design in my view as it will be challenging - at least to me - to separate software compilation libraries by modifying PATH, LD_LIBRARY_PATH, etc. and configuration locations, e.g., ~/.local and ~/.config under the shared bash env. Login to Winter HPC ssh winter If you have set bash startup sequence earlier, you should expect ~identical 1 bash env, including ordering of paths (output of echo $PATH ) between CPU and GPU HPCs. Start an interactive job, so that we can use compute and not login node for setup/ This is to avoid our setup being potentially killed on the login node due to compute and/or memory intensive commands we will run during setup. ## interactive job command may vary across HPCs ## requesting partition: gpu with one gpu core srun --job-name = gpusetup --qos = dev --time = 08 :00:00 --mem = 8G --nodes = 1 --ntasks = 2 --mail-type = FAIL --export = all --gres gpu:1 --pty bash --login Notice that I now use bash --login over bash to force interactive login. Details under bash startup sequence . Once you are in the interactive session, bash prompt will change to user@winter200 or some other number than the original login node. We are going to create a new and dedicated conda env for GPU HPC, named rey . That said, we can use previously setup conda env for CPU HPC here too! For example, to start R session on GPU HPC: mamba activate yoda R You should be able to interact with R same as you do on CPU HPC, as long as both HPCs have shared storage paths and an identical OS. Avoid managing conda env across HPCs While it should not matter if you are managing conda env on CPU or GPU HPC, e.g., installing or upgrading conda packages, I prefer to manage all of CPU optimized conda envs , i.e., base , yoda , leia , etc. from Sumner (CPU) HPC. Accordingly, I will use Winter GPU HPC to manage GPU-optimized conda env, i.e., rey and ben . This is particularly important for managing GPU env as CUDA and other GPU-specific libraries are not available on CPU HPC and so, installing or upgrading GPU packages may fail if you use CPU HPC to manage GPU env. Let's deactivate yoda and return to base env in the Winter HPC. mamba deactivate Create a GPU env rey \u00b6 Now, we are going to install sizable (3 GB or more) worth of packages into a new conda env, rey . These packages form core of deep learning or specifically provide set of algorithms to employ artificial neural network based machine learning. mamba create -c conda-forge -c pytorch -n rey python = 3 .9 tensorflow-gpu keras pytorch torchvision torchaudio cudatoolkit = 11 .1.1 cudatoolkit-dev = 11 .1.1 scikit-learn xgboost r-base = 4 .1.1 r-tensorflow r-keras r-tfdatasets tensorflow-hub cupy dask dask-ml pyopencl pocl ## lazy way to check if above command had any errors. ## should return 0, meaning successful execution of ## the most recent previous command. echo $? Before running above command, please know what we are installing here: Create a new conda env, rey for GPU-based HPC. Install all packages with the highest preference from conda-forge channel followed by pytorch . PyTorch is a a commonly used deep learning library and PyTorch team distributes some of dependencies with their own conda channel. For all practical purposes, we will try to keep rey env similar to yoda env while adding GPU support in rey . Accordingly, we need to ensure that we are using similar major.minor version for Python and R - two major programming languages that I use on daily basis. I am using python 3.10 in yoda . However, python 3.10 support is not yet available , a commonly used deep learning library. Hence, I am specifying python=3.9 to ensure that mamba create will do the best to keep that version. You may try using the same python version as you have in yoda env, and check if mamba create ... command above throws an error regarding conflicting versions. If it does, update =3.XX to a one lower minor version until mamba allows you to create a new env, rey . Same logic for R by using r-base=4.1.1 to match R version in yoda env. Please read important notes below on using R packages across two or more conda envs. Install popular ML frameworks with GPU support: Tensorflow 2 , Keras , and PyTorch . Keras now ships with Tensorflow 2 and so specifying keras in command above is optional. Notice that unlike restricting version for Python and R (to match with that in yoda env), we are not specifying versions for any of ML libraries. Doing so has one drawback that in rare cases, conda may end up installing an older but compatible version (with our Python and R) of one or more ML libraries. If this happens and you are in need of the latest ML library, you have an option to create yet another conda env to install the most recent ML library at the cost of possibly installing a different version of Python and R than one in yoda env. CUDA toolkit is the heart of leveraging GPU support on Winter HPC. Using conda, we are installing CUDA toolkit and related development kit to install NVIDIA\u00ae cuDNN library . However, we will ensure - by appending, =11.1.1 to a package - that CUDA toolkit version must match that of system installed CUDA drivers by HPC staff . If there is a mismatch, GPU hardware (NVIDIA V100 card) may fail to recognize our instructions (commands) to perform machine learning analysis. For Winter HPC at JAX, I am using CUDA 11.1.1 based on available drivers in Winter HPC. These drivers are typically configured using HPC modules and you can list those using module available and then list details for a specific drivers, e.g, module show cuda11.1/toolkit/11.1.1 . Besides toolkit, HPC staff also provides following other core drivers which may be required for installing or running a specific GPU-compiled package. For now, I am not loading any of these default modules and instead relying on conda-managed (and minimal) packages. cuda11.1/blas/11.1.1 cuda11.1/fft/11.1.1 cuda11.1/nsight/11.1.1 cuda11.1/profiler/11.1.1 cuda11.1/toolkit/11.1.1 We will also install scikit-learn and XGBoost , two popular libraries for machine learning at-large. Then, I will install GPU support for R language and a few packages for using Tensorflow for R . We have earlier installed R in yoda but we cannot use it in rey env. To leverage R for machine (and deep) learning, I will install the identical R version, i.e. 4.1.1 in conda env, rey . That way, I can use most of R packages from yoda (CPU-based) env for use in rey (GPU-based) env. Careful sharing R packages across two or more conda envs Do note that some R packages requires additional packages (libraries) to be installed in the respective conda env, e.g., rJava requires java from yoda env and may not work with rey env. In such cases, use mamba install to install such packages in rey while on GPU env but avoid running install.packages command from a R session running in rey env. Why? I have briefly touched on this issue in Part 3: Tips on compiling packages . If we use install.packages from rey env, it will end up installing the same package, e.g., rJava and perhaps, all of its dependencies into the same library path as of yoda env, i.e., as defined by first entry of .libPaths() . That is a recipe for warnings and errors because doing so will inevitably mix up library dependencies between two different conda envs, each optimized for CPU and GPU. There is a solution though! We can update .libPaths() for rey on-the-fly when we activate or deactivate rey env ( explained later ) and that way, we can ensure that install.packages should install packages in rey specific R package path. I say \"should\" as R may end up updating packages in any of user-writable paths, even if the path is set as a second or lower preference. In nutshell, to avoid breaking R in multiple conda env, use mamba install or mamba update over R install.packages() when possible. I will also install a few additional packages for image classification and related machine learning purpose. These are: cupy , dask , dask-ml , pyopencl, and pocl . A notable exception is that I am not installing Theano which is not under active development but now forked as Aesara . I am installing all major packages at once to ensure package dependencies are not in conflict and we have a stable GPU env. While some of packages, e.g., r-tensorflow may work on CPU-based HPC too, I would recommend to use this conda env only on the GPU-based HPC as most of packages require GPU support. Install essentials \u00b6 I use following tools in routine and have installed these tools into yoda env - a default for CPU-based HPC 2 . Similarly, I am installing in these tools here in rey env too for GPU-based HPC. mamba install -c conda-forge wget curl rsync libiconv parallel ipyparallel git rsync vim globus-cli tmux screen openjdk = 11 .0 r-rjava matplotlib r-reticulate rpy2 For java (openjdk), prefer using the same version as in yoda , e.g., restrict java major.minor version to be 11.0 but allow a different patch (11.0.1 or 11.0.2,...). Loading GPU env \u00b6 Once we have rey env ready, we can check type of GPU, CUDA drivers, etc. We should also ensure that we configure CUDA drivers and related env variables correctly, so future installations of GPU tools, like TensorRT recognize CUDA related variables and work correctly. Check CUDA drivers \u00b6 Let's activate rey and power up GPU! mamba activate rey Check NVIDIA driver version nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2020 NVIDIA Corporation Built on Mon_Oct_12_20:09:46_PDT_2020 Cuda compilation tools, release 11.1, V11.1.105 Build cuda_11.1.TC455_06.29190527_0 Check GPU usage activity on the compute node using nvidia-smi . This command is from a system-installed cuda libraries, typically under /usr/bin or /usr/local/bin . On Winter HPC at JAX, it is only available on compute nodes and not on a login node. Setup GPU env as Modulefile \u00b6 Unfortunately, conda installed CUDA toolkit is not a full CUDA installation and it does not set any of CUDA related bash env variables, especially CUDA_PATH , CUDA_HOME , CUDNN_PATH variables. conda/conda#7757 Since I have installed the identical cuda toolkit (v11.1.1) in rey conda env to the one managed by out HPC admins, i.e., module show cuda11.1/toolkit/11.1.1 , I will create a module file that includes combination of env variables from both of these toolkits. That way, I can load this module during bash startup such that the module will configure GPU env only on the Winter (GPU) HPC and not on the Sumner (CPU) HPC. Create an empty local directory structure to store user-installed GPU libraries, e.g., configs related to CUPTI, etc. cd \" ${ HPCAPPS } \" && \\ mkdir -p gpu/11.1.1/local Following command will create directory scaffold similar to /usr/local env cd \" ${ HPCAPPS } \" /gpu/11.1.1/local && \\ mkdir -p { bin,etc,include,lib,lib64,libexec,share/ { doc,info,locale,man/ { man1,man3 }}} Create a module file at \"${HPCMODULES}\" mkdir -p \" ${ HPCMODULES } \" /gpu cd \" ${ HPCMODULES } \" /gpu ## create a module file that matches CUDA version. touch 11 .1.1 I have placed GPU configurations from both, admin installed CUDA drivers and GPU packages that I just have installed above. You may need to consult your HPC team to get an idea on configurations that you may able override with conda installed cuda toolkit. Example modulefiles for GPU HPC My gpu modulefile are at /confs/hpc/modules/def Once we have a modulefile ready, we can load custom gpu env using module load gpu/11.1.1 . Notice change in PATH, LD_LIBRARY_PATH, and related env variables. For now, you will notice that \"${CONDA_PREFIX}\"/bin is pushed behind other cuda related paths we have configured using modulefile. Since I prefer to have \"${CONDA_PREFIX}\"/bin take precedence over rest of $PATH contents, I will reset PATH such that \"${CONDA_PREFIX}\"/bin , i.e., ../envs/rey/bin in Winter HPC, will take precedence over other paths that we are loading via above modulefile. See bash startup section for details. Jupyter kernels \u00b6 Let's install Python and R jupyter kernels for rey with configuration similar to that for yoda env. First, we will install required packages. Here, I am not interested in installing bash_kernel as I did with yoda as I rarely use bash kernel and rather prefer terminal. If installing reticulate and rpy2 packages throw warnings about potential upgrade or downgrade of existing packages in rey env, please do not ignore warnings and instead follow steps under installing respective packages in yoda env . mamba install -c conda-forge ipykernel r-irkernel r-reticulate rpy2 Setup Python jupyter kernel for rey python -m ipykernel install --user --name rey_py39 --display-name \"rey_py39\" ## confirm that installation exited without any error ## this should return 0 for successful installation echo $? Setup R jupyter kernel for rey library ( IRkernel ) installspec ( name = \"rey_r41\" , displayname = \"rey_r41\" , user = TRUE ) ## quit R session q ( save = \"no\" ) Configure Python and R kernel loading for rey Don't forget to tweak kernel loading as we did for yoda env else you may encounter issues running GPU-based packages in JupyterLab env. Pro Tip: You can use conditional expression in kernel wrapper, so kernel can only load on GPU-enabled HPC. if [[ \" $( hostname ) \" ! = * \"winter\" * ]] ; then echo -e \"ERROR: Invalid hostname\\nThis kernel works only on winter HPC\\n\" > & 2 exit 1 else #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 CONDA_BASE = $( conda info --base ) && \\ source \" ${ CONDA_BASE } \" /etc/profile.d/conda.sh && \\ conda activate rey #### END CONDA SETUP #### ## Load additional CUDA drivers, toolkit, etc. ## if applicable prior to initializing kernel # module load cuda11.1/toolkit/11.1.1 ##... rest of kernel setup as explained earlier. Renviron setup \u00b6 As explained above in the warning box, be careful installing packages using R from more than one conda envs and instead prefer using mamba install or mamba update to manage R packages. When we start R, it reads ~/.Renviron file or takes precedence based on order as detailed on CRAN - startup webpage. Accordingly, we will create a rey env-specific R Renviron file such that loading R in rey env will use rey specific library path to install new packages 3 , and will not install those under a default library path for yoda that we specified earlier in the setup . Let's create an empty directory to store rey env specific user R packages. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/rey4.1 Create an env specific config directory at the place you like and create a Renviron file inside it. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/R/confs/rey cd /projects/verhaak-lab/amins/hpcenv/opt/R/confs/rey ## create a Renviron file nano Renviron Add following to Renviron file, i.e., we take the R_LIBS path from ~/.Renviron file we created earlier , and then prefix rey env specific paths to it. Here, rey env path consist of two parts: First, /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/rey4.1 is a newly created custom path where install.packages command can install new packages while working in rey but not yoda env. Second, rey env default R library path that we got from .libPaths() output: /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/R/library . This path is used by mamba install or mamba update for managing R packages. R_LIBS = \" /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/rey4.1:/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/R/library:/projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/4.1:/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/library Now setup a custom loading of Renviron for rey env. This will make sure that R environ will switch/revert every time conda env, rey is loaded/unloaded via mamba activate/deactivate command. Example activate.d or deactivate.d scripts to manage conda envs You can view example scripts per respective conda env at /confs/hpc/mambaforge/envs . cd /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/etc/conda ## create a new file nano activate.d/activate-r-env.sh Add following to activate.d/activate-r-env.sh #!/usr/bin/env sh ## Define R_HOME from rey env R_HOME = \" $CONDA_PREFIX /lib/R\" ## override ~/.Renviron which otherwise point to R from yoda env R_ENVIRON_USER = \"/projects/verhaak-lab/amins/hpcenv/opt/R/confs/rey/Renviron\" export R_HOME R_ENVIRON_USER load custom user setup after default setup bash startup reads file in alphanumeric order. There could be other R setup files, e.g., activate.d/activate-r-base.sh . So, make sure to name custom file(s), e.g., activate-r-env.sh file such that it loads after R specific default files. Similarly create a deactivate.d/deactivate-r-env.sh to unload custom changes when we do mamba deactivate to turn off rey env. nano deactivate.d/deactivate-r-env.sh and add following: #!/usr/bin/env sh ## fall back to pre-existing R env unset R_HOME R_ENVIRON_USER Prefer mamba deactivate followed by mamba activate <env name> unset command will erase and not restore the matching custom env variables, if any. So, ideally you should do mamba deactivate to turn off rey and then do mamba activate yoda to properly activate yoda env to restore custom set env variables and all of env specific bash startup under respective activate.d/ directory. Now logout and login to winter HPC again. Do mamba activate rey and start R, and type .libPaths() . Now, exit R and type do mamba deactivate followed by mamba activate yoda . Start R and type .libPaths() . Notice difference in R library paths under two R sessions! Optional Setup \u00b6 Following packages are optional for setup. Tensorboard \u00b6 Tensorboard graphical user interface (GUI) ships with Tensorflow 2 and so does not require additional configuration. Check version for tensorflow and tensorboard ## in rey env python -c 'import tensorflow as tf; print(tf.__version__)' #2.6.2 or higher python -c 'import tensorboard as tb; print(tb.__version__)' #2.6.0 or higher Checkout getting started guide for more on how to use GUI app. If tensorboard python notebook extension, %load_ext tensorboard fails to initialize tensorboard within notebook, you can manually initialize tensorboard using a terminal command as follows: tensorboard serve --logdir logs --host <IP address to bind to> where IP address can be a localhost or hostname -I as long as it is on the secure network. Tensorboard should be accessible at an unsecure http address shown in the output of above command. To quit tensorboard web server on the terminal, press Ctrl + C . TensorRT \u00b6 NVIDIA\u00ae TensorRT\u2122 a software development kit (SDK) for NVIDIA compliant GPU cards. Conda does not provide TensorRT package, so we need to install it using getting started guide and installation using tarball instructions . This requires membership into NVIDIA developer program. Installation steps \u00b6 Download tarball specific to CUDA and cuDNN version as determined by following commands. ## CUDA version, 11.1 nvcc --version ## cuDNN version 8.2 cat ${ CONDA_PREFIX } /include/cudnn_version.h | grep CUDNN_MAJOR -A 2 Accordingly, I have downloaded following tarball: TensorRT-8.2.2.1.Linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz Extract tarball to apps folder and rename path to extracted contents, so that we can load TensorRT as a module . cd \" ${ HPCAPPS } \" mkdir -p tensorrt cd tensorrt ## place tarball in tensorrt directory and then extract it. tar xvzf TensorRT-8.2.2.1.Linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz ## rename extracted directory for consistency on naming modules. mv TensorRT-8.2.2.1 8 .2.2.1 To install TensorRT, we need to temporarily export TensorRT library path to LD_LIBRARY_PATH. For future logins to GPU HPC, we can then load this path as and when needed using modulefile or permanently insert this into LD_LIBRARY_PATH for GPU HPC using GPU-specific bash startup . export LD_LIBRARY_PATH = \" ${ HPCAPPS } /tensorrt/8.2.2.1/lib: ${ LD_LIBRARY_PATH } \" Install Python TensorRT wheel file. There are more than one file with different cp3x . I could not figure out what it means and so ended up installing the most recent one, i.e., cp39 . cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1/python && \\ pip install tensorrt-8.2.2.1-cp39-none-linux_x86_64.whl |& tee -a tensorrt_8.2.2.1_install.log Expected output: Processing ./tensorrt-8.2.2.1-cp39-none-linux_x86_64.whl Installing collected packages: tensorrt Successfully installed tensorrt-8.2.2.1 Install Python UFF wheel file which is required for working with TensorFlow. cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1/uff && \\ pip install uff-0.6.9-py2.py3-none-any.whl |& tee -a tensorrt_8.2.2.1_install.log Expected output: Processing ./uff-0.6.9-py2.py3-none-any.whl Requirement already satisfied: numpy>=1.11.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from uff==0.6.9) (1.19.5) Requirement already satisfied: protobuf>=3.3.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from uff==0.6.9) (3.18.1) Installing collected packages: uff Successfully installed uff-0.6.9 When to use bash startup versus module file Above installation step should include convert-to-uff in PATH which you can check within output of which convert-to-uff . Since installation has already inserted binaries, e.g., convert-to-uff into bash PATH variable, I will now prefer to setup TensorRT related PATH and LD_LIBRARY_PATH using bash startup instead of loading TensorRT using module file. Module file works better if installation setup does not alter core bash startup variables like PATH and LD_LIBRARY_PATH. Install the Python graphsurgeon wheel file. cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1/graphsurgeon && \\ pip install graphsurgeon-0.4.5-py2.py3-none-any.whl |& tee -a tensorrt_8.2.2.1_install.log Expected output: Processing ./graphsurgeon-0.4.5-py2.py3-none-any.whl Installing collected packages: graphsurgeon Successfully installed graphsurgeon-0.4.5 Install the Python onnx-graphsurgeon wheel file. cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1/onnx_graphsurgeon && \\ pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl |& tee -a tensorrt_8.2.2.1_install.log Expected output: Processing ./onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl Requirement already satisfied: numpy in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx-graphsurgeon==0.3.12) (1.19.5) Collecting onnx Downloading onnx-1.10.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.7 MB) Requirement already satisfied: protobuf in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx->onnx-graphsurgeon==0.3.12) (3.18.1) Requirement already satisfied: six in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx->onnx-graphsurgeon==0.3.12) (1.15.0) Requirement already satisfied: typing-extensions>=3.6.2.1 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx->onnx-graphsurgeon==0.3.12) (3.7.4.3) Installing collected packages: onnx, onnx-graphsurgeon Successfully installed onnx-1.10.2 onnx-graphsurgeon-0.3.12 Test GPU functionality \u00b6 Once we have rey env ready, we can test GPU functionality of installed packages. This is not a required step but I like to make sure that I am using GPU and not CPU for computation, e.g., tensorflow and r-keras package may fall back to CPU if it finds missing or badly configured support for GPU. Test Tensorflow and Keras \u00b6 I have followed beginner scripts from tensorflow tutorials to test GPU functionality. Similarly, RStudio section on Tensorflow for R provides beginners tutorials for testing machine learning using GPU. Test PyTorch \u00b6 See details on PyTorch website. import torch x = torch . rand ( 5 , 3 ) print ( x ) Test TensorRT \u00b6 cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1 ## ensure that gpu module is loaded module load gpu/11.1.1 cd samples/sampleMNIST && \\ make && \\ echo \"make OK\" cd ../../data/mnist && \\ ## Download MNIST dataset wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz ls *ubyte.gz | parallel -j2 gunzip {} cd ../.. && \\ ./bin/sample_mnist -h && \\ ./bin/sample_mnist --datadir = data/mnist If all goes well, you will see tests passed ok and a predicted digit in ASCII art . Dask \u00b6 Read docs at http://distributed.dask.org/en/stable/client.html Image Classification \u00b6 Optional: Libraries specific to cell segmentation. I am creating a new conda env, ben for installing tools related to cell segmentation analysis. These tools require additional set of packages (including installing using pip ) and are updated often which together can make rey env unstable over long run. Most of packages are based on package requirements for CellPose tool: setup.py and requirements.txt file. mamba create -c conda-forge -c pytorch -n ben python = 3 .9 tensorflow-gpu keras pytorch torchvision cudatoolkit = 11 .1.1 cudatoolkit-dev = 11 .1.1 scikit-learn numpy scipy natsort tifffile tqdm numba torch-optimizer Before activating ben env, duplicate modulefile, gpu/11.1.1 that we created earlier to gpu/11.1.1_ben . Replace conda env name from rey to ben in gpu/11.1.1_ben . This will allow to load a valid GPU env and avoid potential danger of putting rey paths in PATH and LD_LIBRARY_PATH while we work in ben env. Activate ben env mamba activate ben Check for a valid bash env If you notice any of rey env related paths, especially taking precedence over ben env paths, something is wrong and you should check modulefiles above to load conda env specific valid env. Installing cellpose and related package dependencies with invalid bash env will invariably break the core, rey env . module load gpu/11.1.1_ben ## These should point to paths related to ben and not rey env. echo $PATH echo $LD_LIBRARY_PATH Cellpose \u00b6 A generalized algorithm for cellular segmentation. MouseLand/cellpose Not recommended but given many dependencies for cellpose are not available or of conflicting nature using mamba install , I am falling back to pip install . pip install cellpose [ all ] |& tee -a ~/logs/cellpose_install.log In case of errors or unstable env, I can always purge ben env without any impact on rey conda env. Installation log and warnings, if any Installing collected packages: googleapis-common-protos, pyparsing, numpy, google-crc32c, google-api-core, PyWavelets, pyqt5.sip, PyQt5-Qt5, packaging, opencv-python-headless, networkx, imageio, google-resumable-media, google-cloud-core, fastremap, edt, scikit-image, pyqtgraph, pyqt5, google-cloud-storage, cellpose Attempting uninstall: numpy Found existing installation: numpy 1.19.5 Uninstalling numpy-1.19.5: Successfully uninstalled numpy-1.19.5 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow 2.6.2 requires numpy~=1.19.2, but you have numpy 1.21.5 which is incompatible. Successfully installed PyQt5-Qt5-5.15.2 PyWavelets-1.2.0 cellpose-0.7.2 edt-2.1.1 fastremap-1.12.2 google-api-core-2.4.0 google-cloud-core-2.2.1 google-cloud-storage-2.0.0 google-crc32c-1.3.0 google-resumable-media-2.1.0 googleapis-common-protos-1.54.0 imageio-2.13.5 networkx-2.6.3 numpy-1.21.5 opencv-python-headless-4.5.5.62 packaging-21.3 pyparsing-3.0.6 pyqt5-5.15.6 pyqt5.sip-12.9.0 pyqtgraph-0.11.0rc0 scikit-image-0.19.1 Turns out tensorflow 2 (GPU) works with updated numpy and should not be throw an error. python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\" CellPose should now be all set for running in ben env. cellpose --help Stardist \u00b6 StarDist - Object Detection with Star-convex Shapes. stardist/stardist ## in ben env pip install stardist |& tee -a stardist_install.log Installation log and warnings, if any Installing collected packages: python-dateutil, kiwisolver, fonttools, cycler, matplotlib, csbdeep, stardist Successfully installed csbdeep-0.6.3 cycler-0.11.0 fonttools-4.28.5 kiwisolver-1.3.2 matplotlib-3.5.1 python-dateutil-2.8.2 stardist-0.7.3 To test run, follow example from stardist repo . Cellprofiler \u00b6 Tool for image analysis, cellprofiler.org Related bioformats2raw and raw2ometiff were downloaded as standalone binary packages and installed as modules. PS: Cellprofiler has a limited GPU support for now but it may change in the future. Follow Cellprofiler forum for updates. For now, I am installing it in grogu env which is a toy env! ## login to CPU HPC ssh sumner Create grogu conda env mamba create -c conda-forge -c bioconda -n grogu cellprofiler Run cellprofiler mamba activate grogu cellprofiler --help Update bash startup \u00b6 Finally, I am tweaking bash startup sequence that we had setup earlier, such that it can allow loading GPU-specific bash env only when we login to Winter GPU HPC and not on Sumner CPU HPC. I have made following changes to bash startup. You can download my bash startup files here . Update SET PATH block of ~/.bash_profile to reset PATH for Winter GPU. See my notes under elif [[ \"$(hostname)\" == *\"winter\"* ]]; then section in an example .bash_profile file. Update ~/.profile.d/void/VW01_set_winter_gpu.sh to load Winter specific settings. See more into an example VW01_set_winter_gpu.sh file. Logout and login again to Winter HPC. You will see a near identical bash prompt like Sumner HPC, e.g., user@winter-log1 . However, when you check echo $PATH output and echo $CONDA_DEFAULT_ENV , you will notice that a default conda env in Winter HPC is now rey while in Sumner HPC, it is base (sometimes called root ). Of course, you can revert to base or any other conda env in Winter HPC by doing mamba deactivate (because we changed from base to rey during bash startup) and then mamba activate base (or yoda, or any other env). If you have also setup activate.d/deactivate.d scripts as detailed earlier , you will be able to fine tune loading and unloading of conda env specific to HPC type (CPU or GPU) as well as type of R and GPU-specific configs. See /confs/hpc/mambaforge/envs for example scripts. Done! \u00b6 Hope you have found this documentation helpful. I think this is more technical that I originally expected and you may have to look into stackoverflow or elsewhere to understand jargons I used across pages. Hopefully, I can go through some of sections again and put emphasis on rationale behind setting up my linux environment. That said, I hope this documentation, at least the CPU part, should get you started with HPC setup. For learning specific programming language and data analysis, I will post a few external resources on getting started guide to learn programming in Python, R, and more. Best wishes! There could be a difference though if both HPCs do not share common OS or they are using different system defaults, e.g., loading different bash env from /etc/profile which is managed by HPC staff. \u21a9 Installing common packages in yoda env and installing essentials . \u21a9 See warning box above on why our setup can still update R packages managed by yoda env despite using custom Renviron file. \u21a9","title":"GPU setup"},{"location":"hpc/gpu/winter_1/#login-to-gpu-hpc","text":"First, let's move away from CPU HPC, aka Sumer HPC at JAX, and instead login to GPU HPC, i.e., Winter HPC at JAX. We have a common linux base operating system (OS), i.e., Cent OS 7 and a user home directory for both HPCs, hence I will have an identical bash env - via bash startup sequence - in Winter as of Sumner. home directory and operating system If you have a separate OS and a home directory for CPU and GPU HPCs, you need to start from a scratch in setting up GPU HPC, i.e., initial setup is identical to CPU setup , preferably all three parts or at least installing yoda env and bash startup sequence. If you have an identical home directory but different OS, e.g., Cent OS 6 on CPU HPC and Cent OS 7 on GPU HPC, that's a bad system design in my view as it will be challenging - at least to me - to separate software compilation libraries by modifying PATH, LD_LIBRARY_PATH, etc. and configuration locations, e.g., ~/.local and ~/.config under the shared bash env. Login to Winter HPC ssh winter If you have set bash startup sequence earlier, you should expect ~identical 1 bash env, including ordering of paths (output of echo $PATH ) between CPU and GPU HPCs. Start an interactive job, so that we can use compute and not login node for setup/ This is to avoid our setup being potentially killed on the login node due to compute and/or memory intensive commands we will run during setup. ## interactive job command may vary across HPCs ## requesting partition: gpu with one gpu core srun --job-name = gpusetup --qos = dev --time = 08 :00:00 --mem = 8G --nodes = 1 --ntasks = 2 --mail-type = FAIL --export = all --gres gpu:1 --pty bash --login Notice that I now use bash --login over bash to force interactive login. Details under bash startup sequence . Once you are in the interactive session, bash prompt will change to user@winter200 or some other number than the original login node. We are going to create a new and dedicated conda env for GPU HPC, named rey . That said, we can use previously setup conda env for CPU HPC here too! For example, to start R session on GPU HPC: mamba activate yoda R You should be able to interact with R same as you do on CPU HPC, as long as both HPCs have shared storage paths and an identical OS. Avoid managing conda env across HPCs While it should not matter if you are managing conda env on CPU or GPU HPC, e.g., installing or upgrading conda packages, I prefer to manage all of CPU optimized conda envs , i.e., base , yoda , leia , etc. from Sumner (CPU) HPC. Accordingly, I will use Winter GPU HPC to manage GPU-optimized conda env, i.e., rey and ben . This is particularly important for managing GPU env as CUDA and other GPU-specific libraries are not available on CPU HPC and so, installing or upgrading GPU packages may fail if you use CPU HPC to manage GPU env. Let's deactivate yoda and return to base env in the Winter HPC. mamba deactivate","title":"Login to GPU HPC"},{"location":"hpc/gpu/winter_1/#create-a-gpu-env-rey","text":"Now, we are going to install sizable (3 GB or more) worth of packages into a new conda env, rey . These packages form core of deep learning or specifically provide set of algorithms to employ artificial neural network based machine learning. mamba create -c conda-forge -c pytorch -n rey python = 3 .9 tensorflow-gpu keras pytorch torchvision torchaudio cudatoolkit = 11 .1.1 cudatoolkit-dev = 11 .1.1 scikit-learn xgboost r-base = 4 .1.1 r-tensorflow r-keras r-tfdatasets tensorflow-hub cupy dask dask-ml pyopencl pocl ## lazy way to check if above command had any errors. ## should return 0, meaning successful execution of ## the most recent previous command. echo $? Before running above command, please know what we are installing here: Create a new conda env, rey for GPU-based HPC. Install all packages with the highest preference from conda-forge channel followed by pytorch . PyTorch is a a commonly used deep learning library and PyTorch team distributes some of dependencies with their own conda channel. For all practical purposes, we will try to keep rey env similar to yoda env while adding GPU support in rey . Accordingly, we need to ensure that we are using similar major.minor version for Python and R - two major programming languages that I use on daily basis. I am using python 3.10 in yoda . However, python 3.10 support is not yet available , a commonly used deep learning library. Hence, I am specifying python=3.9 to ensure that mamba create will do the best to keep that version. You may try using the same python version as you have in yoda env, and check if mamba create ... command above throws an error regarding conflicting versions. If it does, update =3.XX to a one lower minor version until mamba allows you to create a new env, rey . Same logic for R by using r-base=4.1.1 to match R version in yoda env. Please read important notes below on using R packages across two or more conda envs. Install popular ML frameworks with GPU support: Tensorflow 2 , Keras , and PyTorch . Keras now ships with Tensorflow 2 and so specifying keras in command above is optional. Notice that unlike restricting version for Python and R (to match with that in yoda env), we are not specifying versions for any of ML libraries. Doing so has one drawback that in rare cases, conda may end up installing an older but compatible version (with our Python and R) of one or more ML libraries. If this happens and you are in need of the latest ML library, you have an option to create yet another conda env to install the most recent ML library at the cost of possibly installing a different version of Python and R than one in yoda env. CUDA toolkit is the heart of leveraging GPU support on Winter HPC. Using conda, we are installing CUDA toolkit and related development kit to install NVIDIA\u00ae cuDNN library . However, we will ensure - by appending, =11.1.1 to a package - that CUDA toolkit version must match that of system installed CUDA drivers by HPC staff . If there is a mismatch, GPU hardware (NVIDIA V100 card) may fail to recognize our instructions (commands) to perform machine learning analysis. For Winter HPC at JAX, I am using CUDA 11.1.1 based on available drivers in Winter HPC. These drivers are typically configured using HPC modules and you can list those using module available and then list details for a specific drivers, e.g, module show cuda11.1/toolkit/11.1.1 . Besides toolkit, HPC staff also provides following other core drivers which may be required for installing or running a specific GPU-compiled package. For now, I am not loading any of these default modules and instead relying on conda-managed (and minimal) packages. cuda11.1/blas/11.1.1 cuda11.1/fft/11.1.1 cuda11.1/nsight/11.1.1 cuda11.1/profiler/11.1.1 cuda11.1/toolkit/11.1.1 We will also install scikit-learn and XGBoost , two popular libraries for machine learning at-large. Then, I will install GPU support for R language and a few packages for using Tensorflow for R . We have earlier installed R in yoda but we cannot use it in rey env. To leverage R for machine (and deep) learning, I will install the identical R version, i.e. 4.1.1 in conda env, rey . That way, I can use most of R packages from yoda (CPU-based) env for use in rey (GPU-based) env. Careful sharing R packages across two or more conda envs Do note that some R packages requires additional packages (libraries) to be installed in the respective conda env, e.g., rJava requires java from yoda env and may not work with rey env. In such cases, use mamba install to install such packages in rey while on GPU env but avoid running install.packages command from a R session running in rey env. Why? I have briefly touched on this issue in Part 3: Tips on compiling packages . If we use install.packages from rey env, it will end up installing the same package, e.g., rJava and perhaps, all of its dependencies into the same library path as of yoda env, i.e., as defined by first entry of .libPaths() . That is a recipe for warnings and errors because doing so will inevitably mix up library dependencies between two different conda envs, each optimized for CPU and GPU. There is a solution though! We can update .libPaths() for rey on-the-fly when we activate or deactivate rey env ( explained later ) and that way, we can ensure that install.packages should install packages in rey specific R package path. I say \"should\" as R may end up updating packages in any of user-writable paths, even if the path is set as a second or lower preference. In nutshell, to avoid breaking R in multiple conda env, use mamba install or mamba update over R install.packages() when possible. I will also install a few additional packages for image classification and related machine learning purpose. These are: cupy , dask , dask-ml , pyopencl, and pocl . A notable exception is that I am not installing Theano which is not under active development but now forked as Aesara . I am installing all major packages at once to ensure package dependencies are not in conflict and we have a stable GPU env. While some of packages, e.g., r-tensorflow may work on CPU-based HPC too, I would recommend to use this conda env only on the GPU-based HPC as most of packages require GPU support.","title":"Create a GPU env rey"},{"location":"hpc/gpu/winter_1/#install-essentials","text":"I use following tools in routine and have installed these tools into yoda env - a default for CPU-based HPC 2 . Similarly, I am installing in these tools here in rey env too for GPU-based HPC. mamba install -c conda-forge wget curl rsync libiconv parallel ipyparallel git rsync vim globus-cli tmux screen openjdk = 11 .0 r-rjava matplotlib r-reticulate rpy2 For java (openjdk), prefer using the same version as in yoda , e.g., restrict java major.minor version to be 11.0 but allow a different patch (11.0.1 or 11.0.2,...).","title":"Install essentials"},{"location":"hpc/gpu/winter_1/#loading-gpu-env","text":"Once we have rey env ready, we can check type of GPU, CUDA drivers, etc. We should also ensure that we configure CUDA drivers and related env variables correctly, so future installations of GPU tools, like TensorRT recognize CUDA related variables and work correctly.","title":"Loading GPU env"},{"location":"hpc/gpu/winter_1/#check-cuda-drivers","text":"Let's activate rey and power up GPU! mamba activate rey Check NVIDIA driver version nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2020 NVIDIA Corporation Built on Mon_Oct_12_20:09:46_PDT_2020 Cuda compilation tools, release 11.1, V11.1.105 Build cuda_11.1.TC455_06.29190527_0 Check GPU usage activity on the compute node using nvidia-smi . This command is from a system-installed cuda libraries, typically under /usr/bin or /usr/local/bin . On Winter HPC at JAX, it is only available on compute nodes and not on a login node.","title":"Check CUDA drivers"},{"location":"hpc/gpu/winter_1/#setup-gpu-env-as-modulefile","text":"Unfortunately, conda installed CUDA toolkit is not a full CUDA installation and it does not set any of CUDA related bash env variables, especially CUDA_PATH , CUDA_HOME , CUDNN_PATH variables. conda/conda#7757 Since I have installed the identical cuda toolkit (v11.1.1) in rey conda env to the one managed by out HPC admins, i.e., module show cuda11.1/toolkit/11.1.1 , I will create a module file that includes combination of env variables from both of these toolkits. That way, I can load this module during bash startup such that the module will configure GPU env only on the Winter (GPU) HPC and not on the Sumner (CPU) HPC. Create an empty local directory structure to store user-installed GPU libraries, e.g., configs related to CUPTI, etc. cd \" ${ HPCAPPS } \" && \\ mkdir -p gpu/11.1.1/local Following command will create directory scaffold similar to /usr/local env cd \" ${ HPCAPPS } \" /gpu/11.1.1/local && \\ mkdir -p { bin,etc,include,lib,lib64,libexec,share/ { doc,info,locale,man/ { man1,man3 }}} Create a module file at \"${HPCMODULES}\" mkdir -p \" ${ HPCMODULES } \" /gpu cd \" ${ HPCMODULES } \" /gpu ## create a module file that matches CUDA version. touch 11 .1.1 I have placed GPU configurations from both, admin installed CUDA drivers and GPU packages that I just have installed above. You may need to consult your HPC team to get an idea on configurations that you may able override with conda installed cuda toolkit. Example modulefiles for GPU HPC My gpu modulefile are at /confs/hpc/modules/def Once we have a modulefile ready, we can load custom gpu env using module load gpu/11.1.1 . Notice change in PATH, LD_LIBRARY_PATH, and related env variables. For now, you will notice that \"${CONDA_PREFIX}\"/bin is pushed behind other cuda related paths we have configured using modulefile. Since I prefer to have \"${CONDA_PREFIX}\"/bin take precedence over rest of $PATH contents, I will reset PATH such that \"${CONDA_PREFIX}\"/bin , i.e., ../envs/rey/bin in Winter HPC, will take precedence over other paths that we are loading via above modulefile. See bash startup section for details.","title":"Setup GPU env as Modulefile"},{"location":"hpc/gpu/winter_1/#jupyter-kernels","text":"Let's install Python and R jupyter kernels for rey with configuration similar to that for yoda env. First, we will install required packages. Here, I am not interested in installing bash_kernel as I did with yoda as I rarely use bash kernel and rather prefer terminal. If installing reticulate and rpy2 packages throw warnings about potential upgrade or downgrade of existing packages in rey env, please do not ignore warnings and instead follow steps under installing respective packages in yoda env . mamba install -c conda-forge ipykernel r-irkernel r-reticulate rpy2 Setup Python jupyter kernel for rey python -m ipykernel install --user --name rey_py39 --display-name \"rey_py39\" ## confirm that installation exited without any error ## this should return 0 for successful installation echo $? Setup R jupyter kernel for rey library ( IRkernel ) installspec ( name = \"rey_r41\" , displayname = \"rey_r41\" , user = TRUE ) ## quit R session q ( save = \"no\" ) Configure Python and R kernel loading for rey Don't forget to tweak kernel loading as we did for yoda env else you may encounter issues running GPU-based packages in JupyterLab env. Pro Tip: You can use conditional expression in kernel wrapper, so kernel can only load on GPU-enabled HPC. if [[ \" $( hostname ) \" ! = * \"winter\" * ]] ; then echo -e \"ERROR: Invalid hostname\\nThis kernel works only on winter HPC\\n\" > & 2 exit 1 else #### Activate CONDA in subshell #### ## Read https://github.com/conda/conda/issues/7980 CONDA_BASE = $( conda info --base ) && \\ source \" ${ CONDA_BASE } \" /etc/profile.d/conda.sh && \\ conda activate rey #### END CONDA SETUP #### ## Load additional CUDA drivers, toolkit, etc. ## if applicable prior to initializing kernel # module load cuda11.1/toolkit/11.1.1 ##... rest of kernel setup as explained earlier.","title":"Jupyter kernels"},{"location":"hpc/gpu/winter_1/#renviron-setup","text":"As explained above in the warning box, be careful installing packages using R from more than one conda envs and instead prefer using mamba install or mamba update to manage R packages. When we start R, it reads ~/.Renviron file or takes precedence based on order as detailed on CRAN - startup webpage. Accordingly, we will create a rey env-specific R Renviron file such that loading R in rey env will use rey specific library path to install new packages 3 , and will not install those under a default library path for yoda that we specified earlier in the setup . Let's create an empty directory to store rey env specific user R packages. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/rey4.1 Create an env specific config directory at the place you like and create a Renviron file inside it. mkdir -p /projects/verhaak-lab/amins/hpcenv/opt/R/confs/rey cd /projects/verhaak-lab/amins/hpcenv/opt/R/confs/rey ## create a Renviron file nano Renviron Add following to Renviron file, i.e., we take the R_LIBS path from ~/.Renviron file we created earlier , and then prefix rey env specific paths to it. Here, rey env path consist of two parts: First, /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/rey4.1 is a newly created custom path where install.packages command can install new packages while working in rey but not yoda env. Second, rey env default R library path that we got from .libPaths() output: /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/R/library . This path is used by mamba install or mamba update for managing R packages. R_LIBS = \" /projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/rey4.1:/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/R/library:/projects/verhaak-lab/amins/hpcenv/opt/R/pkgs/4.1:/projects/verhaak-lab/amins/hpcenv/mambaforge/envs/yoda/lib/R/library Now setup a custom loading of Renviron for rey env. This will make sure that R environ will switch/revert every time conda env, rey is loaded/unloaded via mamba activate/deactivate command. Example activate.d or deactivate.d scripts to manage conda envs You can view example scripts per respective conda env at /confs/hpc/mambaforge/envs . cd /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/etc/conda ## create a new file nano activate.d/activate-r-env.sh Add following to activate.d/activate-r-env.sh #!/usr/bin/env sh ## Define R_HOME from rey env R_HOME = \" $CONDA_PREFIX /lib/R\" ## override ~/.Renviron which otherwise point to R from yoda env R_ENVIRON_USER = \"/projects/verhaak-lab/amins/hpcenv/opt/R/confs/rey/Renviron\" export R_HOME R_ENVIRON_USER load custom user setup after default setup bash startup reads file in alphanumeric order. There could be other R setup files, e.g., activate.d/activate-r-base.sh . So, make sure to name custom file(s), e.g., activate-r-env.sh file such that it loads after R specific default files. Similarly create a deactivate.d/deactivate-r-env.sh to unload custom changes when we do mamba deactivate to turn off rey env. nano deactivate.d/deactivate-r-env.sh and add following: #!/usr/bin/env sh ## fall back to pre-existing R env unset R_HOME R_ENVIRON_USER Prefer mamba deactivate followed by mamba activate <env name> unset command will erase and not restore the matching custom env variables, if any. So, ideally you should do mamba deactivate to turn off rey and then do mamba activate yoda to properly activate yoda env to restore custom set env variables and all of env specific bash startup under respective activate.d/ directory. Now logout and login to winter HPC again. Do mamba activate rey and start R, and type .libPaths() . Now, exit R and type do mamba deactivate followed by mamba activate yoda . Start R and type .libPaths() . Notice difference in R library paths under two R sessions!","title":"Renviron setup"},{"location":"hpc/gpu/winter_1/#optional-setup","text":"Following packages are optional for setup.","title":"Optional Setup"},{"location":"hpc/gpu/winter_1/#tensorboard","text":"Tensorboard graphical user interface (GUI) ships with Tensorflow 2 and so does not require additional configuration. Check version for tensorflow and tensorboard ## in rey env python -c 'import tensorflow as tf; print(tf.__version__)' #2.6.2 or higher python -c 'import tensorboard as tb; print(tb.__version__)' #2.6.0 or higher Checkout getting started guide for more on how to use GUI app. If tensorboard python notebook extension, %load_ext tensorboard fails to initialize tensorboard within notebook, you can manually initialize tensorboard using a terminal command as follows: tensorboard serve --logdir logs --host <IP address to bind to> where IP address can be a localhost or hostname -I as long as it is on the secure network. Tensorboard should be accessible at an unsecure http address shown in the output of above command. To quit tensorboard web server on the terminal, press Ctrl + C .","title":"Tensorboard"},{"location":"hpc/gpu/winter_1/#tensorrt","text":"NVIDIA\u00ae TensorRT\u2122 a software development kit (SDK) for NVIDIA compliant GPU cards. Conda does not provide TensorRT package, so we need to install it using getting started guide and installation using tarball instructions . This requires membership into NVIDIA developer program.","title":"TensorRT"},{"location":"hpc/gpu/winter_1/#installation-steps","text":"Download tarball specific to CUDA and cuDNN version as determined by following commands. ## CUDA version, 11.1 nvcc --version ## cuDNN version 8.2 cat ${ CONDA_PREFIX } /include/cudnn_version.h | grep CUDNN_MAJOR -A 2 Accordingly, I have downloaded following tarball: TensorRT-8.2.2.1.Linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz Extract tarball to apps folder and rename path to extracted contents, so that we can load TensorRT as a module . cd \" ${ HPCAPPS } \" mkdir -p tensorrt cd tensorrt ## place tarball in tensorrt directory and then extract it. tar xvzf TensorRT-8.2.2.1.Linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz ## rename extracted directory for consistency on naming modules. mv TensorRT-8.2.2.1 8 .2.2.1 To install TensorRT, we need to temporarily export TensorRT library path to LD_LIBRARY_PATH. For future logins to GPU HPC, we can then load this path as and when needed using modulefile or permanently insert this into LD_LIBRARY_PATH for GPU HPC using GPU-specific bash startup . export LD_LIBRARY_PATH = \" ${ HPCAPPS } /tensorrt/8.2.2.1/lib: ${ LD_LIBRARY_PATH } \" Install Python TensorRT wheel file. There are more than one file with different cp3x . I could not figure out what it means and so ended up installing the most recent one, i.e., cp39 . cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1/python && \\ pip install tensorrt-8.2.2.1-cp39-none-linux_x86_64.whl |& tee -a tensorrt_8.2.2.1_install.log Expected output: Processing ./tensorrt-8.2.2.1-cp39-none-linux_x86_64.whl Installing collected packages: tensorrt Successfully installed tensorrt-8.2.2.1 Install Python UFF wheel file which is required for working with TensorFlow. cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1/uff && \\ pip install uff-0.6.9-py2.py3-none-any.whl |& tee -a tensorrt_8.2.2.1_install.log Expected output: Processing ./uff-0.6.9-py2.py3-none-any.whl Requirement already satisfied: numpy>=1.11.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from uff==0.6.9) (1.19.5) Requirement already satisfied: protobuf>=3.3.0 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from uff==0.6.9) (3.18.1) Installing collected packages: uff Successfully installed uff-0.6.9 When to use bash startup versus module file Above installation step should include convert-to-uff in PATH which you can check within output of which convert-to-uff . Since installation has already inserted binaries, e.g., convert-to-uff into bash PATH variable, I will now prefer to setup TensorRT related PATH and LD_LIBRARY_PATH using bash startup instead of loading TensorRT using module file. Module file works better if installation setup does not alter core bash startup variables like PATH and LD_LIBRARY_PATH. Install the Python graphsurgeon wheel file. cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1/graphsurgeon && \\ pip install graphsurgeon-0.4.5-py2.py3-none-any.whl |& tee -a tensorrt_8.2.2.1_install.log Expected output: Processing ./graphsurgeon-0.4.5-py2.py3-none-any.whl Installing collected packages: graphsurgeon Successfully installed graphsurgeon-0.4.5 Install the Python onnx-graphsurgeon wheel file. cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1/onnx_graphsurgeon && \\ pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl |& tee -a tensorrt_8.2.2.1_install.log Expected output: Processing ./onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl Requirement already satisfied: numpy in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx-graphsurgeon==0.3.12) (1.19.5) Collecting onnx Downloading onnx-1.10.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.7 MB) Requirement already satisfied: protobuf in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx->onnx-graphsurgeon==0.3.12) (3.18.1) Requirement already satisfied: six in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx->onnx-graphsurgeon==0.3.12) (1.15.0) Requirement already satisfied: typing-extensions>=3.6.2.1 in /projects/verhaak-lab/amins/hpcenv/mambaforge/envs/rey/lib/python3.9/site-packages (from onnx->onnx-graphsurgeon==0.3.12) (3.7.4.3) Installing collected packages: onnx, onnx-graphsurgeon Successfully installed onnx-1.10.2 onnx-graphsurgeon-0.3.12","title":"Installation steps"},{"location":"hpc/gpu/winter_1/#test-gpu-functionality","text":"Once we have rey env ready, we can test GPU functionality of installed packages. This is not a required step but I like to make sure that I am using GPU and not CPU for computation, e.g., tensorflow and r-keras package may fall back to CPU if it finds missing or badly configured support for GPU.","title":"Test GPU functionality"},{"location":"hpc/gpu/winter_1/#test-tensorflow-and-keras","text":"I have followed beginner scripts from tensorflow tutorials to test GPU functionality. Similarly, RStudio section on Tensorflow for R provides beginners tutorials for testing machine learning using GPU.","title":"Test Tensorflow and Keras"},{"location":"hpc/gpu/winter_1/#test-pytorch","text":"See details on PyTorch website. import torch x = torch . rand ( 5 , 3 ) print ( x )","title":"Test PyTorch"},{"location":"hpc/gpu/winter_1/#test-tensorrt","text":"cd \" ${ HPCAPPS } \" /tensorrt/8.2.2.1 ## ensure that gpu module is loaded module load gpu/11.1.1 cd samples/sampleMNIST && \\ make && \\ echo \"make OK\" cd ../../data/mnist && \\ ## Download MNIST dataset wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz && \\ wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz ls *ubyte.gz | parallel -j2 gunzip {} cd ../.. && \\ ./bin/sample_mnist -h && \\ ./bin/sample_mnist --datadir = data/mnist If all goes well, you will see tests passed ok and a predicted digit in ASCII art .","title":"Test TensorRT"},{"location":"hpc/gpu/winter_1/#dask","text":"Read docs at http://distributed.dask.org/en/stable/client.html","title":"Dask"},{"location":"hpc/gpu/winter_1/#image-classification","text":"Optional: Libraries specific to cell segmentation. I am creating a new conda env, ben for installing tools related to cell segmentation analysis. These tools require additional set of packages (including installing using pip ) and are updated often which together can make rey env unstable over long run. Most of packages are based on package requirements for CellPose tool: setup.py and requirements.txt file. mamba create -c conda-forge -c pytorch -n ben python = 3 .9 tensorflow-gpu keras pytorch torchvision cudatoolkit = 11 .1.1 cudatoolkit-dev = 11 .1.1 scikit-learn numpy scipy natsort tifffile tqdm numba torch-optimizer Before activating ben env, duplicate modulefile, gpu/11.1.1 that we created earlier to gpu/11.1.1_ben . Replace conda env name from rey to ben in gpu/11.1.1_ben . This will allow to load a valid GPU env and avoid potential danger of putting rey paths in PATH and LD_LIBRARY_PATH while we work in ben env. Activate ben env mamba activate ben Check for a valid bash env If you notice any of rey env related paths, especially taking precedence over ben env paths, something is wrong and you should check modulefiles above to load conda env specific valid env. Installing cellpose and related package dependencies with invalid bash env will invariably break the core, rey env . module load gpu/11.1.1_ben ## These should point to paths related to ben and not rey env. echo $PATH echo $LD_LIBRARY_PATH","title":"Image Classification"},{"location":"hpc/gpu/winter_1/#cellpose","text":"A generalized algorithm for cellular segmentation. MouseLand/cellpose Not recommended but given many dependencies for cellpose are not available or of conflicting nature using mamba install , I am falling back to pip install . pip install cellpose [ all ] |& tee -a ~/logs/cellpose_install.log In case of errors or unstable env, I can always purge ben env without any impact on rey conda env. Installation log and warnings, if any Installing collected packages: googleapis-common-protos, pyparsing, numpy, google-crc32c, google-api-core, PyWavelets, pyqt5.sip, PyQt5-Qt5, packaging, opencv-python-headless, networkx, imageio, google-resumable-media, google-cloud-core, fastremap, edt, scikit-image, pyqtgraph, pyqt5, google-cloud-storage, cellpose Attempting uninstall: numpy Found existing installation: numpy 1.19.5 Uninstalling numpy-1.19.5: Successfully uninstalled numpy-1.19.5 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow 2.6.2 requires numpy~=1.19.2, but you have numpy 1.21.5 which is incompatible. Successfully installed PyQt5-Qt5-5.15.2 PyWavelets-1.2.0 cellpose-0.7.2 edt-2.1.1 fastremap-1.12.2 google-api-core-2.4.0 google-cloud-core-2.2.1 google-cloud-storage-2.0.0 google-crc32c-1.3.0 google-resumable-media-2.1.0 googleapis-common-protos-1.54.0 imageio-2.13.5 networkx-2.6.3 numpy-1.21.5 opencv-python-headless-4.5.5.62 packaging-21.3 pyparsing-3.0.6 pyqt5-5.15.6 pyqt5.sip-12.9.0 pyqtgraph-0.11.0rc0 scikit-image-0.19.1 Turns out tensorflow 2 (GPU) works with updated numpy and should not be throw an error. python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\" CellPose should now be all set for running in ben env. cellpose --help","title":"Cellpose"},{"location":"hpc/gpu/winter_1/#stardist","text":"StarDist - Object Detection with Star-convex Shapes. stardist/stardist ## in ben env pip install stardist |& tee -a stardist_install.log Installation log and warnings, if any Installing collected packages: python-dateutil, kiwisolver, fonttools, cycler, matplotlib, csbdeep, stardist Successfully installed csbdeep-0.6.3 cycler-0.11.0 fonttools-4.28.5 kiwisolver-1.3.2 matplotlib-3.5.1 python-dateutil-2.8.2 stardist-0.7.3 To test run, follow example from stardist repo .","title":"Stardist"},{"location":"hpc/gpu/winter_1/#cellprofiler","text":"Tool for image analysis, cellprofiler.org Related bioformats2raw and raw2ometiff were downloaded as standalone binary packages and installed as modules. PS: Cellprofiler has a limited GPU support for now but it may change in the future. Follow Cellprofiler forum for updates. For now, I am installing it in grogu env which is a toy env! ## login to CPU HPC ssh sumner Create grogu conda env mamba create -c conda-forge -c bioconda -n grogu cellprofiler Run cellprofiler mamba activate grogu cellprofiler --help","title":"Cellprofiler"},{"location":"hpc/gpu/winter_1/#update-bash-startup","text":"Finally, I am tweaking bash startup sequence that we had setup earlier, such that it can allow loading GPU-specific bash env only when we login to Winter GPU HPC and not on Sumner CPU HPC. I have made following changes to bash startup. You can download my bash startup files here . Update SET PATH block of ~/.bash_profile to reset PATH for Winter GPU. See my notes under elif [[ \"$(hostname)\" == *\"winter\"* ]]; then section in an example .bash_profile file. Update ~/.profile.d/void/VW01_set_winter_gpu.sh to load Winter specific settings. See more into an example VW01_set_winter_gpu.sh file. Logout and login again to Winter HPC. You will see a near identical bash prompt like Sumner HPC, e.g., user@winter-log1 . However, when you check echo $PATH output and echo $CONDA_DEFAULT_ENV , you will notice that a default conda env in Winter HPC is now rey while in Sumner HPC, it is base (sometimes called root ). Of course, you can revert to base or any other conda env in Winter HPC by doing mamba deactivate (because we changed from base to rey during bash startup) and then mamba activate base (or yoda, or any other env). If you have also setup activate.d/deactivate.d scripts as detailed earlier , you will be able to fine tune loading and unloading of conda env specific to HPC type (CPU or GPU) as well as type of R and GPU-specific configs. See /confs/hpc/mambaforge/envs for example scripts.","title":"Update bash startup"},{"location":"hpc/gpu/winter_1/#done","text":"Hope you have found this documentation helpful. I think this is more technical that I originally expected and you may have to look into stackoverflow or elsewhere to understand jargons I used across pages. Hopefully, I can go through some of sections again and put emphasis on rationale behind setting up my linux environment. That said, I hope this documentation, at least the CPU part, should get you started with HPC setup. For learning specific programming language and data analysis, I will post a few external resources on getting started guide to learn programming in Python, R, and more. Best wishes! There could be a difference though if both HPCs do not share common OS or they are using different system defaults, e.g., loading different bash env from /etc/profile which is managed by HPC staff. \u21a9 Installing common packages in yoda env and installing essentials . \u21a9 See warning box above on why our setup can still update R packages managed by yoda env despite using custom Renviron file. \u21a9","title":"Done!"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: bash \u00b6 Setting up CPU env - Part 1 conda \u00b6 Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 database \u00b6 Setting up CPU env - Part 3 deep learning \u00b6 Setting up GPU env gpu \u00b6 Setting up GPU env hpc \u00b6 Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env imaging \u00b6 Setting up GPU env jupyter \u00b6 Setting up CPU env - Part 2 Setting up CPU env - Part 3 kernels \u00b6 Setting up CPU env - Part 2 programming \u00b6 Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env setup \u00b6 Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env startup \u00b6 Setting up CPU env - Part 3","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#bash","text":"Setting up CPU env - Part 1","title":"bash"},{"location":"tags/#conda","text":"Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3","title":"conda"},{"location":"tags/#database","text":"Setting up CPU env - Part 3","title":"database"},{"location":"tags/#deep-learning","text":"Setting up GPU env","title":"deep learning"},{"location":"tags/#gpu","text":"Setting up GPU env","title":"gpu"},{"location":"tags/#hpc","text":"Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env","title":"hpc"},{"location":"tags/#imaging","text":"Setting up GPU env","title":"imaging"},{"location":"tags/#jupyter","text":"Setting up CPU env - Part 2 Setting up CPU env - Part 3","title":"jupyter"},{"location":"tags/#kernels","text":"Setting up CPU env - Part 2","title":"kernels"},{"location":"tags/#programming","text":"Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env","title":"programming"},{"location":"tags/#setup","text":"Setting up CPU env - Part 1 Setting up CPU env - Part 2 Setting up CPU env - Part 3 Setting up GPU env","title":"setup"},{"location":"tags/#startup","text":"Setting up CPU env - Part 3","title":"startup"}]}